[
  {
    "objectID": "new index.html",
    "href": "new index.html",
    "title": "About",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "data_mining/purple martins/purple martins.html",
    "href": "data_mining/purple martins/purple martins.html",
    "title": "Coordinate Reference Systems (purple martins)",
    "section": "",
    "text": "Coordinate Reference System Exercise, Kaggle\n\n\nIntroduction\n귀하는 조류 보호 전문가이며 purple martins의 이동 패턴을 이해하고 싶어합니다. 당신의 연구에서 당신은 이 새들이 일반적으로 미국 동부에서 여름 번식기를 보낸 다음 겨울 동안 남미로 이동한다는 사실을 발견했습니다. 하지만 이 새는 멸종 위기에 처해 있으므로 이 새들이 방문할 가능성이 더 높은 위치를 자세히 살펴보고자 합니다.\n\n\n\n남미에는 여러 보호 구역이 있으며 그곳으로 이동(또는 거주)하는 종들이 번성할 수 있는 최상의 기회를 갖도록 특별 규정에 따라 운영됩니다. purple martins이 이 지역을 방문하는 경향이 있는지 알고 싶습니다. 이 질문에 답하기 위해 11가지 새의 연중 위치를 추적하는 최근 수집된 데이터를 사용합니다.\n시작하기 전에 아래 코드 셀을 실행하여 모든 것을 설정하십시오.\n\n\nReading package\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom shapely.geometry import LineString\n\n# from learntools.core import binder\n# binder.bind(globals())\n# from learntools.geospatial.ex2 import *\n\n\n\nExercises\n\n1) Load the data.\n다음 코드 셀(변경 사항 없음)을 실행하여 GPS 데이터를 pandas DataFrame birds_df로 로드합니다.\n\n# Load the data and print the first 5 rows\nbirds_df = pd.read_csv(\"/Users/jungwoolee/Desktop/college/data mining/geo_data/purple_martin.csv\", parse_dates=['timestamp'])\nprint(\"There are {} different birds in the dataset.\".format(birds_df[\"tag-local-identifier\"].nunique()))\nbirds_df.head()\n\nThere are 11 different birds in the dataset.\n\n\n\n\n\n\n  \n    \n      \n      timestamp\n      location-long\n      location-lat\n      tag-local-identifier\n    \n  \n  \n    \n      0\n      2014-08-15 05:56:00\n      -88.146014\n      17.513049\n      30448\n    \n    \n      1\n      2014-09-01 05:59:00\n      -85.243501\n      13.095782\n      30448\n    \n    \n      2\n      2014-10-30 23:58:00\n      -62.906089\n      -7.852436\n      30448\n    \n    \n      3\n      2014-11-15 04:59:00\n      -61.776826\n      -11.723898\n      30448\n    \n    \n      4\n      2014-11-30 09:59:00\n      -61.241538\n      -11.612237\n      30448\n    \n  \n\n\n\n\n데이터 세트에는 11마리의 새가 있으며 각 새는 “tag-local-identifier” 열의 고유한 값으로 식별됩니다. 각 새는 일년 중 서로 다른 시간에 수집된 몇 가지 측정값을 가지고 있습니다.\n다음 코드 셀을 사용하여 GeoDataFrame ’birds’를 만듭니다. - birds에는 (경도, 위도) 위치가 있는 Point 객체를 포함하는 “도형” 열과 함께 birds_df의 모든 열이 있어야 합니다. - birds의 CRS를 {'init': 'epsg:4326'}로 설정합니다.\n\nbirds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[\"location-long\"], birds_df[\"location-lat\"]))\n\nbirds.crs = {'init' :'epsg:4326'}\nbirds.head()\n\n/Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      timestamp\n      location-long\n      location-lat\n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      2014-08-15 05:56:00\n      -88.146014\n      17.513049\n      30448\n      POINT (-88.14601 17.51305)\n    \n    \n      1\n      2014-09-01 05:59:00\n      -85.243501\n      13.095782\n      30448\n      POINT (-85.24350 13.09578)\n    \n    \n      2\n      2014-10-30 23:58:00\n      -62.906089\n      -7.852436\n      30448\n      POINT (-62.90609 -7.85244)\n    \n    \n      3\n      2014-11-15 04:59:00\n      -61.776826\n      -11.723898\n      30448\n      POINT (-61.77683 -11.72390)\n    \n    \n      4\n      2014-11-30 09:59:00\n      -61.241538\n      -11.612237\n      30448\n      POINT (-61.24154 -11.61224)\n    \n  \n\n\n\n\n\n\n2) Plot the data.\n다음으로 GeoPandas의 naturalearth_lowres 데이터세트를 로드하고 아메리카(북미와 남미 모두)에 있는 모든 국가의 경계를 포함하는 GeoDataFrame으로 아메리카를 설정합니다. 변경 없이 다음 코드 셀을 실행합니다.\n\n# Load a GeoDataFrame with country boundaries in North/South America, print the first 5 rows\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\namericas = world.loc[world['continent'].isin(['North America', 'South America'])]\namericas.head()\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n    \n      4\n      328239523.0\n      North America\n      United States of America\n      USA\n      21433226\n      MULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n    \n    \n      9\n      44938712.0\n      South America\n      Argentina\n      ARG\n      445445\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n    \n    \n      10\n      18952038.0\n      South America\n      Chile\n      CHL\n      282318\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n    \n    \n      16\n      11263077.0\n      North America\n      Haiti\n      HTI\n      14332\n      POLYGON ((-71.71236 19.71446, -71.62487 19.169...\n    \n  \n\n\n\n\n다음 코드 셀을 사용하여 (1) 미국 GeoDataFrame의 국가 경계와 (2) birds_gdf GeoDataFrame의 모든 지점을 모두 표시하는 단일 플롯을 만듭니다.\n여기서 특별한 스타일에 대해 걱정하지 마십시오. 모든 데이터가 제대로 로드되었는지 빠른 온전성 검사로 예비 플롯을 만드십시오. 특히, 새를 구별하기 위해 포인트를 색상으로 구분할 필요가 없으며, 시작점과 끝점을 구별할 필요가 없습니다. 우리는 연습의 다음 부분에서 그것을 할 것입니다.\n\nax = americas.plot(figsize = (10,10), color = 'white', linestyle = '-', edgecolor = 'gray')\nbirds.plot(ax = ax, markersize = 10)\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n3) Where does each bird start and end its journey? (Part 1)\n이제 각 새의 이동 경로를 자세히 살펴볼 준비가 되었습니다. 다음 코드 셀을 실행하여 두 개의 GeoDataFrame을 만듭니다. - path_gdf에는 각 새의 경로를 표시하는 LineString 개체가 포함되어 있습니다. ‘LineString()’ 메서드를 사용하여 Point 개체 목록에서 LineString 개체를 만듭니다.\n\nstart_gdf는 각 새의 시작점을 포함합니다.\n\n\n# GeoDataFrame showing path for each bird\npath_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry = path_df.geometry)\npath_gdf.crs = {'init' :'epsg:4326'}\n\n# GeoDataFrame showing starting point for each bird\nstart_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry = start_df.geometry)\nstart_gdf.crs = {'init' :'epsg:4326'}\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\n/Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n/Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      30048\n      POINT (-90.12992 20.73242)\n    \n    \n      1\n      30054\n      POINT (-93.60861 46.50563)\n    \n    \n      2\n      30198\n      POINT (-80.31036 25.92545)\n    \n    \n      3\n      30263\n      POINT (-76.78146 42.99209)\n    \n    \n      4\n      30275\n      POINT (-76.78213 42.99207)\n    \n  \n\n\n\n\n다음 코드 셀을 사용하여 각 새의 최종 위치를 포함하는 GeoDataFrame end_gdf를 생성합니다.\n\n형식은 두 개의 열(“tag-local-identifier” 및 “geometry”)이 있는 start_gdf의 형식과 동일해야 합니다. 여기서 “geometry” 열에는 Point 객체가 포함됩니다.\nend_gdf의 CRS를 {'init': 'epsg:4326'}로 설정합니다.\n\n\nend_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_df, geometry = end_df.geometry)\nend_gdf.crs = {'init' :'epsg:4326'}\n\n/Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n4) Where does each bird start and end its journey? (Part 2)\n위 질문의 GeoDataFrames(path_gdf, start_gdf 및 end_gdf)를 사용하여 단일 지도에서 모든 새의 경로를 시각화합니다.\namericas GeoDataFrame을 사용할 수도 있습니다.\n\nax = americas.plot(figsize = (10, 10), color = 'white', linestyle = '-', edgecolor = 'gray')\n\nstart_gdf.plot(ax = ax, color = 'red',  markersize = 30)\npath_gdf.plot(ax = ax, cmap = 'tab20b', linestyle = '-', linewidth = 1, zorder = 1)\nend_gdf.plot(ax = ax, color = 'black', markersize = 30)\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n5) Where are the protected areas in South America? (Part 1)\n모든 새들이 남미 어딘가에 도착하는 것처럼 보입니다. 그러나 그들은 보호 지역으로 가고 있습니까?\n다음 코드 셀에서는 남미의 모든 보호 지역 위치를 포함하는 GeoDataFrame protected_areas를 생성합니다. 해당 shapefile은 파일 경로 protected_filepath에 있습니다.\n\n# Path of the shapefile to load\nprotected_filepath = \"/Users/jungwoolee/Desktop/college/data mining/geo_data/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp\"\n\n# Your code here\nprotected_areas = gpd.read_file(protected_filepath)\n\n\n\n6) Where are the protected areas in South America? (Part 2)\nprotected_areas GeoDataFrame을 사용하여 남아메리카의 보호 지역 위치를 표시하는 플롯을 만듭니다. (일부 보호 지역은 육지에 있고 다른 지역은 바다에 있음을 알 수 있습니다.)\n\n# Country boundaries in South America\nsouth_america = americas.loc[americas['continent'] == 'South America']\n\n# Your code here: plot protected areas in South America\nax = south_america.plot(figsize = (10, 10), color = 'white', edgecolor = 'gray')\nprotected_areas.plot(ax = ax, alpha = 0.4)\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n7) What percentage of South America is protected?\n남미의 몇 퍼센트가 새들에게 적합한지 알기 위해 보호되는 남미의 비율을 결정하는 데 관심이 있습니다.\n첫 번째 단계로 남미의 모든 보호 토지(해양 지역 제외)의 총 면적을 계산합니다. 이를 위해 총 면적과 총 해양 면적을 각각 평방 킬로미터 단위로 포함하는 “REP_AREA” 및 “REP_M_AREA” 열을 사용합니다.\n변경 없이 아래 코드 셀을 실행합니다.\n\nP_Area = sum(protected_areas['REP_AREA']-protected_areas['REP_M_AREA'])\nprint(\"South America has {} square kilometers of protected areas.\".format(P_Area))\n\nSouth America has 5396761.9116883585 square kilometers of protected areas.\n\n\n그런 다음 계산을 완료하기 위해 south_america GeoDataFrame을 사용합니다.\n\nsouth_america.head()\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      9\n      44938712.0\n      South America\n      Argentina\n      ARG\n      445445\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n    \n    \n      10\n      18952038.0\n      South America\n      Chile\n      CHL\n      282318\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n    \n    \n      20\n      3398.0\n      South America\n      Falkland Is.\n      FLK\n      282\n      POLYGON ((-61.20000 -51.85000, -60.00000 -51.2...\n    \n    \n      28\n      3461734.0\n      South America\n      Uruguay\n      URY\n      56045\n      POLYGON ((-57.62513 -30.21629, -56.97603 -30.1...\n    \n    \n      29\n      211049527.0\n      South America\n      Brazil\n      BRA\n      1839758\n      POLYGON ((-53.37366 -33.76838, -53.65054 -33.2...\n    \n  \n\n\n\n\n다음 단계에 따라 남아메리카의 전체 면적을 계산합니다. - 각 다각형(CRS는 EPSG 3035)의 area 속성을 사용하여 각 국가의 면적을 계산하고 결과를 더합니다. 계산된 면적은 평방 미터 단위입니다. - 답을 평방 킬로미터 단위로 변환하십시오.\n\ntotalArea = sum(south_america.geometry.to_crs(epsg = 3035).area) / 10 ** 6\n\n아래 코드 셀을 실행하여 보호되는 남미의 비율을 계산합니다.\n\n# What percentage of South America is protected?\npercentage_protected = P_Area/totalArea\nprint('Approximately {}% of South America is protected.'.format(round(percentage_protected * 100, 2)))\n\nApproximately 30.39% of South America is protected.\n\n\n\n\n8) Where are the birds in South America?\n그렇다면 새들은 보호 구역에 있습니까?\n남미에서 새가 발견된 모든 위치를 모든 새에 대해 표시하는 플롯을 만듭니다. 또한 남미의 모든 보호 지역 위치를 플로팅합니다.\n(토지 구성 요소가 없는) 순수 해양 지역인 보호 지역을 제외하려면 “MARINE” 열을 사용할 수 있습니다(그리고 대신 protected_areas[protected_areas['MARINE']!='2']의 행만 표시) protected_areas GeoDataFrame의 모든 행).\n\nax = south_america.plot(figsize = (10, 10), color = 'white', edgecolor = 'gray')\nprotected_areas[protected_areas['MARINE'] != '2'].plot(ax = ax, alpha = 0.4, zorder = 1)\nbirds[birds.geometry.y < 0].plot(ax = ax, color = 'red', alpha = 0.6, markersize = 10, zorder = 2)\n\n<AxesSubplot:>"
  },
  {
    "objectID": "data_mining/japan earthequake/japan earthquake.html",
    "href": "data_mining/japan earthequake/japan earthquake.html",
    "title": "Interactive Maps (Japan earthequake)",
    "section": "",
    "text": "Interactive Maps Exercise, Kaggle\n\n\nIntroduction\n귀하는 일본의 도시 안전 계획자이며 추가 지진 보강이 필요한 일본 지역을 분석하고 있습니다. 인구 밀도가 높고 지진이 자주 발생하는 지역은 어디입니까?\n\n\n\n시작하기 전에 아래 코드 셀을 실행하여 모든 것을 설정하십시오.\n\nimport pandas as pd\nimport geopandas as gpd\n\nimport folium\nfrom folium import Choropleth\nfrom folium.plugins import HeatMap\n\n# from learntools.core import binder\n# binder.bind(globals())\n# from learntools.geospatial.ex3 import *\n\n대화형 지도를 표시하기 위해 embed_map() 함수를 정의합니다. 지도를 포함하는 변수와 지도가 저장될 HTML 파일의 이름이라는 두 가지 인수를 허용합니다.\n이 기능은 지도가 모든 웹 브라우저에서 표시되도록 합니다.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\n\nExercises\n\n1) 지진은 판 경계와 일치합니까?\n아래 코드 셀을 실행하여 글로벌 판 경계를 표시하는 DataFrame plate_boundaries를 생성합니다. “columns” 열은 경계를 따라 위치(위도, 경도)의 목록입니다.\n\nplate_boundaries = gpd.read_file(\"/Users/jungwoolee/Desktop/college/data mining/geo_data/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp\")\nplate_boundaries['coordinates'] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis = 'columns')\nplate_boundaries.drop('geometry', axis = 1, inplace = True)\n\nplate_boundaries.head()\n\n\n\n\n\n  \n    \n      \n      HAZ_PLATES\n      HAZ_PLAT_1\n      HAZ_PLAT_2\n      Shape_Leng\n      coordinates\n    \n  \n  \n    \n      0\n      TRENCH\n      SERAM TROUGH (ACTIVE)\n      6722\n      5.843467\n      [(-5.444200361999947, 133.6808931800001), (-5....\n    \n    \n      1\n      TRENCH\n      WETAR THRUST\n      6722\n      1.829013\n      [(-7.760600482999962, 125.47879802900002), (-7...\n    \n    \n      2\n      TRENCH\n      TRENCH WEST OF LUZON (MANILA TRENCH) NORTHERN ...\n      6621\n      6.743604\n      [(19.817899819000047, 120.09999798800004), (19...\n    \n    \n      3\n      TRENCH\n      BONIN TRENCH\n      9821\n      8.329381\n      [(26.175899215000072, 143.20620700100005), (26...\n    \n    \n      4\n      TRENCH\n      NEW GUINEA TRENCH\n      8001\n      11.998145\n      [(0.41880004000006466, 132.8273013480001), (0....\n    \n  \n\n\n\n\n다음으로 아래의 코드 셀을 변경 없이 실행하여 과거 지진 데이터를 DataFrame earthquakes로 로드합니다.\n\n# Load the data\nearthquakes = pd.read_csv(\"/Users/jungwoolee/Desktop/college/data mining/geo_data/earthquakes1970-2014.csv\", parse_dates = [\"DateTime\"])\nearthquakes.head()\n\n\n\n\n\n  \n    \n      \n      DateTime\n      Latitude\n      Longitude\n      Depth\n      Magnitude\n      MagType\n      NbStations\n      Gap\n      Distance\n      RMS\n      Source\n      EventID\n    \n  \n  \n    \n      0\n      1970-01-04 17:00:40.200\n      24.139\n      102.503\n      31.0\n      7.5\n      Ms\n      90.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970010e+09\n    \n    \n      1\n      1970-01-06 05:35:51.800\n      -9.628\n      151.458\n      8.0\n      6.2\n      Ms\n      85.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      2\n      1970-01-08 17:12:39.100\n      -34.741\n      178.568\n      179.0\n      6.1\n      Mb\n      59.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      3\n      1970-01-10 12:07:08.600\n      6.825\n      126.737\n      73.0\n      6.1\n      Mb\n      91.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      4\n      1970-01-16 08:05:39.000\n      60.280\n      -152.660\n      85.0\n      6.0\n      ML\n      0.0\n      NaN\n      NaN\n      NaN\n      AK\n      NaN\n    \n  \n\n\n\n\n아래 코드 셀은 지도에서 플레이트 경계를 시각화합니다. 모든 지진 데이터를 사용하여 동일한 지도에 히트맵을 추가하여 지진이 판 경계와 일치하는지 확인합니다.\n\n# Create a base map with plate boundaries\nm_1 = folium.Map(location = [35, 136], tiles = 'cartodbpositron', zoom_start = 5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations = plate_boundaries.coordinates.iloc[i], weight = 2, color = 'black').add_to(m_1)\n\n# Your code here: Add a heatmap to the map\nHeatMap(data = earthquakes[['Latitude', 'Longitude']], radius = 15).add_to(m_1)\n\n# Show the map\nm_1\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n위의 지도에서 지진은 판 경계와 일치합니까?\n\n\n2) 일본에서 지진의 깊이와 판 경계와의 근접성 사이에 관계가 있습니까?\n당신은 최근에 지진의 깊이가 지구의 구조에 대한 중요한 정보를 알려준다는 것을 읽었습니다. 흥미로운 글로벌 패턴이 있는지 확인하고 일본에서 깊이가 어떻게 다른지 이해하고 싶습니다.\n\n# Create a base map with plate boundaries\nm_2 = folium.Map(location = [35,136], tiles = 'cartodbpositron', zoom_start = 5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations = plate_boundaries.coordinates.iloc[i], weight = 2, color = 'black').add_to(m_2)\n    \n# Your code here: Add a map to visualize earthquake depth\ndef color_producer(val):\n    if val < 50:\n        return 'green'\n    elif val < 100:\n        return 'yellow'\n    else:\n        return 'red'\n\n# Add a map to visualize earthquake depth\nfor i in range(0, len(earthquakes)):\n    folium.Circle(\n        location = [earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        radius = 2000,\n        color = color_producer(earthquakes.iloc[i]['Depth'])).add_to(m_2)\n\n# View the map\nm_2\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n판 경계에 대한 근접성과 지진 깊이 사이의 관계를 감지할 수 있습니까? 이 패턴이 전 세계적으로 유지됩니까? 일본에서?\n해결책: 일본 북부에서는 판 경계에 가까운 지진이 더 얕아지는 경향이 있는 것으로 보입니다(판 경계에서 멀리 있는 지진은 더 깊습니다). 이 패턴은 남미 서부 해안과 같은 다른 위치에서도 반복됩니다. 그러나 모든 곳에서 적용되는 것은 아닙니다(예: 중국, 몽골 및 러시아).\n\n\n3) 인구 밀도가 높은 현은 어디입니까?\n변경 없이 다음 코드 셀을 실행하여 일본 현의 지리적 경계를 포함하는 GeoDataFrame 현(일본 행정구역)을 생성합니다.\n\n# GeoDataFrame with prefecture boundaries\nprefectures = gpd.read_file(\"/Users/jungwoolee/Desktop/college/data mining/geo_data/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp\")\nprefectures.set_index('prefecture', inplace=True)\nprefectures.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n    \n      prefecture\n      \n    \n  \n  \n    \n      Aichi\n      MULTIPOLYGON (((137.09523 34.65330, 137.09546 ...\n    \n    \n      Akita\n      MULTIPOLYGON (((139.55725 39.20330, 139.55765 ...\n    \n    \n      Aomori\n      MULTIPOLYGON (((141.39860 40.92472, 141.39806 ...\n    \n    \n      Chiba\n      MULTIPOLYGON (((139.82488 34.98967, 139.82434 ...\n    \n    \n      Ehime\n      MULTIPOLYGON (((132.55859 32.91224, 132.55904 ...\n    \n  \n\n\n\n\n다음 코드 셀은 각 일본 현의 인구, 면적(평방 킬로미터) 및 인구 밀도(평방 킬로미터당)를 포함하는 DataFrame stats를 생성합니다. 변경 없이 코드 셀을 실행합니다.\n\n# DataFrame containing population of each prefecture\npopulation = pd.read_csv(\"/Users/jungwoolee/Desktop/college/data mining/geo_data/japan-prefecture-population.csv\")\npopulation.set_index('prefecture', inplace = True)\n\n# Calculate area (in square kilometers) of each prefecture\narea_sqkm = pd.Series(prefectures.geometry.to_crs(epsg = 32654).area / 10**6, name = 'area_sqkm')\nstats = population.join(area_sqkm)\n\n# Add density (per square kilometer) of each prefecture\nstats['density'] = stats[\"population\"] / stats[\"area_sqkm\"]\nstats.head()\n\n\n\n\n\n  \n    \n      \n      population\n      area_sqkm\n      density\n    \n    \n      prefecture\n      \n      \n      \n    \n  \n  \n    \n      Tokyo\n      12868000\n      1800.614782\n      7146.448049\n    \n    \n      Kanagawa\n      8943000\n      2383.038975\n      3752.771186\n    \n    \n      Osaka\n      8801000\n      1923.151529\n      4576.342460\n    \n    \n      Aichi\n      7418000\n      5164.400005\n      1436.372085\n    \n    \n      Saitama\n      7130000\n      3794.036890\n      1879.264806\n    \n  \n\n\n\n\n다음 코드 셀을 사용하여 등치 맵을 생성하여 인구 밀도를 시각화합니다.\n\n# Create a base map\nm_3 = folium.Map(location = [35,136], tiles = 'cartodbpositron', zoom_start = 5)\n\n# Your code here: create a choropleth map to visualize population density\nChoropleth(geo_data = prefectures['geometry'].__geo_interface__, \n           data = stats['density'], \n           key_on = \"feature.id\", \n           fill_color='YlGnBu', \n           legend_name = '인구 밀도 (평방 킬로미터)'\n          ).add_to(m_3)\n\n# View the map\nm_3\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n어느 3개 현이 다른 현보다 상대적으로 밀도가 높습니까? 그들은 전국적으로 퍼져 있습니까, 아니면 모두 거의 같은 지리적 지역에 위치하고 있습니까? (일본 지리에 익숙하지 않은 경우 이 지도가 질문에 답하는 데 유용할 수 있습니다.)\n도쿄, 가나가와, 오사카는 인구 밀도가 가장 높습니다. 이들 현은 모두 일본 중부에 위치하고 있으며 도쿄와 가나가와가 인접해 있습니다.\n\n\n4) 규모가 큰 지진이 발생하기 쉬운 고밀도 현은 어디입니까?\n지진 보강으로 혜택을 받을 수 있는 한 현을 제안하는 지도를 만드십시오. 지도는 밀도와 지진 규모를 모두 시각화해야 합니다.\n\n# Create a base map\nm_4 = folium.Map(location = [35, 136], tiles = 'cartodbpositron', zoom_start = 5)\n\n# Your code here: create a map\ndef color_producer(magnitude):\n    if magnitude > 6.5:\n        return 'red'\n    else:\n        return 'green'\n\nChoropleth(\n    geo_data = prefectures['geometry'].__geo_interface__,\n    data = stats['density'],\n    key_on = \"feature.id\",\n    fill_color = 'YlGnBu',\n    legend_name = '인구 밀도 (평방 킬로미터)').add_to(m_4)\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location = [earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        popup = (\"{} ({})\").format(\n            earthquakes.iloc[i]['Magnitude'],\n            earthquakes.iloc[i]['DateTime'].year),\n        radius = earthquakes.iloc[i]['Magnitude'] ** 5.5,\n        color = color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)\n\n\n# View the map\nm_4\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n추가 지진 보강을 위해 어느 현을 추천합니까?\n해결: 이 질문에 대한 명확한 단일 답변은 없지만 몇 가지 합리적인 옵션이 있습니다. 도쿄는 지금까지 가장 인구 밀도가 높은 현이며 여러 지진을 경험했습니다. 오사카는 상대적으로 인구 밀도가 낮지만 도쿄 근처보다 상대적으로 강한 지진이 발생했습니다. 그리고 가나가와의 긴 해안(높은 밀도와 역사적으로 강한 지진의 근접성 외에도)은 추가적인 잠재적인 쓰나미 위험에 대해 걱정하게 만들 수 있습니다."
  },
  {
    "objectID": "data_mining/geopandas/geopandas.html",
    "href": "data_mining/geopandas/geopandas.html",
    "title": "geopandas",
    "section": "",
    "text": "Your First Map, Kaggle\n\n\nReading data\n\npip install geopandas\n\nCollecting geopandas\n  Downloading geopandas-0.12.2-py3-none-any.whl (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 13.4 MB/s eta 0:00:0000:0100:01\nRequirement already satisfied: pandas>=1.0.0 in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (1.4.3)\nRequirement already satisfied: packaging in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (21.3)\nCollecting fiona>=1.8\n  Downloading Fiona-1.9.3-cp39-cp39-macosx_10_15_x86_64.whl (18.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.3/18.3 MB 22.0 MB/s eta 0:00:0000:0100:01\nCollecting shapely>=1.7\n  Downloading shapely-2.0.1-cp39-cp39-macosx_10_9_x86_64.whl (1.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 23.4 MB/s eta 0:00:00a 0:00:01\nCollecting pyproj>=2.6.1.post1\n  Downloading pyproj-3.5.0-cp39-cp39-macosx_10_9_x86_64.whl (8.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 23.0 MB/s eta 0:00:00a 0:00:01\nCollecting munch>=2.3.2\n  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: importlib-metadata in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (4.8.1)\nCollecting cligj>=0.5\n  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\nRequirement already satisfied: click~=8.0 in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (8.0.3)\nCollecting click-plugins>=1.0\n  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\nRequirement already satisfied: attrs>=19.2.0 in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (21.2.0)\nRequirement already satisfied: certifi in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (2022.9.14)\nRequirement already satisfied: python-dateutil>=2.8.1 in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (2021.3)\nRequirement already satisfied: numpy>=1.18.5 in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (1.20.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from packaging->geopandas) (3.0.4)\nRequirement already satisfied: six in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from munch>=2.3.2->fiona>=1.8->geopandas) (1.16.0)\nRequirement already satisfied: zipp>=0.5 in /Users/jungwoolee/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata->fiona>=1.8->geopandas) (3.6.0)\nInstalling collected packages: shapely, pyproj, munch, cligj, click-plugins, fiona, geopandas\nSuccessfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.3 geopandas-0.12.2 munch-2.5.0 pyproj-3.5.0 shapely-2.0.1\n\n[notice] A new release of pip available: 22.2.2 -> 23.1\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport geopandas as gpd\n\nShapefile, GeoJSON, KML 및 GPKG와 같은 다양한 지리 공간 파일 형식이 있습니다. 우리는 이 마이크로 코스에서 그들의 차이점에 대해 논의하지 않을 것이지만, 다음을 언급하는 것이 중요합니다.\n\nShapefile은 당신이 만나게 될 가장 일반적인 파일 유형이며,\n이러한 모든 파일 유형은 gpd.read_file() 함수로 빠르게 로드할 수 있습니다.\n\n다음 코드 셀은 뉴욕 주 환경 보존부의 관리 하에 있는 숲, 야생 지역 및 기타 토지에 대한 정보가 포함된 셰이프 파일을 로드합니다.\n\n# Read in the data\nfull_data = gpd.read_file(\"/Users/jungwoolee/Downloads/archive/DEC_lands/DEC_lands/DEC_lands.shp\")\n\n# View the first five rows of the data\nfull_data.head()\n\n\n\n\n\n  \n    \n      \n      OBJECTID\n      CATEGORY\n      UNIT\n      FACILITY\n      CLASS\n      UMP\n      DESCRIPTIO\n      REGION\n      COUNTY\n      URL\n      SOURCE\n      UPDATE_\n      OFFICE\n      ACRES\n      LANDS_UID\n      GREENCERT\n      SHAPE_AREA\n      SHAPE_LEN\n      geometry\n    \n  \n  \n    \n      0\n      1\n      FOR PRES DET PAR\n      CFP\n      HANCOCK FP DETACHED PARCEL\n      WILD FOREST\n      NaN\n      DELAWARE COUNTY DETACHED PARCEL\n      4\n      DELAWARE\n      http://www.dec.ny.gov/\n      DELAWARE RPP\n      5/12\n      STAMFORD\n      738.620192\n      103\n      N\n      2.990365e+06\n      7927.662385\n      POLYGON ((486093.245 4635308.586, 486787.235 4...\n    \n    \n      1\n      2\n      FOR PRES DET PAR\n      CFP\n      HANCOCK FP DETACHED PARCEL\n      WILD FOREST\n      NaN\n      DELAWARE COUNTY DETACHED PARCEL\n      4\n      DELAWARE\n      http://www.dec.ny.gov/\n      DELAWARE RPP\n      5/12\n      STAMFORD\n      282.553140\n      1218\n      N\n      1.143940e+06\n      4776.375600\n      POLYGON ((491931.514 4637416.256, 491305.424 4...\n    \n    \n      2\n      3\n      FOR PRES DET PAR\n      CFP\n      HANCOCK FP DETACHED PARCEL\n      WILD FOREST\n      NaN\n      DELAWARE COUNTY DETACHED PARCEL\n      4\n      DELAWARE\n      http://www.dec.ny.gov/\n      DELAWARE RPP\n      5/12\n      STAMFORD\n      234.291262\n      1780\n      N\n      9.485476e+05\n      5783.070364\n      POLYGON ((486000.287 4635834.453, 485007.550 4...\n    \n    \n      3\n      4\n      FOR PRES DET PAR\n      CFP\n      GREENE COUNTY FP DETACHED PARCEL\n      WILD FOREST\n      NaN\n      NaN\n      4\n      GREENE\n      http://www.dec.ny.gov/\n      GREENE RPP\n      5/12\n      STAMFORD\n      450.106464\n      2060\n      N\n      1.822293e+06\n      7021.644833\n      POLYGON ((541716.775 4675243.268, 541217.579 4...\n    \n    \n      4\n      6\n      FOREST PRESERVE\n      AFP\n      SARANAC LAKES WILD FOREST\n      WILD FOREST\n      SARANAC LAKES\n      NaN\n      5\n      ESSEX\n      http://www.dec.ny.gov/lands/22593.html\n      DECRP, ESSEX RPP\n      12/96\n      RAY BROOK\n      69.702387\n      1517\n      N\n      2.821959e+05\n      2663.909932\n      POLYGON ((583896.043 4909643.187, 583891.200 4...\n    \n  \n\n\n\n\nCLASS 열에서 볼 수 있듯이, 처음 다섯 행은 각각 다른 숲에 해당합니다.\n이 튜토리얼의 나머지 부분에서는, 이 데이터를 사용하여 주말 캠핑 여행을 계획하고 싶은 시나리오를 고려해 보세요. 온라인에서 크라우드 소싱 리뷰에 의존하는 대신, 당신은 당신만의 지도를 만들기로 결정했습니다. 이렇게 하면, 당신은 당신의 특정한 관심사에 맞게 여행을 조정할 수 있습니다.\n\n\nPrerequisites\n데이터의 처음 다섯 행을 보기 위해, 우리는 head() 메소드를 사용했다. 당신은 이것이 또한 우리가 Pandas DataFrame을 미리 보는 데 사용하는 것이라는 것을 기억할 것입니다. 사실, DataFrame과 함께 사용할 수 있는 모든 명령은 데이터와 함께 작동합니다!\n이것은 데이터가 (Pandas) DataFrame의 모든 기능을 가진 (GeoPandas) GeoDataFrame 객체에 로드되었기 때문입니다.\n\ntype(full_data)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n예를 들어, 모든 열을 사용할 계획이 없다면, 그 중 일부의 하위 집합을 선택할 수 있습니다. (데이터를 선택하는 다른 방법을 검토하려면, 팬더 마이크로 코스에서 이 튜토리얼을 확인하세요.)\n\ndata = full_data.loc[:, [\"CLASS\", \"COUNTY\", \"geometry\"]].copy()\n\n\ndata.CLASS.value_counts()\n\nWILD FOREST                   965\nINTENSIVE USE                 108\nPRIMITIVE                      60\nWILDERNESS                     52\nADMINISTRATIVE                 17\nUNCLASSIFIED                    7\nHISTORIC                        5\nPRIMITIVE BICYCLE CORRIDOR      4\nCANOE AREA                      1\nName: CLASS, dtype: int64\n\n\n또한 loc(및 iloc)과 isin을 사용하여 데이터의 하위 집합을 선택할 수 있습니다. (이것을 검토하려면, 팬더 마이크로 코스에서 이 튜토리얼을 확인하세요.)\n\n# Select lands that fall under the \"WILD FOREST\" or \"WILDERNESS\" category\nwild_lands = data.loc[data.CLASS.isin(['WILD FOREST', 'WILDERNESS'])].copy()\nwild_lands.head()\n\n\n\n\n\n  \n    \n      \n      CLASS\n      COUNTY\n      geometry\n    \n  \n  \n    \n      0\n      WILD FOREST\n      DELAWARE\n      POLYGON ((486093.245 4635308.586, 486787.235 4...\n    \n    \n      1\n      WILD FOREST\n      DELAWARE\n      POLYGON ((491931.514 4637416.256, 491305.424 4...\n    \n    \n      2\n      WILD FOREST\n      DELAWARE\n      POLYGON ((486000.287 4635834.453, 485007.550 4...\n    \n    \n      3\n      WILD FOREST\n      GREENE\n      POLYGON ((541716.775 4675243.268, 541217.579 4...\n    \n    \n      4\n      WILD FOREST\n      ESSEX\n      POLYGON ((583896.043 4909643.187, 583891.200 4...\n    \n  \n\n\n\n\n위의 명령에 익숙하지 않다면, 참조를 위해 이 페이지를 북마크하는 것이 좋습니다. 필요에 따라 명령을 찾을 수 있습니다. (대안으로, 당신은 팬더 마이크로 코스를 수강할 수 있습니다.) 우리는 지도를 만들기 전에 데이터를 이해하고 필터링하기 위해 이 마이크로 코스 전반에 걸쳐 이러한 명령을 사용할 것입니다.\n\n\nCreate your first map!\n우리는 plot() 메소드로 데이터를 빠르게 시각화할 수 있습니다.\n\nwild_lands.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n모든 GeoDataFrame에는 특별한 “기하학” 열이 포함되어 있습니다. 그것은 우리가 plot() 메소드를 호출할 때 표시되는 모든 기하학적 객체를 포함한다.\n\n# View the first five entries in the \"geometry\" column\nwild_lands.geometry.head()\n\n0    POLYGON ((486093.245 4635308.586, 486787.235 4...\n1    POLYGON ((491931.514 4637416.256, 491305.424 4...\n2    POLYGON ((486000.287 4635834.453, 485007.550 4...\n3    POLYGON ((541716.775 4675243.268, 541217.579 4...\n4    POLYGON ((583896.043 4909643.187, 583891.200 4...\nName: geometry, dtype: geometry\n\n\n우리 데이터 세트의 “기하학” 열에는 위의 플롯에서 각각 다른 모양에 해당하는 2983개의 다른 다각형 객체가 포함되어 있습니다.\n아래 코드 셀에서, 우리는 캠프장 위치(포인트), 발길(라인스트링) 및 카운티 경계(폴리곤)를 포함하는 세 개의 지오데이터프레임을 더 만듭니다.\n\n# Campsites in New York state (Point)\nPOI_data = gpd.read_file(\"/Users/jungwoolee/Downloads/archive/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp\")\ncampsites = POI_data.loc[POI_data.ASSET=='PRIMITIVE CAMPSITE'].copy()\n\n# Foot trails in New York state (LineString)\nroads_trails = gpd.read_file(\"/Users/jungwoolee/Downloads/archive/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp\")\ntrails = roads_trails.loc[roads_trails.ASSET=='FOOT TRAIL'].copy()\n\n# County boundaries in New York state (Polygon)\ncounties = gpd.read_file(\"/Users/jungwoolee/Downloads/archive/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp\")\n\n다음으로, 우리는 네 개의 GeoDataFrames 모두에서 지도를 만듭니다.\nPlot() 메서드는 모양을 사용자 정의하는 데 사용할 수 있는 여러 매개 변수를 (선택 사항) 입력합니다. 가장 중요한 것은, 도끼 값을 설정하면 모든 정보가 동일한 지도에 그려진다는 것이다.\n\n# Define a base map with county boundaries\nax = counties.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)\n\n# Add wild lands, campsites, and foot trails to the base map\nwild_lands.plot(color='lightgreen', ax=ax)\ncampsites.plot(color='maroon', markersize=2, ax=ax)\ntrails.plot(color='black', markersize=1, ax=ax)\n\n<AxesSubplot:>\n\n\n\n\n\n주의 북동부는 캠핑 여행을 위한 좋은 선택이 될 것 같아!"
  },
  {
    "objectID": "data_mining/numpy/numpy.html",
    "href": "data_mining/numpy/numpy.html",
    "title": "Numpy",
    "section": "",
    "text": "“numpy 기본 코드 실습(한글)”\n\n\ntoc: true\nbranch: master\nbadges: true\ncomments: true\nauthor: Jungwoo Lee\ncategories: [jupyter, python]\n\n도구 - 넘파이(NumPy)\n*넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.”\n\n\n\n구글 코랩에서 실행하기"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#np.zeros",
    "href": "data_mining/numpy/numpy.html#np.zeros",
    "title": "Numpy",
    "section": "np.zeros",
    "text": "np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\n\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.zeros((3,4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#용어",
    "href": "data_mining/numpy/numpy.html#용어",
    "title": "Numpy",
    "section": "용어",
    "text": "용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, \\(3 \\times 4=12\\)).\n\n\na = np.zeros((3,4))\na\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\na.shape\n\n(3, 4)\n\n\n\na.ndim  # len(a.shape)와 같습니다\n\n2\n\n\n\na.size\n\n12"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#n-차원-배열",
    "href": "data_mining/numpy/numpy.html#n-차원-배열",
    "title": "Numpy",
    "section": "N-차원 배열",
    "text": "N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다:\n\nnp.zeros((2,2,5))\n\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#배열-타입",
    "href": "data_mining/numpy/numpy.html#배열-타입",
    "title": "Numpy",
    "section": "배열 타입",
    "text": "배열 타입\n넘파이 배열의 타입은 ndarray입니다:\n\ntype(np.zeros((3,4)))\n\nnumpy.ndarray"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#np.ones",
    "href": "data_mining/numpy/numpy.html#np.ones",
    "title": "Numpy",
    "section": "np.ones",
    "text": "np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.ones((3,4))\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#np.full",
    "href": "data_mining/numpy/numpy.html#np.full",
    "title": "Numpy",
    "section": "np.full",
    "text": "np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\n\nnp.full((3,4), np.pi)\n\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#np.empty",
    "href": "data_mining/numpy/numpy.html#np.empty",
    "title": "Numpy",
    "section": "np.empty",
    "text": "np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\n\nnp.empty((2,3))\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#np.array",
    "href": "data_mining/numpy/numpy.html#np.array",
    "title": "Numpy",
    "section": "np.array",
    "text": "np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\n\nnp.array([[1,2,3,4], [10, 20, 30, 40]])\n\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#np.arange",
    "href": "data_mining/numpy/numpy.html#np.arange",
    "title": "Numpy",
    "section": "np.arange",
    "text": "np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\n\nnp.arange(1, 5)\n\narray([1, 2, 3, 4])\n\n\n부동 소수도 가능합니다:\n\nnp.arange(1.0, 5.0)\n\narray([1., 2., 3., 4.])\n\n\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\n\nnp.arange(1, 5, 0.5)\n\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n\n\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\n\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#np.linspace",
    "href": "data_mining/numpy/numpy.html#np.linspace",
    "title": "Numpy",
    "section": "np.linspace",
    "text": "np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\n\nprint(np.linspace(0, 5/3, 6))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#np.rand와-np.randn",
    "href": "data_mining/numpy/numpy.html#np.rand와-np.randn",
    "title": "Numpy",
    "section": "np.rand와 np.randn",
    "text": "np.rand와 np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다.\nrand()와 randn()함수는 둘 다 난수(랜덤 값)를 생성하는 함수입니다.\n예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\nrand(): 함수는 0과 1 사이의 균일한(uniform) 분포에서 무작위로 샘플링된 난수를 반환합니다.\nrandn(): 함수는 평균이 0이고 표준편차가 1인 표준 정규 분포(standard normal distribution)에서 무작위로 샘플링된 난수를 반환합니다.\n\nnp.random.rand(3,4)\n\narray([[0.37892456, 0.17966937, 0.38206837, 0.34922123],\n       [0.80462136, 0.9845914 , 0.9416127 , 0.28305275],\n       [0.21201033, 0.54891417, 0.03781613, 0.4369229 ]])\n\n\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\n\nnp.random.randn(3,4)\n\narray([[ 0.83811287, -0.57131751, -0.4381827 ,  1.1485899 ],\n       [ 1.45316084, -0.47259181, -1.23426057, -0.0669813 ],\n       [ 1.01003549,  1.04381736, -0.93060038,  2.39043293]])\n\n\n따라서, rand() 함수는 균일 분포에서 샘플링하고 randn() 함수는 정규 분포에서 샘플링한다는 차이점이 있습니다. 이러한 함수들은 데이터 분석과 머신러닝 분야에서 데이터 생성 및 시뮬레이션에 널리 사용됩니다.\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#np.fromfunction",
    "href": "data_mining/numpy/numpy.html#np.fromfunction",
    "title": "Numpy",
    "section": "np.fromfunction",
    "text": "np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\nnp.fromfunction: 함수 형식을 가져와 array를 만들어줍니다\n\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\n\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n\n\n\nnp.fromfunction(my_function, (2, 6, 10))\n\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.],\n        [ 20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.],\n        [ 30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.],\n        [ 40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.],\n        [ 50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.],\n        [120., 121., 122., 123., 124., 125., 126., 127., 128., 129.],\n        [130., 131., 132., 133., 134., 135., 136., 137., 138., 139.],\n        [140., 141., 142., 143., 144., 145., 146., 147., 148., 149.],\n        [150., 151., 152., 153., 154., 155., 156., 157., 158., 159.]]])\n\n\n\nmy_function(3, 2, 10)\n\n330\n\n\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "data_mining/numpy/numpy.html#dtype",
    "href": "data_mining/numpy/numpy.html#dtype",
    "title": "Numpy",
    "section": "dtype",
    "text": "dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\n\nc = np.arange(1, 5)\nprint(c.dtype, c)\n\nint64 [1 2 3 4]\n\n\n\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\n\nfloat64 [1. 2. 3. 4.]\n\n\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\n\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\n\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n\n\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "data_mining/numpy/numpy.html#itemsize",
    "href": "data_mining/numpy/numpy.html#itemsize",
    "title": "Numpy",
    "section": "itemsize",
    "text": "itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\n\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n\n8"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#data-버퍼",
    "href": "data_mining/numpy/numpy.html#data-버퍼",
    "title": "Numpy",
    "section": "data 버퍼",
    "text": "data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\n\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nf.data\n\n<memory at 0x7f97929dd790>\n\n\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\n\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\n\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n\n\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "data_mining/numpy/numpy.html#자신을-변경",
    "href": "data_mining/numpy/numpy.html#자신을-변경",
    "title": "Numpy",
    "section": "자신을 변경",
    "text": "자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\nndim: NumPy 배열의 차원 수를 나타냅니다. 즉, 배열의 축(axis) 수를 나타낸다.\n\ng = np.arange(24)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\n\n\n\ng.shape = (6, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\n\n\n\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#reshape",
    "href": "data_mining/numpy/numpy.html#reshape",
    "title": "Numpy",
    "section": "reshape",
    "text": "reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\n\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\n\n\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\n\ng2[1, 2] = 999\ng2\n\narray([[  0,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n\n\n이에 상응하는 g의 원소도 수정됩니다.\n\ng\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#ravel",
    "href": "data_mining/numpy/numpy.html#ravel",
    "title": "Numpy",
    "section": "ravel",
    "text": "ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\nravel(): 함수는 다차원 배열을 1차원 배열로 평탄화(flattening)하는 데 사용\n\ng.ravel()\n\narray([  0,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#규칙-1",
    "href": "data_mining/numpy/numpy.html#규칙-1",
    "title": "Numpy",
    "section": "규칙 1",
    "text": "규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\n\nh = np.arange(5).reshape(1, 1, 5)\nh\n\narray([[[0, 1, 2, 3, 4]]])\n\n\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\n\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\n\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#규칙-2",
    "href": "data_mining/numpy/numpy.html#규칙-2",
    "title": "Numpy",
    "section": "규칙 2",
    "text": "규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\n\nk = np.arange(6).reshape(2, 3)\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\n\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\n\narray([[100, 101, 102],\n       [203, 204, 205]])\n\n\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\n\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\n\narray([[100, 201, 302],\n       [103, 204, 305]])\n\n\n또 매우 간단히 다음 처럼 해도 됩니다:\n\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\n\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#규칙-3",
    "href": "data_mining/numpy/numpy.html#규칙-3",
    "title": "Numpy",
    "section": "규칙 3",
    "text": "규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\n\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\n\noperands could not be broadcast together with shapes (2,3) (2,) \n\n\n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "data_mining/numpy/numpy.html#업캐스팅",
    "href": "data_mining/numpy/numpy.html#업캐스팅",
    "title": "Numpy",
    "section": "업캐스팅",
    "text": "업캐스팅\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\n업캐스팅(Upcasting)은 다른 데이터 유형을 갖는 두 개 이상의 배열을 연산할 때 발생합니다. 업캐스팅은 연산 결과가 더 넓은 범위의 데이터 유형을 사용하도록 강제하는 것입니다. 이렇게 함으로써 데이터 유형이 작은 배열이 데이터 손실 없이 연산에 참여할 수 있도록 합니다.\n\n연산에 참여하는 모든 배열 중에서 데이터 유형이 더 넓은 배열을 찾습니다.\n모든 배열이 같은 데이터 유형을 가지고 있지 않으면, 가장 넓은 데이터 유형을 가진 배열로 모든 배열을 변환합니다.\n\n\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\n\nuint8 [0 1 2 3 4]\n\n\n\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\n\nint16 [ 5  7  9 11 13]\n\n\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\n\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\n\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#ndarray-메서드",
    "href": "data_mining/numpy/numpy.html#ndarray-메서드",
    "title": "Numpy",
    "section": "ndarray 메서드",
    "text": "ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) # 각 행끼리 더하고 두개의 열의 값을 더한 후 원소의 개수로 나눠줌\nprint(a)\nprint(\"평균 =\", a.mean())\n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = 6.766666666666667\n\n\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\n\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var): # std 표준편차\n    print(func.__name__, \"=\", func())\n\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n\n\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\n\nc=np.arange(24).reshape(2,3,4)\nc\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\n\n\n\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\n\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\n\n\n여러 축에 대해서 더할 수도 있습니다:\n\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\n\narray([ 60,  92, 124])\n\n\n\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n\n(60, 92, 124)"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#일반-함수",
    "href": "data_mining/numpy/numpy.html#일반-함수",
    "title": "Numpy",
    "section": "일반 함수",
    "text": "일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a) # square는 제곱\n\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n\n\n다음은 유용한 단항 일반 함수들입니다:\n\nprint(\"원본 ndarray\")\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a)) # abs는 절댓값을 반환\n\n원본 ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n\nRuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\n<ipython-input-59-d791c8e37e6f>:5: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#이항-일반-함수",
    "href": "data_mining/numpy/numpy.html#이항-일반-함수",
    "title": "Numpy",
    "section": "이항 일반 함수",
    "text": "이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다:\n\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\n\narray([ 3,  6,  2, 11])\n\n\n\nnp.greater(a, b)  # a > b 와 동일\n\narray([False, False,  True, False])\n\n\n\nnp.maximum(a, b)\n\narray([2, 8, 3, 7])\n\n\n\nnp.copysign(a, b)\n\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#차원-배열",
    "href": "data_mining/numpy/numpy.html#차원-배열",
    "title": "Numpy",
    "section": "1차원 배열",
    "text": "1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\n\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n\n19\n\n\n\na[2:5]\n\narray([ 3, 19, 13])\n\n\n\na[2:-1]\n\narray([ 3, 19, 13,  7])\n\n\n\na[:2]\n\narray([1, 5])\n\n\n\na[2::2]\n\narray([ 3, 13,  3])\n\n\n\na[::-1]\n\narray([ 3,  7, 13, 19,  3,  5,  1])\n\n\n물론 원소를 수정할 수 있죠:\n\na[3]=999\na\n\narray([  1,   5,   3, 999,  13,   7,   3])\n\n\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\n\na[2:5] = [997, 998, 999]\na\n\narray([  1,   5, 997, 998, 999,   7,   3])\n\n\n\nQuiz\n\narray_2d = np.array([[5, 10, 15],\n                     [20, 25, 30],\n                     [35, 40, 45]])\n\n\n2차원 배열 ’array_2d’에서 첫 번째 행(row)의 모든 요소를 선택해 보세요.\n힌트: 인덱싱을 사용하여 첫 번째 행을 선택할 수 있습니다.\n\n\narray_2d[1,]\n\narray([20, 25, 30])\n\n\n\n2차원 배열 ’array_2d’에서 두 번째 열(column)의 모든 요소를 선택해 보세요.\n힌트: 인덱싱과 슬라이싱을 사용하여 두 번째 열을 선택할 수 있습니다.\n\n\narray_2d[:,2]\n\narray([15, 30, 45])\n\n\n\n2차원 배열 ’array_2d’에서 다음 요소들을 선택해 보세요: 25,30,45\n힌트: 팬시 인데싱(Fancy indexing)을 사용하여 여러 요소를 한 번에 선택할 수 있습니다.\n\n\narray_2d[(1,1,2),(1,2,2)]\n\narray([25, 30, 45])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#보통의-파이썬-배열과-차이점",
    "href": "data_mining/numpy/numpy.html#보통의-파이썬-배열과-차이점",
    "title": "Numpy",
    "section": "보통의 파이썬 배열과 차이점",
    "text": "보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\n\na[2:5] = -1\na\n\narray([ 1,  5, -1, -1, -1,  7,  3])\n\n\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다:\n\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\n\ncannot copy sequence with size 6 to array axis with dimension 3\n\n\n원소를 삭제할 수도 없습니다:\n\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\n\ncannot delete array elements\n\n\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\n\na_slice = a[2:6]\na_slice[1] = 1000\na  # 원본 배열이 수정됩니다!\n\narray([   1,    5,   -1, 1000,   -1,    7,    3])\n\n\n\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다!\n\narray([  -1, 2000,   -1,    7])\n\n\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\n\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\na  # 원본 배열이 수정되지 않습니다\n\narray([   1,    5,   -1, 2000,   -1,    7,    3])\n\n\n\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다\n\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#다차원-배열",
    "href": "data_mining/numpy/numpy.html#다차원-배열",
    "title": "Numpy",
    "section": "다차원 배열",
    "text": "다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nb[1, 2]  # 행 1, 열 2\n\n14\n\n\n\nb[1, :]  # 행 1, 모든 열\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[:, 1]  # 모든 행, 열 1\n\narray([ 1, 13, 25, 37])\n\n\n주의: 다음 두 표현에는 미묘한 차이가 있습니다:\n\nb[1, :]\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[1:2, :]\n\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n\nb[1, :].shape\n\n(12,)\n\n\n\nb[1:2, :].shape\n\n(1, 12)\n\n\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "data_mining/numpy/numpy.html#팬시-인덱싱fancy-indexing",
    "href": "data_mining/numpy/numpy.html#팬시-인덱싱fancy-indexing",
    "title": "Numpy",
    "section": "팬시 인덱싱(Fancy indexing)",
    "text": "팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\n\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nb[(0,2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\n\narray([[ 2,  3,  4],\n       [26, 27, 28]])\n\n\n\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\n\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n\n\n\nb[2:, 0:2]\n\narray([[24, 25],\n       [36, 37]])\n\n\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\n\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\n\narray([41, 33, 37, 33])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#고차원",
    "href": "data_mining/numpy/numpy.html#고차원",
    "title": "Numpy",
    "section": "고차원",
    "text": "고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다:\n\nc = b.reshape(4,2,6)\nc\n\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\n\n\n\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n\n34\n\n\n\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\n\narray([27, 33])\n\n\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\n\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\n\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#생략-부호-...",
    "href": "data_mining/numpy/numpy.html#생략-부호-...",
    "title": "Numpy",
    "section": "생략 부호 (...)",
    "text": "생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\n\nc[2, ...]  #  행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\n\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n\n\n\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\n\narray([30, 31, 32, 33, 34, 35])\n\n\n\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\n\narray([27, 33])\n\n\n\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\n\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#불리언-인덱싱",
    "href": "data_mining/numpy/numpy.html#불리언-인덱싱",
    "title": "Numpy",
    "section": "불리언 인덱싱",
    "text": "불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # 행 0과 2, 모든 열. b[(0, 2), :]와 동일\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n\n\n\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\n\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#np.ix_",
    "href": "data_mining/numpy/numpy.html#np.ix_",
    "title": "Numpy",
    "section": "np.ix_",
    "text": "np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다:\n\nb[np.ix_(rows_on, cols_on)]\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\nnp.ix_(rows_on, cols_on)\n\n(array([[0],\n        [2]]),\n array([[ 1,  4,  7, 10]]))\n\n\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다:\n\nb[b % 3 == 1]\n\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#vstack",
    "href": "data_mining/numpy/numpy.html#vstack",
    "title": "Numpy",
    "section": "vstack",
    "text": "vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\n\nq4 = np.vstack((q1, q2, q3))\nq4\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nnp.vstack((q1[0,...],q2[0,...],q3[0,...]))\n\narray([[1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.]])\n\n\n\nq4.shape\n\n(10, 4)\n\n\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다)."
  },
  {
    "objectID": "data_mining/numpy/numpy.html#hstack",
    "href": "data_mining/numpy/numpy.html#hstack",
    "title": "Numpy",
    "section": "hstack",
    "text": "hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\n\nq5 = np.hstack((q1, q3))\nq5\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\nq5.shape\n\n(3, 8)\n\n\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3만 수평으로 쌓을 수 없습니다:\n\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\n\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#concatenate",
    "href": "data_mining/numpy/numpy.html#concatenate",
    "title": "Numpy",
    "section": "concatenate",
    "text": "concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\n\nq1.shape, q2.shape, q3.shape\n\n((3, 4), (4, 4), (3, 4))\n\n\n\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq7.shape\n\n(10, 4)\n\n\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다.\n\nq5\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\nnp.concatenate((q1, q2[0:3,:], q3), axis = 1)\n\narray([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#stack",
    "href": "data_mining/numpy/numpy.html#stack",
    "title": "Numpy",
    "section": "stack",
    "text": "stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\n\nq8 = np.stack((q1, q3))\nq8\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\n\n\n\nq8.shape\n\n(2, 3, 4)"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#행렬-전치",
    "href": "data_mining/numpy/numpy.html#행렬-전치",
    "title": "Numpy",
    "section": "행렬 전치",
    "text": "행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다:\n\nm1 = np.arange(10).reshape(2,5)\nm1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nm1.T\n\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\n\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다:\n\nm2 = np.arange(5)\nm2\n\narray([0, 1, 2, 3, 4])\n\n\n\nm2.T\n\narray([0, 1, 2, 3, 4])\n\n\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다:\n\nm2r = m2.reshape(1,5)\nm2r\n\narray([[0, 1, 2, 3, 4]])\n\n\n\nm2r.T\n\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#행렬-곱셈",
    "href": "data_mining/numpy/numpy.html#행렬-곱셈",
    "title": "Numpy",
    "section": "행렬 곱셈",
    "text": "행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠.\n\nn1 = np.arange(10).reshape(2, 5)\nn1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nn2 = np.arange(15).reshape(5,3)\nn2\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\n\n\n\nn1.dot(n2)\n\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n\n\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "data_mining/numpy/numpy.html#역행렬과-유사-역행렬",
    "href": "data_mining/numpy/numpy.html#역행렬과-유사-역행렬",
    "title": "Numpy",
    "section": "역행렬과 유사 역행렬",
    "text": "역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다:\n\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\n\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\n\n\n\nlinalg.inv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\n\n\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다:\n\nlinalg.pinv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#단위-행렬",
    "href": "data_mining/numpy/numpy.html#단위-행렬",
    "title": "Numpy",
    "section": "단위 행렬",
    "text": "단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다):\n\nm3.dot(linalg.inv(m3))\n\narray([[ 1.00000000e+00, -1.66533454e-16,  0.00000000e+00],\n       [ 6.31439345e-16,  1.00000000e+00, -1.38777878e-16],\n       [ 5.21110932e-15, -2.38697950e-15,  1.00000000e+00]])\n\n\neye 함수는 NxN 크기의 단위 행렬을 만듭니다:\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#qr-분해",
    "href": "data_mining/numpy/numpy.html#qr-분해",
    "title": "Numpy",
    "section": "QR 분해",
    "text": "QR 분해\nqr 함수는 행렬을 QR 분해합니다:\n\nq, r = linalg.qr(m3)\nq\n\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\n\n\n\nr\n\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\n\n\n\nq.dot(r)  # q.r는 m3와 같습니다\n\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#행렬식",
    "href": "data_mining/numpy/numpy.html#행렬식",
    "title": "Numpy",
    "section": "행렬식",
    "text": "행렬식\ndet 함수는 행렬식을 계산합니다:\n\nlinalg.det(m3)  # 행렬식 계산\n\n43.99999999999997"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#고윳값과-고유벡터",
    "href": "data_mining/numpy/numpy.html#고윳값과-고유벡터",
    "title": "Numpy",
    "section": "고윳값과 고유벡터",
    "text": "고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다:\n\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # λ\n\narray([42.26600592, -0.35798416, -2.90802176])\n\n\n\neigenvectors # v\n\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\n\n\n\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\n\narray([[ 8.88178420e-15,  2.22044605e-16, -3.10862447e-15],\n       [ 3.55271368e-15,  2.02615702e-15, -1.11022302e-15],\n       [ 3.55271368e-14,  3.33413852e-15, -8.43769499e-15]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#특잇값-분해",
    "href": "data_mining/numpy/numpy.html#특잇값-분해",
    "title": "Numpy",
    "section": "특잇값 분해",
    "text": "특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다:\n\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\n\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\n\n\n\nU, S_diag, V = linalg.svd(m4)\nU\n\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\n\n\n\nS_diag\n\narray([3.        , 2.23606798, 2.        , 0.        ])\n\n\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다:\n\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\n\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\n\n\n\nV\n\narray([[-0.        ,  0.        ,  1.        , -0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        , -0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\n\n\n\nU.dot(S).dot(V) # U.Σ.V == m4\n\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#대각원소와-대각합",
    "href": "data_mining/numpy/numpy.html#대각원소와-대각합",
    "title": "Numpy",
    "section": "대각원소와 대각합",
    "text": "대각원소와 대각합\n\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\n\narray([ 1,  7, 31])\n\n\n\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n\n39"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#선형-방정식-풀기",
    "href": "data_mining/numpy/numpy.html#선형-방정식-풀기",
    "title": "Numpy",
    "section": "선형 방정식 풀기",
    "text": "선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\n\narray([-3.,  2.])\n\n\nsolution을 확인해 보죠:\n\ncoeffs.dot(solution), depvars  # 네 같네요\n\n(array([ 6., -9.]), array([ 6, -9]))\n\n\n좋습니다! 다른 방식으로도 solution을 확인해 보죠:\n\nnp.allclose(coeffs.dot(solution), depvars)\n\nTrue"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#바이너리-.npy-포맷",
    "href": "data_mining/numpy/numpy.html#바이너리-.npy-포맷",
    "title": "Numpy",
    "section": "바이너리 .npy 포맷",
    "text": "바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 보죠.\n\na = np.random.rand(2,3)\na\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])\n\n\n\nnp.save(\"my_array\", a)\n\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다:\n\nwith open(\"my_array.npy\", \"rb\") as f:\n    content = f.read()\n\ncontent\n\nb\"\\x93NUMPY\\x01\\x00v\\x00{'descr': '<f8', 'fortran_order': False, 'shape': (2, 3), }                                                          \\nY\\xc1\\xfc\\xd0\\x1ee\\xe1?\\xde{3\\t?\\xb9\\xed?\\x80V\\x08\\xef\\xa5p\\x8f?\\x96I}\\xe0J\\x9b\\xda?\\xe0U\\xfaav \\xed?\\xd8\\xe50\\xc59\\xa4\\xe1?\"\n\n\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다:\n\na_loaded = np.load(\"my_array.npy\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#텍스트-포맷",
    "href": "data_mining/numpy/numpy.html#텍스트-포맷",
    "title": "Numpy",
    "section": "텍스트 포맷",
    "text": "텍스트 포맷\n배열을 텍스트 포맷으로 저장해 보죠:\n\nnp.savetxt(\"my_array.csv\", a)\n\n파일 내용을 확인해 보겠습니다:\n\nwith open(\"my_array.csv\", \"rt\") as f:\n    print(f.read())\n\n5.435937959464737235e-01 9.288630656918674955e-01 1.535157809943688001e-02\n4.157283012656532994e-01 9.102126992826775620e-01 5.512970782648904944e-01\n\n\n\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다:\n\nnp.savetxt(\"my_array.csv\", a, delimiter=\",\")\n\n이 파일을 로드하려면 loadtxt 함수를 사용합니다:\n\na_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "data_mining/numpy/numpy.html#압축된-.npz-포맷",
    "href": "data_mining/numpy/numpy.html#압축된-.npz-포맷",
    "title": "Numpy",
    "section": "압축된 .npz 포맷",
    "text": "압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다:\n\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\n\n\n\nnp.savez(\"my_arrays\", my_a=a, my_b=b)\n\n파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다.\n\nwith open(\"my_arrays.npz\", \"rb\") as f:\n    content = f.read()\n\nrepr(content)[:180] + \"[...]\"\n\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x00\\\\x063\\\\xcf\\\\xb9\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x[...]'\n\n\n다음과 같이 이 파일을 로드할 수 있습니다:\n\nmy_arrays = np.load(\"my_arrays.npz\")\nmy_arrays\n\n<numpy.lib.npyio.NpzFile at 0x7f9791c73d60>\n\n\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\n\nmy_arrays.keys()\n\nKeysView(<numpy.lib.npyio.NpzFile object at 0x7f9791c73d60>)\n\n\n\nmy_arrays[\"my_a\"]\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "data_mining/kiva/kiva.html",
    "href": "data_mining/kiva/kiva.html",
    "title": "Your First Map (kiva)",
    "section": "",
    "text": "Your First Map Exercise, Kaggle\n\n\nIntroduction\nKiva.org는 전 세계 가난한 사람들에게 금융 서비스를 제공하는 온라인 크라우드 펀딩 플랫폼입니다. Kiva 대출 기관은 200만 명이 넘는 사람들에게 10억 달러 이상의 대출을 제공했습니다.\n\n\n\nKiva는 “Field Partners”의 글로벌 네트워크를 통해 세계에서 가장 외진 곳에 도달합니다. 이러한 파트너는 차용자를 심사하고 서비스를 제공하며 대출을 관리하기 위해 지역 사회에서 일하는 지역 조직입니다\n이 실습에서는 필리핀의 Kiva 대출을 조사합니다. 새로운 현장 파트너를 모집할 기회를 식별하기 위해 Kiva의 현재 네트워크 외부에 있을 수 있는 지역을 식별할 수 있습니까?\n시작하려면 아래 코드 셀을 실행하여 피드백 시스템을 설정하십시오.\n\n\nReading data\n\nimport pandas as pd\nimport geopandas as gpd\n\n\n1) Get the data.\n다음 셀을 사용하여 loans_filepath에 있는 shapefile을 로드하여 GeoDataFrame world_loans를 생성합니다.\n\nworld_loans = gpd.read_file(\"/Users/jungwoolee/Desktop/college/data mining/geo_data/kiva_loans/kiva_loans/kiva_loans.shp\")\n\n# Check your answer\n# Uncomment to view the first five rows of the data\nworld_loans.head()\n\n\n\n\n\n  \n    \n      \n      Partner ID\n      Field Part\n      sector\n      Loan Theme\n      country\n      amount\n      geometry\n    \n  \n  \n    \n      0\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      450\n      POINT (102.89751 13.66726)\n    \n    \n      1\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      20275\n      POINT (102.98962 13.02870)\n    \n    \n      2\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      9150\n      POINT (102.98962 13.02870)\n    \n    \n      3\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      604950\n      POINT (105.31312 12.09829)\n    \n    \n      4\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Sanitation\n      Cambodia\n      275\n      POINT (105.31312 12.09829)\n    \n  \n\n\n\n\n\n\n2) Plot the data.\n변경 없이 다음 코드 셀을 실행하여 국가 경계가 포함된 GeoDataFrame world를 로드합니다\n\n# This dataset is provided in GeoPandas\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)\nworld.head()\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      0\n      889953.0\n      Oceania\n      Fiji\n      FJI\n      5496\n      MULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n    \n    \n      1\n      58005463.0\n      Africa\n      Tanzania\n      TZA\n      63177\n      POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n    \n    \n      2\n      603253.0\n      Africa\n      W. Sahara\n      ESH\n      907\n      POLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n    \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n    \n      4\n      328239523.0\n      North America\n      United States of America\n      USA\n      21433226\n      MULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n    \n  \n\n\n\n\nworld 및 world_loans GeoDataFrames를 사용하여 전 세계 Kiva 대출 위치를 시각화합니다.\n\nworld_loans.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nax = world.plot(figsize=(16, 8), color = 'whitesmoke', linestyle = ':', edgecolor = 'black')\nworld_loans.plot(ax=ax, markersize=2)\n\n<AxesSubplot:>\n\n\n\n\n\n\n# 다른 방법\nfig, ax = plt.subplots(figsize = (16, 8))\nworld.plot(ax = ax)\nworld_loans.plot(ax = ax, color = \"C1\")\nfig.tight_layout()\n\n\n\n\n\n\n3) Select loans based in the Philippines.\n다음으로 필리핀에 기반을 둔 대출에 중점을 둘 것입니다. 다음 코드 셀을 사용하여 필리핀에 기반을 둔 대출이 있는 world_loans의 모든 행을 포함하는 GeoDataFrame PHL_loans를 생성합니다.\n\nPHL_loans = world_loans.loc[world_loans[\"country\"] == \"Philippines\"]\nPHL_loans.head()\n\n\n\n\n\n  \n    \n      \n      Partner ID\n      Field Part\n      sector\n      Loan Theme\n      country\n      amount\n      geometry\n    \n  \n  \n    \n      2859\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.73961 17.64228)\n    \n    \n      2860\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.74169 17.63235)\n    \n    \n      2861\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.46667 16.60000)\n    \n    \n      2862\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      6050\n      POINT (121.73333 17.83333)\n    \n    \n      2863\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      625\n      POINT (121.51800 16.72368)\n    \n  \n\n\n\n\n\n\n4) Understand loans in the Philippines.\n변경 없이 다음 코드 셀을 실행하여 필리핀의 모든 섬에 대한 경계가 포함된 GeoDataFrame PHL을 로드합니다.\n\n# Load a KML file containing island boundaries\ngpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\nPHL = gpd.read_file(\"/Users/jungwoolee/Desktop/college/data mining/geo_data/Philippines_AL258.kml\", driver='KML')\nPHL.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      geometry\n    \n  \n  \n    \n      0\n      Autonomous Region in Muslim Mindanao\n      \n      MULTIPOLYGON (((119.46690 4.58718, 119.46653 4...\n    \n    \n      1\n      Bicol Region\n      \n      MULTIPOLYGON (((124.04577 11.57862, 124.04594 ...\n    \n    \n      2\n      Cagayan Valley\n      \n      MULTIPOLYGON (((122.51581 17.04436, 122.51568 ...\n    \n    \n      3\n      Calabarzon\n      \n      MULTIPOLYGON (((120.49202 14.05403, 120.49201 ...\n    \n    \n      4\n      Caraga\n      \n      MULTIPOLYGON (((126.45401 8.24400, 126.45407 8...\n    \n  \n\n\n\n\nPHL 및 PHL_loans GeoDataFrames를 사용하여 필리핀의 대출을 시각화합니다.\n\nax = PHL.plot(figsize =(8, 16), color = 'whitesmoke', linestyle = ':', edgecolor = 'black')\nPHL_loans.plot(ax = ax, markersize = 2)\n\n<AxesSubplot:>\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize = (8, 16))\nPHL.plot(ax = ax)\nPHL_loans.plot(ax = ax, color = \"C1\")\nfig.tight_layout()\n\n\n\n\n새로운 현장 파트너를 모집하는 데 유용할 수 있는 섬을 식별할 수 있습니까? 현재 Kiva의 손이 닿지 않는 섬이 있습니까?\n질문에 답하는 데 이 지도가 유용할 수 있습니다."
  },
  {
    "objectID": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html",
    "href": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html",
    "title": "seaborn and matplotlib",
    "section": "",
    "text": "seaborn and matplotlib 기본 코드\n\n\nMatplotlib보다 더 다양하고 고급 시각화 기능 제공\n다양한 색상 팔레트 제공\n통계 그래프를 쉽게 그릴 수 있도록 지원\n데이터 시각화에 효과적인 기능 제공"
  },
  {
    "objectID": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#load-data",
    "href": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#load-data",
    "title": "seaborn and matplotlib",
    "section": "1.1 Load data",
    "text": "1.1 Load data\n\n예제로 사용할 펭귄 데이터를 불러옵니다.\nseaborn에 내장되어 있습니다.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female"
  },
  {
    "objectID": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#figure-and-axes",
    "href": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#figure-and-axes",
    "title": "seaborn and matplotlib",
    "section": "1.2 Figure and Axes",
    "text": "1.2 Figure and Axes\n\nmatplotlib으로 도화지figure를 깔고 축공간axes를 만듭니다.\n1 x 2 축공간을 구성합니다.\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\n\nfig.tight_layout()"
  },
  {
    "objectID": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#plot-with-matplotlib",
    "href": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#plot-with-matplotlib",
    "title": "seaborn and matplotlib",
    "section": "1.3 plot with matplotlib",
    "text": "1.3 plot with matplotlib\n\nmatplotlib 기능을 이용해서 산점도를 그립니다.\n\nx축은 부리 길이 bill length\ny축은 부리 위 아래 두께 bill depth\n색상은 종species로 합니다.\nAdelie, Chinstrap, Gentoo이 있습니다.\n\n두 축공간 중 왼쪽에만 그립니다.\n\n컬러를 다르게 주기 위해 f-string 포맷을 사용했습니다.\nf-string 포맷에 대한 설명은 https://blockdmask.tistory.com/429를 참고하세요\n\nfig, axes = plt.subplots(ncols = 2,figsize = (8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"] == s], # \n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"] == s],\n                    c = f\"C{i}\", label = s, alpha = 0.3)\n    \naxes[0].legend(species_u, title = \"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plt.show()\nfig.tight_layout()\n\n\n\n\n조금 더 간단히 그리는 방법\nmatplotlib는 기본적으로 Categorical 변수를 color로 바로 사용하지 못함\n\n# We transform text categorical variables into numerical variables\npenguins[\"species_codes\"] = pd.Categorical(penguins[\"species\"]).codes # 숫자로 바꾸고 넣어줌\n\nfig, axes = plt.subplots(ncols = 2, figsize = (8,4))\n\naxes[0].scatter(data = penguins, x = \"bill_length_mm\", y = \"bill_depth_mm\", c = \"species_codes\", alpha = 0.3)\n\n<matplotlib.collections.PathCollection at 0x7f7a78e94250>"
  },
  {
    "objectID": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#plot-with-seaborn",
    "href": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#plot-with-seaborn",
    "title": "seaborn and matplotlib",
    "section": "1.4 Plot with seaborn",
    "text": "1.4 Plot with seaborn\n\nfig, axes = plt.subplots(ncols = 2,figsize = (8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"] == s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"] == s],\n                    c = f\"C{i}\", label = s, alpha = 0.3)\n    \naxes[0].legend(species_u, title = \"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n\n# plot 1 : seaborn\nsns.scatterplot(x = \"bill_length_mm\", y = \"bill_depth_mm\", hue = \"species\", data=penguins, alpha = 0.3, ax = axes[1]) # hue = color\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\n\n\n단 세 줄로 거의 동일한 그림이 나왔습니다.\n\nscatter plot의 점 크기만 살짝 작습니다.\nlabel의 투명도만 살짝 다릅니다.\n\nseaborn 명령 scatterplot()을 그대로 사용했습니다.\nx축과 y축 label도 바꾸었습니다.\n\nax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다.\nmatplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다."
  },
  {
    "objectID": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#matplotlib-seaborn-seaborn-matplotlib",
    "href": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#matplotlib-seaborn-seaborn-matplotlib",
    "title": "seaborn and matplotlib",
    "section": "1.5 matplotlib + seaborn & seaborn + matplotlib",
    "text": "1.5 matplotlib + seaborn & seaborn + matplotlib\n\nmatplotlib과 seaborn이 자유롭게 섞일 수 있습니다.\n\nmatplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고,\nseaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다.\n\n파이썬 코드는 다음과 같습니다.\n\n\nfig, axes = plt.subplots(ncols = 2, figsize = (8, 4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib + seaborn\nfor i, s in enumerate(species_u):\n    # matplotlib 산점도\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"] == s],\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"] == s],\n                   c = f\"C{i}\", label = s, alpha = 0.3\n                  )\n                  \n    # seaborn 추세선\n    sns.regplot(x = \"bill_length_mm\", y = \"bill_depth_mm\", data = penguins.loc[penguins[\"species\"] == s], \n                scatter = False, ax = axes[0])\n    \naxes[0].legend(species_u, title = \"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plot 1 : seaborn + matplotlib\n# seaborn 산점도\nsns.scatterplot(x = \"bill_length_mm\", y = \"bill_depth_mm\", hue = \"species\", data = penguins, alpha = 0.3, ax = axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfor i, s in enumerate(species_u):\n    # matplotlib 중심점\n    axes[1].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"] == s].mean(),\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"] == s].mean(),\n                   c = f\"C{i}\", alpha = 1, marker = \"x\", s = 100\n                  )\n\nfig.tight_layout()\n\n\n\n\n\nsns.scatterplot(data = penguins, x = 'bill_length_mm', y = 'bill_depth_mm', hue = 'species')\n\nmean_values = penguins.gropupby('species').mean().reset_index(drop = false)\nfor i, row in mean_values.iterrows():\n  plt.scatter(row['bill_length_mm'], row['bill_length_mm'], marker = 'X', s = 100, linewidths = 2, edgecolors = 'black')\n\n# set x and y axis labels\nplt.xlabel('bill length (mm)')\nplt.ylabel('bill depth (mm)')\n\nplt.show()\n\nAttributeError: ignored"
  },
  {
    "objectID": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#seaborn-seaborn-matplotlib",
    "href": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#seaborn-seaborn-matplotlib",
    "title": "seaborn and matplotlib",
    "section": "1.6 seaborn + seaborn + matplotlib",
    "text": "1.6 seaborn + seaborn + matplotlib\n\n안 될 이유가 없습니다.\nseaborn scatterplot + seaborn kdeplot + matplotlib text입니다\n\n\nfig, ax = plt.subplots(figsize=(6,5))\n\n# plot 0: scatter plot\nsns.scatterplot(x = \"bill_length_mm\", y = \"bill_depth_mm\", color = \"k\", data = penguins, alpha = 0.3, ax = ax, legend = False)\n\n# plot 1: kde plot\nsns.kdeplot(x = \"bill_length_mm\", y = \"bill_depth_mm\", hue = \"species\", data = penguins, alpha = 0.5, ax = ax, legend = False)\n\n# text:\nspecies_u = penguins[\"species\"].unique()\nfor i, s in enumerate(species_u):\n    ax.text(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n            penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n            s = s, fontdict={\"fontsize\":14, \"fontweight\":\"bold\",\"color\":\"k\"}\n            )\n\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()"
  },
  {
    "objectID": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#quiz",
    "href": "data_mining/seaborn & matplotlib/seaborn and matplotlib.html#quiz",
    "title": "seaborn and matplotlib",
    "section": "Quiz",
    "text": "Quiz\nBill length를 10단위로 나눈 후, bill length에 따른 Bill depth의 boxplot을 그려봅시다.\n\n# bill_length_mm 변수를 10으로 나누어서 새로운 열(column)을 추가합니다.\npenguins['bill_length_10'] = (penguins['bill_length_mm'] // 10) * 10 \n\nsns.boxplot(x = 'bill_length_10', y = 'bill_depth_mm', data = penguins)\nsns.stripplot(x = 'bill_length_10', y = 'bill_depth_mm', data = penguins, color = 'black', size = 4)\n\nsns.set_style('whitegrid')\n# sns.despine()\n\nplt.show()\n\n\n\n\npd.cut을 이용한 방법\n\n# bill_length_mm을 구간별로 나누어서 범주형 열을 생성합니다.\npenguins['bill_length_group'] = pd.cut(penguins['bill_length_mm'],\n                                       bins = [0, 40, 50, 60],\n                                       labels = ['0-40', '40-50','50-60'])\n\nsns.boxplot(x = 'bill_length_group', y = 'bill_depth_mm', data = penguins)\nsns.stripplot(x = 'bill_length_group', y = 'bill_depth_mm', data = penguins, color = 'black', size = 4)\n\nsns.set_style('whitegrid')\nsns.despine()\n\nplt.show()\n\n\n\n\n\nsns.scatterplot(x = 'bill_length_mm', y = 'bill_depth_mm', hue = 'species', data = penguins, alpha = 0.3)\n\nplt.show()\n\n\n\n\n\ng = sns.FacetGrid(penguins, col = 'species', col_wrap = 4)\ng.map(sns.boxplot, 'bill_length_group', 'bill_depth_mm',order = ['0-40', '40-50','50-60'])\ng.map(sns.stripplot, 'bill_length_group', 'bill_depth_mm',  color = 'black', size = 4, order = ['0-40', '40-50','50-60'])\n\nsns.set_style('whitegrid')\nsns.despine()\n\nplt.show()\n\n\n\n\n\ng = sns.FacetGrid(penguins, col = 'species', col_wrap = 3)\ng.map(sns.scatterplot, 'bill_length_mm', 'bill_depth_mm')\n\nsns.set_style('whitegrid')\nsns.despine()\n\nplt.show()\n\n\n\n\n\ng = sns.FacetGrid(penguins, col = 'species', col_wrap = 3, hue = 'species')\ng.map(sns.scatterplot, 'bill_length_mm', 'bill_depth_mm')\n\nsns.set_style('whitegrid')\nsns.despine()\n\nplt.show()"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html",
    "href": "data_mining/pandas/week_1b_pandas.html",
    "title": "Pandas",
    "section": "",
    "text": "“pandas 기본 코드 실습(한글)”\n\n\ntoc:true\nbranch: master\nbadges: true\ncomments: true\nauthor: Jungwoo Lee\ncategories: [jupyter, python]\n\n도구 - 판다스(pandas)\npandas 라이브러리는 사용하기 쉬운 고성능 데이터 구조와 데이터 분석 도구를 제공합니다. 주 데이터 구조는 DataFrame입니다. 이를 인-메모리(in-memory) 2D 테이블로 생각할 수 있습니다(열 이름과 행 레이블이 있는 스프레드시트와 비슷합니다). 엑셀에 있는 많은 기능을 프로그램에서 사용할 수 있습니다. 여기에는 피봇 테이블이나 다른 열을 기반으로 열을 계산하고 그래프 출력하는 기능 등이 포함됩니다. 열 값으로 행을 그룹핑할 수도 있습니다. 또한 SQL과 비슷하게 테이블을 조인할 수 있습니다. 판다스는 시계열 데이터를 다루는데도 뛰어납니다.\n필요 라이브러리:\n\n넘파이(NumPy) – 넘파이에 익숙하지 않다면 지금 넘파이 튜토리얼을 둘러 보세요.\n\n\n\n\n구글 코랩에서 실행하기"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#series-만들기",
    "href": "data_mining/pandas/week_1b_pandas.html#series-만들기",
    "title": "Pandas",
    "section": "Series 만들기",
    "text": "Series 만들기\n첫 번째 Series 객체를 만들어 보죠!\n\nimport numpy as np\nnp.array([2,-1,3,5])\n\narray([ 2, -1,  3,  5])\n\n\n\ns = pd.Series([2,-1,3,5])\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#d-ndarray와-비슷합니다",
    "href": "data_mining/pandas/week_1b_pandas.html#d-ndarray와-비슷합니다",
    "title": "Pandas",
    "section": "1D ndarray와 비슷합니다",
    "text": "1D ndarray와 비슷합니다\nSeries 객체는 넘파이 ndarray와 비슷하게 동작합니다. 넘파이 함수에 매개변수로 종종 전달할 수 있습니다:\n\nimport numpy as np\nnp.exp(s) # 지수함수\n\n0      7.389056\n1      0.367879\n2     20.085537\n3    148.413159\ndtype: float64\n\n\nSeries 객체에 대한 산술 연산도 가능합니다. ndarray와 비슷하게 원소별로 적용됩니다:\n\ns + [1000,2000,3000,4000]\n\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n\n\n넘파이와 비슷하게 Series에 하나의 숫자를 더하면 Series에 있는 모든 원소에 더해집니다. 이를 브로드캐스팅(broadcasting)이라고 합니다:\n\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64\n\n\n\ns + 1000\n\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n\n\n*나 / 같은 모든 이항 연산과 심지어 조건 연산에서도 마찬가지입니다:\n\ns < 0\n\n0    False\n1     True\n2    False\n3    False\ndtype: bool"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#인덱스-레이블",
    "href": "data_mining/pandas/week_1b_pandas.html#인덱스-레이블",
    "title": "Pandas",
    "section": "인덱스 레이블",
    "text": "인덱스 레이블\nSeries 객체에 있는 각 원소는 인덱스 레이블(index label)이라 불리는 고유한 식별자를 가지고 있습니다. 기본적으로 Series에 있는 원소의 순서입니다(0에서 시작합니다). 하지만 수동으로 인덱스 레이블을 지정할 수도 있습니다:\n\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n그다음 dict처럼 Series를 사용할 수 있습니다:\n\ns2[\"bob\"]\n\n83\n\n\n일반 배열처럼 정수 인덱스를 사용하여 계속 원소에 접근할 수 있습니다:\n\ns2[1]\n\n83\n\n\n레이블이나 정수를 사용해 접근할 때 명확하게 하기 위해 레이블은 loc 속성을 사용하고 정수는 iloc 속성을 사용하는 것이 좋습니다:\n\ns2.loc[\"bob\"]\n\n83\n\n\n\ns2.iloc[1]\n\n83\n\n\nSeries는 인덱스 레이블을 슬라이싱할 수도 있습니다:\n\ns2.iloc[1:3]\n\nbob         83\ncharles    112\ndtype: int64\n\n\n기본 정수 레이블을 사용할 때 예상 외의 결과를 만들 수 있기 때문에 주의해야 합니다:\n\nsurprise = pd.Series([1000, 1001, 1002, 1003])\nsurprise\n\n0    1000\n1    1001\n2    1002\n3    1003\ndtype: int64\n\n\n\nsurprise_slice = surprise[2:]\nsurprise_slice\n\n2    1002\n3    1003\ndtype: int64\n\n\n보세요. 첫 번째 원소의 인덱스 레이블이 2입니다. 따라서 슬라이싱 결과에서 인덱스 레이블 0인 원소는 없습니다:\n\ntry:\n    surprise_slice[0]\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 0\n\n\n하지만 iloc 속성을 사용해 정수 인덱스로 원소에 접근할 수 있습니다. Series 객체를 사용할 때 loc와 iloc를 사용하는 것이 좋은 이유입니다:\n\nsurprise_slice.iloc[0]\n\n1002\n\n\n\nsurprise_slice.loc[2]\n\n1002"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#dict에서-초기화",
    "href": "data_mining/pandas/week_1b_pandas.html#dict에서-초기화",
    "title": "Pandas",
    "section": "dict에서 초기화",
    "text": "dict에서 초기화\ndict에서 Series 객체를 만들 수 있습니다. 키는 인덱스 레이블로 사용됩니다:\n\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\ns3 = pd.Series(weights)\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\nSeries에 포함할 원소를 제어하고 index를 지정하여 명시적으로 순서를 결정할 수 있습니다:\n\ns4 = pd.Series(weights, index = [\"colin\", \"alice\"])\ns4\n\ncolin    86\nalice    68\ndtype: int64"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#자동-정렬",
    "href": "data_mining/pandas/week_1b_pandas.html#자동-정렬",
    "title": "Pandas",
    "section": "자동 정렬",
    "text": "자동 정렬\n여러 개의 Series 객체를 다룰 때 pandas는 자동으로 인덱스 레이블에 따라 원소를 정렬합니다.\n\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\n\nprint(s2.keys())\nprint(s3.keys())\n\ns2 + s3\n\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n\n\n만들어진 Series는 s2와 s3의 인덱스 레이블의 합집합을 담고 있습니다. s2에 \"colin\"이 없고 s3에 \"charles\"가 없기 때문에 이 원소는 NaN 값을 가집니다(Not-a-Number는 누락이란 의미입니다).\n자동 정렬은 구조가 다르고 누락된 값이 있는 여러 데이터를 다룰 때 매우 편리합니다. 하지만 올바른 인덱스 레이블을 지정하는 것을 잊는다면 원치않는 결과를 얻을 수 있습니다:\n\ns5 = pd.Series([1000,1000,1000,1000])\nprint(\"s2 =\", s2.values)\nprint(\"s5 =\", s5.values)\n\ns2 + s5\n\ns2 = [ 68  83 112  68]\ns5 = [1000 1000 1000 1000]\n\n\nalice     NaN\nbob       NaN\ncharles   NaN\ndarwin    NaN\n0         NaN\n1         NaN\n2         NaN\n3         NaN\ndtype: float64\n\n\n레이블이 하나도 맞지 않기 때문에 판다스가 이 Series를 정렬할 수 없습니다. 따라서 모두 NaN이 되었습니다."
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#스칼라로-초기화",
    "href": "data_mining/pandas/week_1b_pandas.html#스칼라로-초기화",
    "title": "Pandas",
    "section": "스칼라로 초기화",
    "text": "스칼라로 초기화\n스칼라와 인덱스 레이블의 리스트로 Series 객체를 초기화할 수도 있습니다: 모든 원소가 이 스칼라 값으로 설정됩니다.\n\nmeaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\nmeaning\n\nlife          42\nuniverse      42\neverything    42\ndtype: int64"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#series-이름",
    "href": "data_mining/pandas/week_1b_pandas.html#series-이름",
    "title": "Pandas",
    "section": "Series 이름",
    "text": "Series 이름\nSeries는 name을 가질 수 있습니다:\n\ns6 = pd.Series([83, 68], index=[\"bob\", \"alice\"], name=\"weights\")\ns6\n\nbob      83\nalice    68\nName: weights, dtype: int64\n\n\n\ns6.name\n\n'weights'"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#series-그래프-출력",
    "href": "data_mining/pandas/week_1b_pandas.html#series-그래프-출력",
    "title": "Pandas",
    "section": "Series 그래프 출력",
    "text": "Series 그래프 출력\n맷플롯립을 사용해 Series 데이터를 쉽게 그래프로 출력할 수 있습니다(맷플롯립에 대한 자세한 설명은 맷플롯립 튜토리얼을 참고하세요). 맷플롯립을 임포트하고 plot() 메서드를 호출하면 끝입니다:\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns7 = pd.Series(temperatures, name=\"Temperature\")\n\ns7.plot()\nplt.show()\n\n\n\n\n데이터를 그래프로 출력하는데 많은 옵션이 있습니다. 여기에서 모두 나열할 필요는 없습니다. 특정 종류의 그래프(히스토그램, 파이 차트 등)가 필요하면 판다스 문서의 시각화 섹션에서 예제 코드를 참고하세요."
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#시간-범위",
    "href": "data_mining/pandas/week_1b_pandas.html#시간-범위",
    "title": "Pandas",
    "section": "시간 범위",
    "text": "시간 범위\n먼저 pd.date_range()를 사용해 시계열을 만들어 보죠. 이 함수는 2016년 10월 29일 5:30pm에서 시작하여 12시간마다 하나의 datetime을 담고 있는 DatetimeIndex를 반환합니다.\n\ndates = pd.date_range('2016/10/29 5:30pm', periods=12, freq='H')\ndates\n\nDatetimeIndex(['2016-10-29 17:30:00', '2016-10-29 18:30:00',\n               '2016-10-29 19:30:00', '2016-10-29 20:30:00',\n               '2016-10-29 21:30:00', '2016-10-29 22:30:00',\n               '2016-10-29 23:30:00', '2016-10-30 00:30:00',\n               '2016-10-30 01:30:00', '2016-10-30 02:30:00',\n               '2016-10-30 03:30:00', '2016-10-30 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n\n\n\npd.date_range('2020-10-07', '2020-10-20', freq='3D')\n\nDatetimeIndex(['2020-10-07', '2020-10-10', '2020-10-13', '2020-10-16',\n               '2020-10-19'],\n              dtype='datetime64[ns]', freq='3D')\n\n\n이 DatetimeIndex를 Series의 인덱스로 사용할수 있습니다:\n\ntemperatures\n\n[4.4, 5.1, 6.1, 6.2, 6.1, 6.1, 5.7, 5.2, 4.7, 4.1, 3.9, 3.5]\n\n\n\ntemp_series = pd.Series(data = temperatures, index = dates)\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n이 시리즈를 그래프로 출력해 보죠:\n\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#리샘플링",
    "href": "data_mining/pandas/week_1b_pandas.html#리샘플링",
    "title": "Pandas",
    "section": "리샘플링",
    "text": "리샘플링\n판다스는 매우 간단하게 시계열을 리샘플링할 수 있습니다. resample() 메서드를 호출하고 새로운 주기를 지정하면 됩니다:\n\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n\ntemp_series_freq_2H = temp_series.resample(\"2H\")\ntemp_series_freq_2H\n\n<pandas.core.resample.DatetimeIndexResampler object at 0x7fafd827db50>\n\n\n리샘플링 연산은 사실 지연된 연산입니다. (https://ko.wikipedia.org/wiki/%EB%8A%90%EA%B8%8B%ED%95%9C_%EA%B3%84%EC%82%B0%EB%B2%95) 그래서 Series 객체 대신 DatetimeIndexResampler 객체가 반환됩니다. 실제 리샘플링 연산을 수행하려면 mean() 같은 메서드를 호출할 수 있습니다. 이 메서드는 연속적인 시간 쌍에 대해 평균을 계산합니다:\n\ntemp_series_freq_2H = temp_series_freq_2H.mean()\n\n\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n결과를 그래프로 출력해 보죠:\n\ntemp_series_freq_2H.plot(kind=\"bar\")\nplt.show()\n\n\n\n\n2시간 간격으로 어떻게 값이 수집되었는지 확인해 보세요. 예를 들어 6-8pm 간격을 보면 6:30pm에서 5.1이고 7:30pm에서 6.1입니다. 리샘플링 후에 5.1과 6.1의 평균인 5.6 하나를 얻었습니다. 평균말고 어떤 집계 함수(aggregation function)도 사용할 수 있습니다. 예를 들어 각 기간에서 최솟값을 찾을 수 있습니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").mean()\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n또는 동일한 효과를 내는 apply() 메서드를 사용할 수 있습니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").apply(np.min)\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#업샘플링과-보간",
    "href": "data_mining/pandas/week_1b_pandas.html#업샘플링과-보간",
    "title": "Pandas",
    "section": "업샘플링과 보간",
    "text": "업샘플링과 보간\n다운샘플링의 예를 보았습니다. 하지만 업샘플링(즉, 빈도를 높입니다)도 할 수 있습니다. 하지만 데이터에 구멍을 만듭니다:\n\ntemp_series_freq_15min = temp_series.resample(\"15Min\").mean()\ntemp_series_freq_15min.head(n=10) # `head`는 상위 n 개의 값만 출력합니다\n\n2016-10-29 17:30:00    4.4\n2016-10-29 17:45:00    NaN\n2016-10-29 18:00:00    NaN\n2016-10-29 18:15:00    NaN\n2016-10-29 18:30:00    5.1\n2016-10-29 18:45:00    NaN\n2016-10-29 19:00:00    NaN\n2016-10-29 19:15:00    NaN\n2016-10-29 19:30:00    6.1\n2016-10-29 19:45:00    NaN\nFreq: 15T, dtype: float64\n\n\n한가지 방법은 보간으로 사이를 채우는 것입니다. 이렇게 하려면 interpolate() 메서드를 호출합니다. 기본값은 선형 보간이지만 3차 보간(cubic interpolation) 같은 다른 방법을 선택할 수 있습니다: https://bskyvision.com/789\n\ntemp_series_freq_15min = temp_series.resample(\"15Min\").interpolate(method=\"cubic\")\ntemp_series_freq_15min.head(n=10)\n\n2016-10-29 17:30:00    4.400000\n2016-10-29 17:45:00    4.452911\n2016-10-29 18:00:00    4.605113\n2016-10-29 18:15:00    4.829758\n2016-10-29 18:30:00    5.100000\n2016-10-29 18:45:00    5.388992\n2016-10-29 19:00:00    5.669887\n2016-10-29 19:15:00    5.915839\n2016-10-29 19:30:00    6.100000\n2016-10-29 19:45:00    6.203621\nFreq: 15T, dtype: float64\n\n\n\ntemp_series.plot(label=\"Period: 1 hour\")\ntemp_series_freq_15min.plot(label=\"Period: 15 minutes\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#시간대-pass",
    "href": "data_mining/pandas/week_1b_pandas.html#시간대-pass",
    "title": "Pandas",
    "section": "시간대 pass",
    "text": "시간대 pass\n기본적으로 datetime은 단순합니다. 시간대(timezone)을 고려하지 않죠. 따라서 2016-10-30 02:30는 파리나 뉴욕이나 2016년 10월 30일 2:30pm입니다. tz_localize() 메서드로 시간대를 고려한 datetime을 만들 수 있습니다: https://www.timeanddate.com/time/map/\n\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n\ntemp_series_ny = temp_series.tz_localize(\"America/New_York\")\ntemp_series_ny\n\n2016-10-29 17:30:00-04:00    4.4\n2016-10-29 18:30:00-04:00    5.1\n2016-10-29 19:30:00-04:00    6.1\n2016-10-29 20:30:00-04:00    6.2\n2016-10-29 21:30:00-04:00    6.1\n2016-10-29 22:30:00-04:00    6.1\n2016-10-29 23:30:00-04:00    5.7\n2016-10-30 00:30:00-04:00    5.2\n2016-10-30 01:30:00-04:00    4.7\n2016-10-30 02:30:00-04:00    4.1\n2016-10-30 03:30:00-04:00    3.9\n2016-10-30 04:30:00-04:00    3.5\ndtype: float64\n\n\n모든 datetime에 -04:00이 추가됩니다. 즉 모든 시간은 UTC - 4시간을 의미합니다.\n다음처럼 파리 시간대로 바꿀 수 있습니다:\n\ntemp_series_paris = temp_series_ny.tz_convert(\"Europe/Paris\")\ntemp_series_paris\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64\n\n\nUTC와의 차이가 +02:00에서 +01:00으로 바뀐 것을 알 수 있습니다. 이는 프랑스가 10월 30일 3am에 겨울 시간으로 바꾸기 때문입니다(2am으로 바뀝니다). 따라서 2:30am이 두 번 등장합니다! 시간대가 없는 표현으로 돌아가 보죠(시간대가 없이 지역 시간으로 매시간 로그를 기록하는 경우 이와 비슷할 것입니다):\n\ntemp_series_paris_naive = temp_series_paris.tz_localize(None)\ntemp_series_paris_naive\n\n2016-10-29 23:30:00    4.4\n2016-10-30 00:30:00    5.1\n2016-10-30 01:30:00    6.1\n2016-10-30 02:30:00    6.2\n2016-10-30 02:30:00    6.1\n2016-10-30 03:30:00    6.1\n2016-10-30 04:30:00    5.7\n2016-10-30 05:30:00    5.2\n2016-10-30 06:30:00    4.7\n2016-10-30 07:30:00    4.1\n2016-10-30 08:30:00    3.9\n2016-10-30 09:30:00    3.5\ndtype: float64\n\n\n이렇게 되면 02:30이 정말 애매합니다. 시간대가 없는 datetime을 파리 시간대로 바꿀 때 에러가 발생합니다:\n\ntry:\n    temp_series_paris_naive.tz_localize(\"Europe/Paris\")\nexcept Exception as e:\n    print(type(e))\n    print(e)\n\n<class 'pytz.exceptions.AmbiguousTimeError'>\nCannot infer dst time from 2016-10-30 02:30:00, try using the 'ambiguous' argument\n\n\n다행히 ambiguous 매개변수를 사용하면 판다스가 타임스탬프의 순서를 기반으로 적절한 DST(일광 절약 시간제)를 추측합니다:\nhttps://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tori-tours&logNo=221221361831\n\ntemp_series_paris_naive.tz_localize(\"Europe/Paris\", ambiguous=\"infer\")\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#기간",
    "href": "data_mining/pandas/week_1b_pandas.html#기간",
    "title": "Pandas",
    "section": "기간",
    "text": "기간\npd.period_range() 함수는 DatetimeIndex가 아니라 PeriodIndex를 반환합니다. 예를 들어 2016과 2017년의 전체 분기를 가져와 보죠:\n\nquarters = pd.period_range('2016Q1', periods=8, freq='Q')\nquarters\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\nPeriodIndex에 숫자 N을 추가하면 PeriodIndex 빈도의 N 배만큼 이동시킵니다:\n\nquarters + 3\n\nPeriodIndex(['2016Q4', '2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1',\n             '2018Q2', '2018Q3'],\n            dtype='period[Q-DEC]')\n\n\nasfreq() 메서드를 사용하면 PeriodIndex의 빈도를 바꿀 수 있습니다. 모든 기간이 늘어나거나 줄어듭니다. 예를 들어 분기 기간을 모두 월별 기간으로 바꾸어 보죠:\n\nquarters.asfreq(\"M\") # 1쿼터가 3개월이기 때문에 M도 3개월씩 끊음.\n\nPeriodIndex(['2016-03', '2016-06', '2016-09', '2016-12', '2017-03', '2017-06',\n             '2017-09', '2017-12'],\n            dtype='period[M]')\n\n\n\nquarters\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\n기본적으로 asfreq는 각 기간의 끝에 맞춥니다. 기간의 시작에 맞추도록 변경할 수 있습니다:\n\nquarters.asfreq(\"M\", how=\"start\")\n\nPeriodIndex(['2016-01', '2016-04', '2016-07', '2016-10', '2017-01', '2017-04',\n             '2017-07', '2017-10'],\n            dtype='period[M]')\n\n\n간격을 늘릴 수도 있습니다: pandas 공식 메뉴얼 참조: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n\nquarters.asfreq(\"A\")\n\nPeriodIndex(['2016', '2016', '2016', '2016', '2017', '2017', '2017', '2017'], dtype='period[A-DEC]')\n\n\n물론 PeriodIndex로 Series를 만들 수 있습니다:\n\nquarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters)\nquarterly_revenue\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\nquarterly_revenue.plot(kind=\"line\")\nplt.show()\n\n\n\n\nto_timestamp를 호출해서 기간을 타임스탬프로 변경할 수 있습니다. 기본적으로 기간의 첫 번째 날을 반환합니다. 하지만 how와 freq를 지정해서 기간의 마지막 시간을 얻을 수 있습니다:\n\nquarterly_revenue\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\nlast_hours = quarterly_revenue.to_timestamp(how=\"end\", freq=\"H\")\nlast_hours\n\n2016-03-31 23:59:59.999999999    300\n2016-06-30 23:59:59.999999999    320\n2016-09-30 23:59:59.999999999    290\n2016-12-31 23:59:59.999999999    390\n2017-03-31 23:59:59.999999999    320\n2017-06-30 23:59:59.999999999    360\n2017-09-30 23:59:59.999999999    310\n2017-12-31 23:59:59.999999999    410\ndtype: int64\n\n\nto_peroid를 호출하면 다시 기간으로 돌아갑니다:\n\nlast_hours.to_period()\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n판다스는 여러 가지 시간 관련 함수를 많이 제공합니다. 온라인 문서를 확인해 보세요. 예를 하나 들면 2016년 매월 마지막 업무일의 9시를 얻는 방법은 다음과 같습니다:\n\nmonths_2022 = pd.period_range(\"2022\", periods=12, freq=\"M\")\none_day_after_last_days = months_2022.asfreq(\"D\") + 1\nlast_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay(n=1)\nlast_bdays.to_period(\"H\") + 9\n\nPeriodIndex(['2022-01-31 09:00', '2022-02-28 09:00', '2022-03-31 09:00',\n             '2022-04-29 09:00', '2022-05-31 09:00', '2022-06-30 09:00',\n             '2022-07-29 09:00', '2022-08-31 09:00', '2022-09-30 09:00',\n             '2022-10-31 09:00', '2022-11-30 09:00', '2022-12-30 09:00'],\n            dtype='period[H]')\n\n\n\none_day_after_last_days.to_timestamp()\n\nDatetimeIndex(['2022-02-01', '2022-03-01', '2022-04-01', '2022-05-01',\n               '2022-06-01', '2022-07-01', '2022-08-01', '2022-09-01',\n               '2022-10-01', '2022-11-01', '2022-12-01', '2023-01-01'],\n              dtype='datetime64[ns]', freq='MS')\n\n\n\none_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay(n=1) # 원래는 12월 31일인데 31일이 휴일이라 30일 (마지막으로 일 한 날)\n\nDatetimeIndex(['2022-01-31', '2022-02-28', '2022-03-31', '2022-04-29',\n               '2022-05-31', '2022-06-30', '2022-07-29', '2022-08-31',\n               '2022-09-30', '2022-10-31', '2022-11-30', '2022-12-30'],\n              dtype='datetime64[ns]', freq=None)"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#dataframe-만들기",
    "href": "data_mining/pandas/week_1b_pandas.html#dataframe-만들기",
    "title": "Pandas",
    "section": "DataFrame 만들기",
    "text": "DataFrame 만들기\nSeries 객체의 딕셔너리를 전달하여 데이터프레임을 만들 수 있습니다:\n\npeople_dict = {\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n}\npeople = pd.DataFrame(people_dict)\npeople\n\n\n\n\n\n  \n    \n      \n      weight\n      birthyear\n      children\n      hobby\n    \n  \n  \n    \n      alice\n      68\n      1985\n      NaN\n      Biking\n    \n    \n      bob\n      83\n      1984\n      3.0\n      Dancing\n    \n    \n      charles\n      112\n      1992\n      0.0\n      NaN\n    \n  \n\n\n\n\n몇가지 알아 두어야 할 것은 다음과 같습니다:\n\nSeries는 인덱스를 기반으로 자동으로 정렬됩니다.\n누란된 값은 NaN으로 표현됩니다.\nSeries 이름은 무시됩니다(\"year\"란 이름은 삭제됩니다).\nDataFrame은 주피터 노트북에서 멋지게 출력됩니다!\n\n예상하는 방식으로 열을 참조할 수 있고 Series 객체가 반환됩니다:\n\npeople[\"birthyear\"]\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n동시에 여러 개의 열을 선택할 수 있습니다:\n\npeople.birthyear\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople[[\"birthyear\", \"hobby\"]]\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n    \n    \n      bob\n      1984\n      Dancing\n    \n    \n      charles\n      1992\n      NaN\n    \n  \n\n\n\n\n열 리스트나 행 인덱스 레이블을 DataFrame 생성자에 전달하면 해당 열과 행으로 채워진 데이터프레임이 반환됩니다. 예를 들면:\n\npeople_dict\n\n{'weight': alice       68\n bob         83\n charles    112\n dtype: int64,\n 'birthyear': bob        1984\n alice      1985\n charles    1992\n Name: year, dtype: int64,\n 'children': charles    0\n bob        3\n dtype: int64,\n 'hobby': alice     Biking\n bob      Dancing\n dtype: object}\n\n\n\nd2 = pd.DataFrame(\n        people_dict,\n        columns=[\"birthyear\", \"weight\", \"height\"],\n        index=[\"bob\", \"alice\", \"eugene\"]\n     )\n\n\nd2\n\n\n\n\n\n  \n    \n      \n      birthyear\n      weight\n      height\n    \n  \n  \n    \n      bob\n      1984.0\n      83.0\n      NaN\n    \n    \n      alice\n      1985.0\n      68.0\n      NaN\n    \n    \n      eugene\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nDataFrame을 만드는 또 다른 편리한 방법은 ndarray나 리스트의 리스트로 모든 값을 생성자에게 전달하고 열 이름과 행 인덱스 레이블을 각기 지정하는 것입니다:\n\nvalues = [\n            [1985, np.nan, \"Biking\",   68],\n            [1984, 3,      \"Dancing\",  83],\n            [1992, 0,      np.nan,    112]\n         ]\nd3 = pd.DataFrame(\n        values,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3.0\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0.0\n      NaN\n      112\n    \n  \n\n\n\n\n누락된 값을 지정하려면 np.nan이나 넘파이 마스크 배열을 사용합니다:\ndtype = object는 문자열 데이터를 의미\n\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array\n\nmasked_array(\n  data=[[1985, nan, 'Biking', 68],\n        [1984, 3, 'Dancing', 83],\n        [1992, 0, nan, 112]],\n  mask=False,\n  fill_value='?',\n  dtype=object)\n\n\n\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array[(0, 2), (1, 2)] = np.ma.masked\nd3 = pd.DataFrame(\n        masked_array,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0\n      NaN\n      112\n    \n  \n\n\n\n\nndarray 대신에 DataFrame 객체를 전달할 수도 있습니다:\n\nd3\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0\n      NaN\n      112\n    \n  \n\n\n\n\n\nd4 = pd.DataFrame(\n         d3,\n         columns=[\"hobby\", \"children\"],\n         index=[\"alice\", \"bob\"]\n     )\nd4\n\n\n\n\n\n  \n    \n      \n      hobby\n      children\n    \n  \n  \n    \n      alice\n      Biking\n      NaN\n    \n    \n      bob\n      Dancing\n      3\n    \n  \n\n\n\n\n딕셔너리의 딕셔너리(또는 리스트의 리스트)로 DataFrame을 만들 수 있습니다:\n\npeople = pd.DataFrame({\n    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992},\n    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"},\n    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112},\n    \"children\": {\"bob\": 3, \"charles\": 0}\n})\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#멀티-인덱싱",
    "href": "data_mining/pandas/week_1b_pandas.html#멀티-인덱싱",
    "title": "Pandas",
    "section": "멀티 인덱싱",
    "text": "멀티 인덱싱\n모든 열이 같은 크기의 튜플이면 멀티 인덱스로 인식합니다. 열 인덱스 레이블에도 같은 방식이 적용됩니다. 예를 들면:\n\nd5 = pd.DataFrame(\n  {\n    (\"public\", \"birthyear\"):\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\n    (\"public\", \"hobby\"):\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n    (\"private\", \"weight\"):\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n    (\"private\", \"children\"):\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n  }\n)\nd5\n\n\n\n\n\n  \n    \n      \n      \n      public\n      private\n    \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n이제 \"public\" 열을 모두 담은 DataFrame을 손쉽게 만들 수 있습니다:\n\nd5[\"public\"]\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n    \n    \n      bob\n      1984\n      Dancing\n    \n    \n      London\n      charles\n      1992\n      NaN\n    \n  \n\n\n\n\n\nd5[\"public\", \"hobby\"]  # d5[\"public\"][\"hobby\"]와 같습니다.\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: (public, hobby), dtype: object\n\n\n\nd5[\"public\"]['hobby']\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: hobby, dtype: object"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#레벨-낮추기",
    "href": "data_mining/pandas/week_1b_pandas.html#레벨-낮추기",
    "title": "Pandas",
    "section": "레벨 낮추기",
    "text": "레벨 낮추기\nd5를 다시 확인해 보죠:\n\nd5\n\n\n\n\n\n  \n    \n      \n      \n      public\n      private\n    \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n열의 레벨(level)이 2개이고 인덱스 레벨이 2개입니다. droplevel()을 사용해 열 레벨을 낮출 수 있습니다(인덱스도 마찬가지입니다):\n\nd5.columns = d5.columns.droplevel(level = 0)\nd5\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\nd6 = d5.copy()\nd6.index = d6.index.droplevel(level = 0)\nd6\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\nd5.droplevel(level = 1)\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      Paris\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      1992\n      NaN\n      112\n      0.0"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#전치",
    "href": "data_mining/pandas/week_1b_pandas.html#전치",
    "title": "Pandas",
    "section": "전치",
    "text": "전치\nT 속성을 사용해 열과 인덱스를 바꿀 수 있습니다:\n\nd5\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\nd6 = d5.T\nd6\n\n\n\n\n\n  \n    \n      \n      Paris\n      London\n    \n    \n      \n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      1985\n      1984\n      1992\n    \n    \n      hobby\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      68\n      83\n      112\n    \n    \n      children\n      NaN\n      3.0\n      0.0"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#레벨-스택과-언스택",
    "href": "data_mining/pandas/week_1b_pandas.html#레벨-스택과-언스택",
    "title": "Pandas",
    "section": "레벨 스택과 언스택",
    "text": "레벨 스택과 언스택\nstack() 메서드는 가장 낮은 열 레벨을 가장 낮은 인덱스 뒤에 추가합니다:\n\nd6\n\n\n\n\n\n  \n    \n      \n      Paris\n      London\n    \n    \n      \n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      1985\n      1984\n      1992\n    \n    \n      hobby\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      68\n      83\n      112\n    \n    \n      children\n      NaN\n      3.0\n      0.0\n    \n  \n\n\n\n\n\nd7 = d6.stack()\nd7\n\n\n\n\n\n  \n    \n      \n      \n      London\n      Paris\n    \n  \n  \n    \n      birthyear\n      alice\n      NaN\n      1985\n    \n    \n      bob\n      NaN\n      1984\n    \n    \n      charles\n      1992\n      NaN\n    \n    \n      hobby\n      alice\n      NaN\n      Biking\n    \n    \n      bob\n      NaN\n      Dancing\n    \n    \n      weight\n      alice\n      NaN\n      68\n    \n    \n      bob\n      NaN\n      83\n    \n    \n      charles\n      112\n      NaN\n    \n    \n      children\n      bob\n      NaN\n      3.0\n    \n    \n      charles\n      0.0\n      NaN\n    \n  \n\n\n\n\nNaN 값이 생겼습니다. 이전에 없던 조합이 생겼기 때문입니다(예를 들어 London에 bob이 없었습니다).\nunstack()을 호출하면 반대가 됩니다. 여기에서도 많은 NaN 값이 생성됩니다.\n\nd8 = d7.unstack()\nd8\n\n\n\n\n\n  \n    \n      \n      London\n      Paris\n    \n    \n      \n      alice\n      bob\n      charles\n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      NaN\n      NaN\n      1992\n      1985\n      1984\n      NaN\n    \n    \n      children\n      NaN\n      NaN\n      0.0\n      NaN\n      3.0\n      NaN\n    \n    \n      hobby\n      NaN\n      NaN\n      NaN\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      NaN\n      NaN\n      112\n      68\n      83\n      NaN\n    \n  \n\n\n\n\nunstack을 다시 호출하면 Series 객체가 만들어 집니다:\n\nd9 = d8.unstack()\nd9\n\nLondon  alice    birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        bob      birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        charles  birthyear       1992\n                 children         0.0\n                 hobby            NaN\n                 weight           112\nParis   alice    birthyear       1985\n                 children         NaN\n                 hobby         Biking\n                 weight            68\n        bob      birthyear       1984\n                 children         3.0\n                 hobby        Dancing\n                 weight            83\n        charles  birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\ndtype: object\n\n\nstack()과 unstack() 메서드를 사용할 때 스택/언스택할 level을 선택할 수 있습니다. 심지어 한 번에 여러 개의 레벨을 스택/언스택할 수도 있습니다:\n\nd10 = d9.unstack(level = (0,1))\nd10\n\n\n\n\n\n  \n    \n      \n      London\n      Paris\n    \n    \n      \n      alice\n      bob\n      charles\n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      NaN\n      NaN\n      1992\n      1985\n      1984\n      NaN\n    \n    \n      children\n      NaN\n      NaN\n      0.0\n      NaN\n      3.0\n      NaN\n    \n    \n      hobby\n      NaN\n      NaN\n      NaN\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      NaN\n      NaN\n      112\n      68\n      83\n      NaN"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "href": "data_mining/pandas/week_1b_pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "title": "Pandas",
    "section": "대부분의 메서드는 수정된 복사본을 반환합니다",
    "text": "대부분의 메서드는 수정된 복사본을 반환합니다\n눈치챘겠지만 stack()과 unstack() 메서드는 객체를 수정하지 않습니다. 대신 복사본을 만들어 반환합니다. 판다스에 있는 대부분의 메서드들이 이렇게 동작합니다.\nStack & Unstack + Pivot에 대한 설명 참고 https://pandas.pydata.org/docs/user_guide/reshaping.html\nData Reshaping!\n\nPivot\n\nimport pandas._testing as tm\n\ndef unpivot(frame):\n    N, K = frame.shape\n    data = {\n        \"value\": frame.to_numpy().ravel(\"F\"),\n        \"variable\": np.asarray(frame.columns).repeat(N),\n        \"date\": np.tile(np.asarray(frame.index), K),\n    }\n    return pd.DataFrame(data, columns=[\"date\", \"variable\", \"value\"])\n\ndf = unpivot(tm.makeTimeDataFrame(3))\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      -0.106608\n    \n    \n      1\n      2000-01-04\n      A\n      -0.152363\n    \n    \n      2\n      2000-01-05\n      A\n      -0.590536\n    \n    \n      3\n      2000-01-03\n      B\n      -0.504281\n    \n    \n      4\n      2000-01-04\n      B\n      1.571623\n    \n    \n      5\n      2000-01-05\n      B\n      -0.516232\n    \n    \n      6\n      2000-01-03\n      C\n      -1.973264\n    \n    \n      7\n      2000-01-04\n      C\n      -0.453793\n    \n    \n      8\n      2000-01-05\n      C\n      -0.240602\n    \n    \n      9\n      2000-01-03\n      D\n      -0.394477\n    \n    \n      10\n      2000-01-04\n      D\n      -0.224817\n    \n    \n      11\n      2000-01-05\n      D\n      0.130486\n    \n  \n\n\n\n\nTo select out everything for variable A we could do:\n\nfiltered = df[df[\"variable\"] == \"A\"]\nfiltered\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      -0.106608\n    \n    \n      1\n      2000-01-04\n      A\n      -0.152363\n    \n    \n      2\n      2000-01-05\n      A\n      -0.590536\n    \n  \n\n\n\n\nBut suppose we wish to do time series operations with the variables. A better representation would be where the columns are the unique variables and an index of dates identifies individual observations. To reshape the data into this form, we use the DataFrame.pivot() method (also implemented as a top level function pivot()):\n\npivoted = df.pivot(index=\"date\", columns=\"variable\", values=\"value\")\n\npivoted\n\n\n\n\n\n  \n    \n      variable\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      -0.106608\n      -0.504281\n      -1.973264\n      -0.394477\n    \n    \n      2000-01-04\n      -0.152363\n      1.571623\n      -0.453793\n      -0.224817\n    \n    \n      2000-01-05\n      -0.590536\n      -0.516232\n      -0.240602\n      0.130486\n    \n  \n\n\n\n\n\npivoted.columns\n\nIndex(['A', 'B', 'C', 'D'], dtype='object', name='variable')\n\n\n\npivoted.index\n\nDatetimeIndex(['2000-01-03', '2000-01-04', '2000-01-05'], dtype='datetime64[ns]', name='date', freq=None)\n\n\nIf the values argument is omitted, and the input DataFrame has more than one column of values which are not used as column or index inputs to pivot(), then the resulting “pivoted” DataFrame will have hierarchical columns whose topmost level indicates the respective value column:\n\ndf[\"value2\"] = df[\"value\"] * 2\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n      value2\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      -0.106608\n      -0.213216\n    \n    \n      1\n      2000-01-04\n      A\n      -0.152363\n      -0.304727\n    \n    \n      2\n      2000-01-05\n      A\n      -0.590536\n      -1.181073\n    \n    \n      3\n      2000-01-03\n      B\n      -0.504281\n      -1.008562\n    \n    \n      4\n      2000-01-04\n      B\n      1.571623\n      3.143247\n    \n    \n      5\n      2000-01-05\n      B\n      -0.516232\n      -1.032464\n    \n    \n      6\n      2000-01-03\n      C\n      -1.973264\n      -3.946528\n    \n    \n      7\n      2000-01-04\n      C\n      -0.453793\n      -0.907586\n    \n    \n      8\n      2000-01-05\n      C\n      -0.240602\n      -0.481204\n    \n    \n      9\n      2000-01-03\n      D\n      -0.394477\n      -0.788953\n    \n    \n      10\n      2000-01-04\n      D\n      -0.224817\n      -0.449633\n    \n    \n      11\n      2000-01-05\n      D\n      0.130486\n      0.260972\n    \n  \n\n\n\n\n\npivoted = df.pivot(index=\"date\", columns=\"variable\")\n\npivoted\n\n\n\n\n\n  \n    \n      \n      value\n      value2\n    \n    \n      variable\n      A\n      B\n      C\n      D\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      -0.106608\n      -0.504281\n      -1.973264\n      -0.394477\n      -0.213216\n      -1.008562\n      -3.946528\n      -0.788953\n    \n    \n      2000-01-04\n      -0.152363\n      1.571623\n      -0.453793\n      -0.224817\n      -0.304727\n      3.143247\n      -0.907586\n      -0.449633\n    \n    \n      2000-01-05\n      -0.590536\n      -0.516232\n      -0.240602\n      0.130486\n      -1.181073\n      -1.032464\n      -0.481204\n      0.260972\n    \n  \n\n\n\n\n\npivoted['value']\n\n\n\n\n\n  \n    \n      variable\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      -0.106608\n      -0.504281\n      -1.973264\n      -0.394477\n    \n    \n      2000-01-04\n      -0.152363\n      1.571623\n      -0.453793\n      -0.224817\n    \n    \n      2000-01-05\n      -0.590536\n      -0.516232\n      -0.240602\n      0.130486"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#행-참조하기",
    "href": "data_mining/pandas/week_1b_pandas.html#행-참조하기",
    "title": "Pandas",
    "section": "행 참조하기",
    "text": "행 참조하기\npeople DataFrame으로 돌아가 보죠:\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\nloc 속성으로 열 대신 행을 참조할 수 있습니다. DataFrame의 열 이름이 행 인덱스 레이블로 매핑된 Series 객체가 반환됩니다:\n\npeople['birthyear']\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople.loc[\"charles\"]\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\niloc 속성을 사용해 정수 인덱스로 행을 참조할 수 있습니다:\n\npeople.iloc[2]\n\nweight        112\nbirthyear    1992\nchildren      0.0\nhobby         NaN\nName: charles, dtype: object\n\n\n행을 슬라이싱할 수 있으며 DataFrame 객체가 반환됩니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\npeople.iloc[1:3]\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n마자믹으로 불리언 배열을 전달하여 해당하는 행을 가져올 수 있습니다:\n\npeople[np.array([True, False, True])]\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n불리언 표현식을 사용할 때 아주 유용합니다:\n\npeople[\"birthyear\"] < 1990\n\nalice       True\nbob         True\ncharles    False\nName: birthyear, dtype: bool\n\n\n\npeople[people[\"birthyear\"] < 1990]\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#열-추가-삭제",
    "href": "data_mining/pandas/week_1b_pandas.html#열-추가-삭제",
    "title": "Pandas",
    "section": "열 추가, 삭제",
    "text": "열 추가, 삭제\nDataFrame을 Series의 딕셔너리처럼 다룰 수 있습니다. 따라서 다음 같이 쓸 수 있습니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\npeople[\"age\"] = 2022 - people[\"birthyear\"]  # \"age\" 열을 추가합니다\npeople[\"over 30\"] = people[\"age\"] > 30      # \"over 30\" 열을 추가합니다\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n      age\n      over 30\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n      37\n      True\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n      38\n      True\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n      30\n      False\n    \n  \n\n\n\n\n\nbirthyears = people.pop(\"birthyear\")\ndel people[\"children\"]\n\n\nbirthyears\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\n# 딕셔너리도 유사함\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\n\n\nweights.pop(\"alice\")\n\n68\n\n\n\nweights\n\n{'bob': 83, 'colin': 86, 'darwin': 68}\n\n\n\ndel weights[\"bob\"]\n\n\nweights\n\n{'colin': 86, 'darwin': 68}\n\n\n새로운 열을 추가할 때 행의 개수는 같아야 합니다. 누락된 행은 NaN으로 채워지고 추가적인 행은 무시됩니다:\n\npeople.index\n\nIndex(['alice', 'bob', 'charles'], dtype='object')\n\n\n\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice 누락됨, eugene은 무시됨\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      age\n      over 30\n      pets\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      37\n      True\n      NaN\n    \n    \n      bob\n      Dancing\n      83\n      38\n      True\n      0.0\n    \n    \n      charles\n      NaN\n      112\n      30\n      False\n      5.0\n    \n  \n\n\n\n\n새로운 열을 추가할 때 기본적으로 (오른쪽) 끝에 추가됩니다. insert() 메서드를 사용해 다른 곳에 열을 추가할 수 있습니다:\n\npeople.insert(1, \"height\", [172, 181, 185])\npeople\n\n\n\n\n\n  \n    \n      \n      weight\n      height\n      birthyear\n      children\n      hobby\n    \n  \n  \n    \n      alice\n      68\n      172\n      1985\n      NaN\n      Biking\n    \n    \n      bob\n      83\n      181\n      1984\n      3.0\n      Dancing\n    \n    \n      charles\n      112\n      185\n      1992\n      0.0\n      NaN"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#새로운-열-할당하기",
    "href": "data_mining/pandas/week_1b_pandas.html#새로운-열-할당하기",
    "title": "Pandas",
    "section": "새로운 열 할당하기",
    "text": "새로운 열 할당하기\nassign() 메서드를 호출하여 새로운 열을 만들 수도 있습니다. 이는 새로운 DataFrame 객체를 반환하며 원본 객체는 변경되지 않습니다:\n\npeople.assign(\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n    has_pets = people[\"pets\"] > 0\n)\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      has_pets\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      False\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n\npeople[\"body_mass_index\"] = people[\"weight\"] / (people[\"height\"] / 100) ** 2\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n    \n  \n\n\n\n\n\ndel people[\"body_mass_index\"]\n\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n    \n  \n\n\n\n\n할당문 안에서 만든 열은 접근할 수 없습니다:\n\ntry:\n    people.assign(\n        body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n        overweight = people[\"body_mass_index\"] > 25\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 'body_mass_index'\n\n\n해결책은 두 개의 연속된 할당문으로 나누는 것입니다:\n\nd6 = people.assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\nd6.assign(overweight = d6[\"body_mass_index\"] > 25)\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n임시 변수 d6를 만들면 불편합니다. assign() 메서드를 연결하고 싶겠지만 people 객체가 첫 번째 할당문에서 실제로 수정되지 않기 때문에 작동하지 않습니다:\n\ntry:\n    (people\n         .assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\n         .assign(overweight = people[\"body_mass_index\"] > 25)\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 'body_mass_index'\n\n\n하지만 걱정하지 마세요. 간단한 방법이 있습니다. assign() 메서드에 함수(전형적으로 lambda 함수)를 전달하면 DataFrame을 매개변수로 이 함수를 호출할 것입니다:\n\n(people\n     .assign(body_mass_index = lambda df: df[\"weight\"] / (df[\"height\"] / 100) ** 2)\n     .assign(overweight = lambda df: df[\"body_mass_index\"] > 25)\n)\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n문제가 해결되었군요!\n\npeople[\"body_mass_index\"] = people[\"weight\"] / (people[\"height\"] / 100) ** 2\npeople[\"overweight\"] = people[\"body_mass_index\"]>25\n\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#표현식-평가",
    "href": "data_mining/pandas/week_1b_pandas.html#표현식-평가",
    "title": "Pandas",
    "section": "표현식 평가",
    "text": "표현식 평가\n판다스가 제공하는 뛰어난 기능 하나는 표현식 평가입니다. 이는 numexpr 라이브러리에 의존하기 때문에 설치가 되어 있어야 합니다.\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n\n\"weight / (height/100) ** 2 > 25\"\n\n'weight / (height/100) ** 2 > 25'\n\n\n\npeople.eval(\"weight / (height/100) ** 2 > 25\")\n\nalice      False\nbob         True\ncharles     True\ndtype: bool\n\n\n할당 표현식도 지원됩니다. inplace=True로 지정하면 수정된 복사본을 만들지 않고 바로 DataFrame을 변경합니다:\n\npeople.eval(\"body_mass_index = weight / (height/100) ** 2\", inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n'@'를 접두어로 사용하여 지역 변수나 전역 변수를 참조할 수 있습니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\noverweight_threshold = 30\npeople.eval(\"overweight = body_mass_index > @overweight_threshold\", inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#dataframe-쿼리하기",
    "href": "data_mining/pandas/week_1b_pandas.html#dataframe-쿼리하기",
    "title": "Pandas",
    "section": "DataFrame 쿼리하기",
    "text": "DataFrame 쿼리하기\nquery() 메서드를 사용하면 쿼리 표현식에 기반하여 DataFrame을 필터링할 수 있습니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\npeople.query(\"age > 30 and pets == 0\")\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\npeople[(people[\"age\"]>30) & (people[\"pets\"] == 0)]\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\nmask = (people[\"age\"]>30) & (people[\"pets\"] == 0)\n\n\npeople[mask]\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#dataframe-정렬",
    "href": "data_mining/pandas/week_1b_pandas.html#dataframe-정렬",
    "title": "Pandas",
    "section": "DataFrame 정렬",
    "text": "DataFrame 정렬\nsort_index 메서드를 호출하여 DataFrame을 정렬할 수 있습니다. 기본적으로 인덱스 레이블을 기준으로 오름차순으로 행을 정렬합니다. 여기에서는 내림차순으로 정렬해 보죠:\n\npeople.sort_index(ascending=False)\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n  \n\n\n\n\nsort_index는 DataFrame의 정렬된 복사본을 반환합니다. people을 직접 수정하려면 inplace 매개변수를 True로 지정합니다. 또한 axis=1로 지정하여 열 대신 행을 정렬할 수 있습니다:\n\npeople.sort_index(axis=1, inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n레이블이 아니라 값을 기준으로 DataFrame을 정렬하려면 sort_values에 정렬하려는 열을 지정합니다:\n\npeople.sort_values(by=\"age\", inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#dataframe-그래프-그리기",
    "href": "data_mining/pandas/week_1b_pandas.html#dataframe-그래프-그리기",
    "title": "Pandas",
    "section": "DataFrame 그래프 그리기",
    "text": "DataFrame 그래프 그리기\nSeries와 마찬가지로 판다스는 DataFrame 기반으로 멋진 그래프를 손쉽게 그릴 수 있습니다.\n예를 들어 plot 메서드를 호출하여 DataFrame의 데이터에서 선 그래프를 쉽게 그릴 수 있습니다:\n\npeople.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\nplt.show()\n\n\n\n\n맷플롯립의 함수가 지원하는 다른 매개변수를 사용할 수 있습니다. 예를 들어, 산점도를 그릴 때 맷플롯립의 scatter() 함수의 s 매개변수를 사용해 크기를 지정할 수 있습니다:\n\npeople.plot(kind = \"scatter\", x = \"height\", y = \"weight\", s=[40, 120, 200])\nplt.show()\n\n\n\n\n선택할 수 있는 옵션이 많습니다. 판다스 문서의 시각화 페이지에서 마음에 드는 그래프를 찾아 예제 코드를 살펴 보세요.\n\nHistogram\n\n\ndf4 = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000) + 1,\n        \"b\": np.random.randn(1000),\n        \"c\": np.random.randn(1000) - 1,\n    },\n    columns=[\"a\", \"b\", \"c\"],\n)\n\nplt.figure();\n\ndf4.plot.hist(alpha=0.5);\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n\ndf4\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1.024702\n      0.205680\n      -1.151098\n    \n    \n      1\n      0.932532\n      -0.406178\n      -0.108419\n    \n    \n      2\n      -0.616295\n      -1.700059\n      -1.133808\n    \n    \n      3\n      1.481105\n      -0.520759\n      0.685474\n    \n    \n      4\n      0.736898\n      -0.533195\n      -1.705509\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      -0.220861\n      -0.182931\n      -1.275840\n    \n    \n      996\n      1.069163\n      -1.368469\n      -0.772002\n    \n    \n      997\n      1.378971\n      -0.554198\n      -2.278343\n    \n    \n      998\n      0.661284\n      -0.835081\n      -0.700224\n    \n    \n      999\n      0.664459\n      -0.083600\n      0.502208\n    \n  \n\n1000 rows × 3 columns\n\n\n\n\ndf4.plot(kind=\"hist\",alpha=0.5, x=\"a\")\nplt.show()\n\n\n\n\n\ndf4['a'].plot.hist()\nplt.show()\n\n\n\n\n\nBoxplot\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n    \n  \n  \n    \n      0\n      0.887491\n      0.028126\n      0.003576\n      0.013055\n      0.509898\n    \n    \n      1\n      0.653231\n      0.003621\n      0.969910\n      0.173393\n      0.409486\n    \n    \n      2\n      0.803033\n      0.382834\n      0.195527\n      0.616920\n      0.581911\n    \n    \n      3\n      0.524122\n      0.926863\n      0.170608\n      0.300242\n      0.930059\n    \n    \n      4\n      0.968483\n      0.187320\n      0.839602\n      0.149723\n      0.650208\n    \n    \n      5\n      0.110352\n      0.393050\n      0.719806\n      0.859684\n      0.501955\n    \n    \n      6\n      0.866960\n      0.221862\n      0.892753\n      0.990645\n      0.736521\n    \n    \n      7\n      0.801126\n      0.614989\n      0.057752\n      0.183695\n      0.569820\n    \n    \n      8\n      0.013725\n      0.439573\n      0.021304\n      0.192832\n      0.270145\n    \n    \n      9\n      0.696394\n      0.974029\n      0.351002\n      0.409430\n      0.581877\n    \n  \n\n\n\n\n\ndf = pd.DataFrame(np.random.rand(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\ndf.plot.box();\n\n\n\n\n\ndf = pd.DataFrame(np.random.rand(10, 2), columns=[\"Col1\", \"Col2\"])\n\ndf[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\ndf\n\n\n\n\n\n  \n    \n      \n      Col1\n      Col2\n      X\n    \n  \n  \n    \n      0\n      0.078610\n      0.759337\n      A\n    \n    \n      1\n      0.443074\n      0.020230\n      A\n    \n    \n      2\n      0.858927\n      0.881363\n      A\n    \n    \n      3\n      0.912618\n      0.354576\n      A\n    \n    \n      4\n      0.784800\n      0.178885\n      A\n    \n    \n      5\n      0.552493\n      0.020503\n      B\n    \n    \n      6\n      0.895306\n      0.838616\n      B\n    \n    \n      7\n      0.901667\n      0.012848\n      B\n    \n    \n      8\n      0.814965\n      0.330909\n      B\n    \n    \n      9\n      0.211971\n      0.653164\n      B\n    \n  \n\n\n\n\n\nplt.figure();\n\nbp = df.boxplot(by=\"X\")\n\n<Figure size 432x288 with 0 Axes>"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#dataframe-연산",
    "href": "data_mining/pandas/week_1b_pandas.html#dataframe-연산",
    "title": "Pandas",
    "section": "DataFrame 연산",
    "text": "DataFrame 연산\nDataFrame이 넘파이 배열을 흉내내려는 것은 아니지만 몇 가지 비슷한 점이 있습니다. 예제 DataFrame을 만들어 보죠:\n\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\nDataFrame에 넘파이 수학 함수를 적용하면 모든 값에 이 함수가 적용됩니다:\n\nnp.sqrt(grades)\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      2.828427\n      2.828427\n      3.000000\n    \n    \n      bob\n      3.162278\n      3.000000\n      3.000000\n    \n    \n      charles\n      2.000000\n      2.828427\n      1.414214\n    \n    \n      darwin\n      3.000000\n      3.162278\n      3.162278\n    \n  \n\n\n\n\n비슷하게 DataFrame에 하나의 값을 더하면 DataFrame의 모든 원소에 이 값이 더해집니다. 이를 브로드캐스팅이라고 합니다:\n\ngrades + 1\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      9\n      9\n      10\n    \n    \n      bob\n      11\n      10\n      10\n    \n    \n      charles\n      5\n      9\n      3\n    \n    \n      darwin\n      10\n      11\n      11\n    \n  \n\n\n\n\n물론 산술 연산(*,/,**…)과 조건 연산(>, ==…)을 포함해 모든 이항 연산에도 마찬가지 입니다:\n\ngrades >= 5\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      True\n      True\n      True\n    \n    \n      bob\n      True\n      True\n      True\n    \n    \n      charles\n      False\n      True\n      False\n    \n    \n      darwin\n      True\n      True\n      True\n    \n  \n\n\n\n\nDataFrame의 max, sum, mean 같은 집계 연산은 각 열에 적용되어 Series 객체가 반환됩니다:\n\ngrades.mean()\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\nall 메서드도 집계 연산입니다: 모든 값이 True인지 아닌지 확인합니다. 모든 학생의 점수가 5 이상인 월을 찾아 보죠:\n\n(grades > 5).all()\n\nsep    False\noct     True\nnov    False\ndtype: bool\n\n\nMost of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let’s find out which students had all grades greater than 5:\n\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\n(grades > 5).all(axis = 1)\n\nalice       True\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nany 메서드는 하나라도 참이면 True를 반환합니다. 한 번이라도 10점을 받은 사람을 찾아 보죠:\n\n(grades == 10).any(axis = 1)\n\nalice      False\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nDataFrame에 Series 객체를 더하면 (또는 다른 이항 연산을 수행하면) 판다스는 DataFrame에 있는 모든 행에 이 연산을 브로드캐스팅합니다. 이는 Series 객체가 DataFrame의 행의 개수와 크기가 같을 때만 동작합니다. 예를 들어 DataFrame의 mean(Series 객체)을 빼보죠:\n\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\ngrades.mean()\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\n\ngrades - grades.mean()  # grades - [7.75, 8.75, 7.50] 와 동일\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      0.25\n      -0.75\n      1.5\n    \n    \n      bob\n      2.25\n      0.25\n      1.5\n    \n    \n      charles\n      -3.75\n      -0.75\n      -5.5\n    \n    \n      darwin\n      1.25\n      1.25\n      2.5\n    \n  \n\n\n\n\n모든 9월 성적에서 7.75를 빼고, 10월 성적에서 8.75를 빼고, 11월 성적에서 7.50을 뺍니다. 이는 다음 DataFrame을 빼는 것과 같습니다:\n\npd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns)\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      7.75\n      8.75\n      7.5\n    \n    \n      bob\n      7.75\n      8.75\n      7.5\n    \n    \n      charles\n      7.75\n      8.75\n      7.5\n    \n    \n      darwin\n      7.75\n      8.75\n      7.5\n    \n  \n\n\n\n\n모든 성적의 전체 평균을 빼고 싶다면 다음과 같은 방법을 사용합니다:\n\ngrades.values.mean()\n\n8.0\n\n\n\ngrades - grades.values.mean() # 모든 점수에서 전체 평균(8.00)을 뺍니다\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      0.0\n      0.0\n      1.0\n    \n    \n      bob\n      2.0\n      1.0\n      1.0\n    \n    \n      charles\n      -4.0\n      0.0\n      -6.0\n    \n    \n      darwin\n      1.0\n      2.0\n      2.0"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#자동-정렬-1",
    "href": "data_mining/pandas/week_1b_pandas.html#자동-정렬-1",
    "title": "Pandas",
    "section": "자동 정렬",
    "text": "자동 정렬\nSeries와 비슷하게 여러 개의 DataFrame에 대한 연산을 수행하면 판다스는 자동으로 행 인덱스 레이블로 정렬하지만 열 이름으로도 정렬할 수 있습니다. 10월부터 12월까지 보너스 포인트를 담은 DataFrame을 만들어 보겠습니다:\n\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]])\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\",\"colin\", \"darwin\", \"charles\"])\nbonus_points\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\n\ngrades + bonus_points\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      bob\n      NaN\n      NaN\n      9.0\n      NaN\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n덧셈 연산이 수행되었지만 너무 많은 원소가 NaN이 되었습니다. DataFrame을 정렬할 때 일부 열과 행이 한 쪽에만 있기 때문입니다. 다른 쪽에는 누란되었다고 간주합니다(NaN). NaN에 어떤 수를 더하면 NaN이 됩니다."
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#누락된-데이터-다루기",
    "href": "data_mining/pandas/week_1b_pandas.html#누락된-데이터-다루기",
    "title": "Pandas",
    "section": "누락된 데이터 다루기",
    "text": "누락된 데이터 다루기\n실제 데이터에서 누락된 데이터를 다루는 경우는 자주 발생합니다. 판다스는 누락된 데이터를 다룰 수 있는 몇 가지 방법을 제공합니다.\n위 데이터에 있는 문제를 해결해 보죠. 예를 들어, 누락된 데이터는 NaN이 아니라 0이 되어야 한다고 결정할 수 있습니다. fillna() 메서드를 사용해 모든 NaN 값을 어떤 값으로 바꿀 수 있습니다:\n\n(grades + bonus_points).fillna(0)\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      bob\n      0.0\n      0.0\n      9.0\n      0.0\n    \n    \n      charles\n      0.0\n      5.0\n      11.0\n      0.0\n    \n    \n      colin\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      darwin\n      0.0\n      11.0\n      10.0\n      0.0\n    \n  \n\n\n\n\n9월의 점수를 0으로 만드는 것은 공정하지 않습니다. 누락된 점수는 그대로 두고, 누락된 보너스 포인트는 0으로 바꿀 수 있습니다:\n\nbonus_points\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\n\nfixed_bonus_points = bonus_points.fillna(0) # NA 값 0으로 바꾸기\nfixed_bonus_points.insert(loc=0, column=\"sep\", value=0) # 누락된 컬럼 만들기\nfixed_bonus_points.loc[\"alice\"] = 0 # 누락된 행 만들기\nfixed_bonus_points\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0\n      0.0\n      0.0\n      2.0\n    \n    \n      colin\n      0\n      0.0\n      1.0\n      0.0\n    \n    \n      darwin\n      0\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      0\n      3.0\n      3.0\n      0.0\n    \n    \n      alice\n      0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n\ngrades + fixed_bonus_points\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n      8.0\n      8.0\n    \n    \n      bob\n      NaN\n      9.0\n      9.0\n      10.0\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      4.0\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      9.0\n    \n  \n\n\n\n\n훨씬 낫네요: 일부 데이터를 꾸며냈지만 덜 불공정합니다.\n누락된 값을 다루는 또 다른 방법은 보간입니다. bonus_points DataFrame을 다시 보죠:\n\nbonus_points\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\ninterpolate 메서드를 사용해 보죠. 기본적으로 수직 방향(axis=0)으로 보간합니다. 따라서 수평으로(axis=1)으로 보간하도록 지정합니다.\n\nbonus_points.interpolate(axis=1)\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      1.0\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\nbob의 보너스 포인트는 10월에 0이고 12월에 2입니다. 11월을 보간하면 평균 보너스 포인트 1을 얻습니다. colin의 보너스 포인트는 11월에 1이지만 9월에 포인트는 얼마인지 모릅니다. 따라서 보간할 수 없고 10월의 포인트는 그대로 누락된 값으로 남아 있습니다. 이를 해결하려면 보간하기 전에 9월의 보너스 포인트를 0으로 설정해야 합니다.\n\nbetter_bonus_points = bonus_points.copy()\nbetter_bonus_points.insert(0, \"sep\", 0)\nbetter_bonus_points.loc[\"alice\"] = 0\nbetter_bonus_points = better_bonus_points.interpolate(axis=1)\nbetter_bonus_points\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      0.0\n      1.0\n      2.0\n    \n    \n      colin\n      0.0\n      0.5\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      0.0\n      3.0\n      3.0\n      0.0\n    \n    \n      alice\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n좋습니다. 이제 모든 보너스 포인트가 합리적으로 보간되었습니다. 최종 점수를 확인해 보죠:\n\ngrades + better_bonus_points\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n      8.0\n      8.0\n    \n    \n      bob\n      NaN\n      10.0\n      9.0\n      10.0\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      4.0\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      9.0\n    \n  \n\n\n\n\n9월 열이 오른쪽에 추가되었는데 좀 이상합니다. 이는 더하려는 DataFrame이 정확히 같은 열을 가지고 있지 않기 때문입니다(grade DataFrame에는 \"dec\" 열이 없습니다). 따라서 판다스는 알파벳 순서로 최종 열을 정렬합니다. 이를 해결하려면 덧셈을 하기 전에 누락된 열을 추가하면 됩니다:\n\ngrades[\"dec\"] = np.nan\nfinal_grades = grades + better_bonus_points\nfinal_grades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n    \n  \n\n\n\n\n12월과 colin에 대해 할 수 있는 것이 많지 않습니다. 보너스 포인트를 만드는 것이 나쁘지만 점수를 합리적으로 올릴 수는 없습니다(어떤 선생님들은 그럴 수 있지만). dropna() 메서드를 사용해 모두 NaN인 행을 삭제합니다:\n\nfinal_grades_clean = final_grades.dropna(how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n    \n  \n\n\n\n\n그다음 axis 매개변수를 1로 지정하여 모두 NaN인 열을 삭제합니다:\n\nfinal_grades_clean = final_grades_clean.dropna(axis=1, how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#groupby로-집계하기",
    "href": "data_mining/pandas/week_1b_pandas.html#groupby로-집계하기",
    "title": "Pandas",
    "section": "groupby로 집계하기",
    "text": "groupby로 집계하기\nSQL과 비슷하게 판다스는 데이터를 그룹핑하고 각 그룹에 대해 연산을 수행할 수 있습니다.\n먼저 그루핑을 위해 각 사람의 데이터를 추가로 만들겠습니다. NaN 값을 어떻게 다루는지 보기 위해 final_grades DataFrame을 다시 사용하겠습니다:\n\nfinal_grades[\"hobby\"] = [\"Biking\", \"Dancing\", np.nan, \"Dancing\", \"Biking\"]\nfinal_grades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n      hobby\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n      Biking\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n      Dancing\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n      Dancing\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n      Biking\n    \n  \n\n\n\n\nhobby로 이 DataFrame을 그룹핑해 보죠:\n\ngrouped_grades = final_grades.groupby(\"hobby\")\ngrouped_grades\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x00000273EBA146D0>\n\n\n이제 hobby마다 평균 점수를 계산할 수 있습니다:\n\ngrouped_grades.mean()\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n    \n      hobby\n      \n      \n      \n      \n    \n  \n  \n    \n      Biking\n      8.5\n      9.0\n      10.0\n      NaN\n    \n    \n      Dancing\n      10.0\n      9.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n\nfinal_grades.groupby(\"hobby\").mean()\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n    \n      hobby\n      \n      \n      \n      \n    \n  \n  \n    \n      Biking\n      8.5\n      9.0\n      10.0\n      NaN\n    \n    \n      Dancing\n      10.0\n      9.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n아주 쉽네요! 평균을 계산할 때 NaN 값은 그냥 무시됩니다."
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#피봇-테이블",
    "href": "data_mining/pandas/week_1b_pandas.html#피봇-테이블",
    "title": "Pandas",
    "section": "피봇 테이블",
    "text": "피봇 테이블\n판다스는 스프레드시트와 비슷하 피봇 테이블을 지원하여 데이터를 빠르게 요약할 수 있습니다. 어떻게 동작하는 알아 보기 위해 간단한 DataFrame을 만들어 보죠:\n\nbonus_points.stack().reset_index()\n\n\n\n\n\n  \n    \n      \n      level_0\n      level_1\n      0\n    \n  \n  \n    \n      0\n      bob\n      oct\n      0.0\n    \n    \n      1\n      bob\n      dec\n      2.0\n    \n    \n      2\n      colin\n      nov\n      1.0\n    \n    \n      3\n      colin\n      dec\n      0.0\n    \n    \n      4\n      darwin\n      oct\n      0.0\n    \n    \n      5\n      darwin\n      nov\n      1.0\n    \n    \n      6\n      darwin\n      dec\n      0.0\n    \n    \n      7\n      charles\n      oct\n      3.0\n    \n    \n      8\n      charles\n      nov\n      3.0\n    \n    \n      9\n      charles\n      dec\n      0.0\n    \n  \n\n\n\n\n\nmore_grades = final_grades_clean.stack().reset_index()\nmore_grades.columns = [\"name\", \"month\", \"grade\"]\nmore_grades[\"bonus\"] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0]\nmore_grades\n\n\n\n\n\n  \n    \n      \n      name\n      month\n      grade\n      bonus\n    \n  \n  \n    \n      0\n      alice\n      sep\n      8.0\n      NaN\n    \n    \n      1\n      alice\n      oct\n      8.0\n      NaN\n    \n    \n      2\n      alice\n      nov\n      9.0\n      NaN\n    \n    \n      3\n      bob\n      sep\n      10.0\n      0.0\n    \n    \n      4\n      bob\n      oct\n      9.0\n      NaN\n    \n    \n      5\n      bob\n      nov\n      10.0\n      2.0\n    \n    \n      6\n      charles\n      sep\n      4.0\n      3.0\n    \n    \n      7\n      charles\n      oct\n      11.0\n      3.0\n    \n    \n      8\n      charles\n      nov\n      5.0\n      0.0\n    \n    \n      9\n      darwin\n      sep\n      9.0\n      0.0\n    \n    \n      10\n      darwin\n      oct\n      10.0\n      1.0\n    \n    \n      11\n      darwin\n      nov\n      11.0\n      0.0\n    \n  \n\n\n\n\n이제 이 DataFrame에 대해 pd.pivot_table() 함수를 호출하고 name 열로 그룹핑합니다. 기본적으로 pivot_table()은 수치 열의 평균을 계산합니다:\n\npd.pivot_table(more_grades, index=\"name\")\n\n\n\n\n\n  \n    \n      \n      bonus\n      grade\n    \n    \n      name\n      \n      \n    \n  \n  \n    \n      alice\n      NaN\n      8.333333\n    \n    \n      bob\n      1.000000\n      9.666667\n    \n    \n      charles\n      2.000000\n      6.666667\n    \n    \n      darwin\n      0.333333\n      10.000000\n    \n  \n\n\n\n\n집계 함수를 aggfunc 매개변수로 바꿀 수 있습니다. 또한 집계 대상의 열을 리스트로 지정할 수 있습니다:\n\npd.pivot_table(more_grades, index=\"name\", values=[\"grade\",\"bonus\"], aggfunc=np.max)\n\n\n\n\n\n  \n    \n      \n      bonus\n      grade\n    \n    \n      name\n      \n      \n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n    \n    \n      bob\n      2.0\n      10.0\n    \n    \n      charles\n      3.0\n      11.0\n    \n    \n      darwin\n      1.0\n      11.0\n    \n  \n\n\n\n\ncolumns 매개변수를 지정하여 수평으로 집계할 수 있고 margins=True로 설정해 각 행과 열에 대해 전체 합을 계산할 수 있습니다:\n\npd.pivot_table(more_grades, index=\"name\", values=\"grade\", columns=\"month\", margins=True)\n\n\n\n\n\n  \n    \n      month\n      nov\n      oct\n      sep\n      All\n    \n    \n      name\n      \n      \n      \n      \n    \n  \n  \n    \n      alice\n      9.00\n      8.0\n      8.00\n      8.333333\n    \n    \n      bob\n      10.00\n      9.0\n      10.00\n      9.666667\n    \n    \n      charles\n      5.00\n      11.0\n      4.00\n      6.666667\n    \n    \n      darwin\n      11.00\n      10.0\n      9.00\n      10.000000\n    \n    \n      All\n      8.75\n      9.5\n      7.75\n      8.666667\n    \n  \n\n\n\n\n마지막으로 여러 개의 인덱스나 열 이름을 지정하면 판다스가 다중 레벨 인덱스를 만듭니다:\n\npd.pivot_table(more_grades, index=(\"name\", \"month\"), margins=True)\n\n\n\n\n\n  \n    \n      \n      \n      bonus\n      grade\n    \n    \n      name\n      month\n      \n      \n    \n  \n  \n    \n      alice\n      nov\n      NaN\n      9.00\n    \n    \n      oct\n      NaN\n      8.00\n    \n    \n      sep\n      NaN\n      8.00\n    \n    \n      bob\n      nov\n      2.000\n      10.00\n    \n    \n      oct\n      NaN\n      9.00\n    \n    \n      sep\n      0.000\n      10.00\n    \n    \n      charles\n      nov\n      0.000\n      5.00\n    \n    \n      oct\n      3.000\n      11.00\n    \n    \n      sep\n      3.000\n      4.00\n    \n    \n      darwin\n      nov\n      0.000\n      11.00\n    \n    \n      oct\n      1.000\n      10.00\n    \n    \n      sep\n      0.000\n      9.00\n    \n    \n      All\n      \n      1.125\n      8.75"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#함수",
    "href": "data_mining/pandas/week_1b_pandas.html#함수",
    "title": "Pandas",
    "section": "함수",
    "text": "함수\n큰 DataFrame을 다룰 때 내용을 간단히 요약하는 것이 도움이 됩니다. 판다스는 이를 위한 몇 가지 함수를 제공합니다. 먼저 수치 값, 누락된 값, 텍스트 값이 섞인 큰 DataFrame을 만들어 보죠. 주피터 노트북은 이 DataFrame의 일부만 보여줍니다:\n\nmuch_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26))\nlarge_df = pd.DataFrame(much_data, columns=list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\nlarge_df[large_df % 16 == 0] = np.nan\nlarge_df.insert(3,\"some_text\", \"Blabla\")\nlarge_df\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      0\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      1\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      2\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      3\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n    \n      4\n      44.0\n      55.0\n      88.0\n      Blabla\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n      ...\n      55.0\n      44.0\n      55.0\n      88.0\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9995\n      NaN\n      NaN\n      33.0\n      Blabla\n      88.0\n      165.0\n      77.0\n      11.0\n      154.0\n      132.0\n      ...\n      NaN\n      NaN\n      NaN\n      33.0\n      88.0\n      165.0\n      77.0\n      11.0\n      154.0\n      132.0\n    \n    \n      9996\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      9997\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      9998\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      9999\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n  \n\n10000 rows × 27 columns\n\n\n\nhead() 메서드는 처음 5개 행을 반환합니다:\n\nlarge_df.head(n=10)\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      0\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      1\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      2\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      3\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n    \n      4\n      44.0\n      55.0\n      88.0\n      Blabla\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n      ...\n      55.0\n      44.0\n      55.0\n      88.0\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n    \n    \n      5\n      55.0\n      66.0\n      99.0\n      Blabla\n      154.0\n      44.0\n      143.0\n      77.0\n      33.0\n      11.0\n      ...\n      66.0\n      55.0\n      66.0\n      99.0\n      154.0\n      44.0\n      143.0\n      77.0\n      33.0\n      11.0\n    \n    \n      6\n      66.0\n      77.0\n      110.0\n      Blabla\n      165.0\n      55.0\n      154.0\n      88.0\n      44.0\n      22.0\n      ...\n      77.0\n      66.0\n      77.0\n      110.0\n      165.0\n      55.0\n      154.0\n      88.0\n      44.0\n      22.0\n    \n    \n      7\n      77.0\n      88.0\n      121.0\n      Blabla\n      NaN\n      66.0\n      165.0\n      99.0\n      55.0\n      33.0\n      ...\n      88.0\n      77.0\n      88.0\n      121.0\n      NaN\n      66.0\n      165.0\n      99.0\n      55.0\n      33.0\n    \n    \n      8\n      88.0\n      99.0\n      132.0\n      Blabla\n      NaN\n      77.0\n      NaN\n      110.0\n      66.0\n      44.0\n      ...\n      99.0\n      88.0\n      99.0\n      132.0\n      NaN\n      77.0\n      NaN\n      110.0\n      66.0\n      44.0\n    \n    \n      9\n      99.0\n      110.0\n      143.0\n      Blabla\n      11.0\n      88.0\n      NaN\n      121.0\n      77.0\n      55.0\n      ...\n      110.0\n      99.0\n      110.0\n      143.0\n      11.0\n      88.0\n      NaN\n      121.0\n      77.0\n      55.0\n    \n  \n\n10 rows × 27 columns\n\n\n\n마지막 5개 행을 반환하는 tail() 함수도 있습니다. 원하는 행 개수를 전달할 수도 있습니다:\n\nlarge_df.tail(n=2)\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      9998\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      9999\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n  \n\n2 rows × 27 columns\n\n\n\ninfo() 메서드는 각 열의 내용을 요약하여 출력합니다:\n\nlarge_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 27 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   A          8823 non-null   float64\n 1   B          8824 non-null   float64\n 2   C          8824 non-null   float64\n 3   some_text  10000 non-null  object \n 4   D          8824 non-null   float64\n 5   E          8822 non-null   float64\n 6   F          8824 non-null   float64\n 7   G          8824 non-null   float64\n 8   H          8822 non-null   float64\n 9   I          8823 non-null   float64\n 10  J          8823 non-null   float64\n 11  K          8822 non-null   float64\n 12  L          8824 non-null   float64\n 13  M          8824 non-null   float64\n 14  N          8822 non-null   float64\n 15  O          8824 non-null   float64\n 16  P          8824 non-null   float64\n 17  Q          8824 non-null   float64\n 18  R          8823 non-null   float64\n 19  S          8824 non-null   float64\n 20  T          8824 non-null   float64\n 21  U          8824 non-null   float64\n 22  V          8822 non-null   float64\n 23  W          8824 non-null   float64\n 24  X          8824 non-null   float64\n 25  Y          8822 non-null   float64\n 26  Z          8823 non-null   float64\ndtypes: float64(26), object(1)\nmemory usage: 2.1+ MB\n\n\n마지막으로 describe() 메서드는 각 열에 대한 주요 집계 연산을 수행한 결과를 보여줍니다:\nFinally, the describe() method gives a nice overview of the main aggregated values over each column: * count: null(NaN)이 아닌 값의 개수 * mean: null이 아닌 값의 평균 * std: null이 아닌 값의 표준 편차 * min: null이 아닌 값의 최솟값 * 25%, 50%, 75%: null이 아닌 값의 25번째, 50번째, 75번째 백분위수 * max: null이 아닌 값의 최댓값\n\nlarge_df.describe()\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n      F\n      G\n      H\n      I\n      J\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      count\n      8823.000000\n      8824.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8823.000000\n      8823.000000\n      ...\n      8824.000000\n      8823.000000\n      8824.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8823.000000\n    \n    \n      mean\n      87.977559\n      87.972575\n      87.987534\n      88.012466\n      87.983791\n      88.007480\n      87.977561\n      88.000000\n      88.022441\n      88.022441\n      ...\n      87.972575\n      87.977559\n      87.972575\n      87.987534\n      88.012466\n      87.983791\n      88.007480\n      87.977561\n      88.000000\n      88.022441\n    \n    \n      std\n      47.535911\n      47.535523\n      47.521679\n      47.521679\n      47.535001\n      47.519371\n      47.529755\n      47.536879\n      47.535911\n      47.535911\n      ...\n      47.535523\n      47.535911\n      47.535523\n      47.521679\n      47.521679\n      47.535001\n      47.519371\n      47.529755\n      47.536879\n      47.535911\n    \n    \n      min\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      ...\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n    \n    \n      25%\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      ...\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n    \n    \n      50%\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      ...\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n    \n    \n      75%\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      ...\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n    \n    \n      max\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      ...\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n    \n  \n\n8 rows × 26 columns"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#저장",
    "href": "data_mining/pandas/week_1b_pandas.html#저장",
    "title": "Pandas",
    "section": "저장",
    "text": "저장\nCSV, HTML, JSON로 저장해 보죠:\n\nmy_df.to_csv(\"my_df.csv\")\nmy_df.to_html(\"my_df.html\")\nmy_df.to_json(\"my_df.json\")\n\n저장된 내용을 확인해 보죠:\n\nfor filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"):\n    print(\"#\", filename)\n    with open(filename, \"rt\") as f:\n        print(f.read())\n        print()\n\n# my_df.csv\n,hobby,weight,birthyear,children\nalice,Biking,68.5,1985,\nbob,Dancing,83.1,1984,3.0\n\n\n# my_df.html\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hobby</th>\n      <th>weight</th>\n      <th>birthyear</th>\n      <th>children</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alice</th>\n      <td>Biking</td>\n      <td>68.5</td>\n      <td>1985</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>bob</th>\n      <td>Dancing</td>\n      <td>83.1</td>\n      <td>1984</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n\n# my_df.json\n{\"hobby\":{\"alice\":\"Biking\",\"bob\":\"Dancing\"},\"weight\":{\"alice\":68.5,\"bob\":83.1},\"birthyear\":{\"alice\":1985,\"bob\":1984},\"children\":{\"alice\":null,\"bob\":3.0}}\n\n\n\n인덱스는 (이름 없이) CSV 파일의 첫 번째 열에 저장되었습니다. HTML에서는 <th> 태그와 JSON에서는 키로 저장되었습니다.\n다른 포맷으로 저장하는 것도 비슷합니다. 하지만 일부 포맷은 추가적인 라이브러리 설치가 필요합니다. 예를 들어, 엑셀로 저장하려면 openpyxl 라이브러리가 필요합니다:\n\ntry:\n    my_df.to_excel(\"my_df.xlsx\", sheet_name='People')\nexcept ImportError as e:\n    print(e)\n\nNo module named 'openpyxl'"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#로딩",
    "href": "data_mining/pandas/week_1b_pandas.html#로딩",
    "title": "Pandas",
    "section": "로딩",
    "text": "로딩\nCSV 파일을 DataFrame으로 로드해 보죠:\n\nmy_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0)\nmy_df_loaded\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      birthyear\n      children\n    \n  \n  \n    \n      alice\n      Biking\n      68.5\n      1985\n      NaN\n    \n    \n      bob\n      Dancing\n      83.1\n      1984\n      3.0\n    \n  \n\n\n\n\n예상할 수 있듯이 read_json, read_html, read_excel 함수도 있습니다. 인터넷에서 데이터를 바로 읽을 수도 있습니다. 예를 들어 깃허브에서 1,000개의 U.S. 도시를 로드해 보죠:\n\nus_cities = None\ntry:\n    csv_url = \"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\"\n    us_cities = pd.read_csv(csv_url, index_col=0)\n    us_cities = us_cities.head()\nexcept IOError as e:\n    print(e)\nus_cities\n\n\n\n\n\n  \n    \n      \n      State\n      Population\n      lat\n      lon\n    \n    \n      City\n      \n      \n      \n      \n    \n  \n  \n    \n      Marysville\n      Washington\n      63269\n      48.051764\n      -122.177082\n    \n    \n      Perris\n      California\n      72326\n      33.782519\n      -117.228648\n    \n    \n      Cleveland\n      Ohio\n      390113\n      41.499320\n      -81.694361\n    \n    \n      Worcester\n      Massachusetts\n      182544\n      42.262593\n      -71.802293\n    \n    \n      Columbia\n      South Carolina\n      133358\n      34.000710\n      -81.034814\n    \n  \n\n\n\n\n이외에도 많은 옵션이 있습니다. 특히 datetime 포맷에 관련된 옵션이 많습니다. 더 자세한 내용은 온라인 문서를 참고하세요."
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#sql-조인",
    "href": "data_mining/pandas/week_1b_pandas.html#sql-조인",
    "title": "Pandas",
    "section": "SQL 조인",
    "text": "SQL 조인\n판다스의 강력한 기능 중 하나는 DataFrame에 대해 SQL 같은 조인(join)을 수행할 수 있는 것입니다. 여러 종류의 조인이 지원됩니다. 이너 조인(inner join), 레프트/라이트 아우터 조인(left/right outer join), 풀 조인(full join)입니다. 이에 대해 알아 보기 위해 간단한 DataFrame을 만들어 보죠:\n\ncity_loc = pd.DataFrame(\n    [\n        [\"CA\", \"San Francisco\", 37.781334, -122.416728],\n        [\"NY\", \"New York\", 40.705649, -74.008344],\n        [\"FL\", \"Miami\", 25.791100, -80.320733],\n        [\"OH\", \"Cleveland\", 41.473508, -81.739791],\n        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657]\n    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\ncity_loc\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n    \n  \n\n\n\n\n\ncity_pop = pd.DataFrame(\n    [\n        [808976, \"San Francisco\", \"California\"],\n        [8363710, \"New York\", \"New-York\"],\n        [413201, \"Miami\", \"Florida\"],\n        [2242193, \"Houston\", \"Texas\"]\n    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"])\ncity_pop\n\n\n\n\n\n  \n    \n      \n      population\n      city\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n이제 merge() 함수를 사용해 이 DataFrame을 조인해 보죠:\n\npd.merge(left=city_loc, right=city_pop, on=\"city\")\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Florida\n    \n  \n\n\n\n\n두 DataFrame은 state란 이름의 열을 가지고 있으므로 state_x와 state_y로 이름이 바뀌었습니다.\n또한 Cleveland, Salt Lake City, Houston은 두 DataFrame에 모두 존재하지 않기 때문에 삭제되었습니다. SQL의 INNER JOIN과 동일합니다. 도시를 삭제하지 않고 NaN으로 채우는 FULL OUTER JOIN을 원하면 how=\"outer\"로 지정합니다:\n\nall_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\")\nall_cities\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976.0\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710.0\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201.0\n      Florida\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n      NaN\n    \n    \n      5\n      NaN\n      Houston\n      NaN\n      NaN\n      2242193.0\n      Texas\n    \n  \n\n\n\n\n물론 LEFT OUTER JOIN은 how=\"left\"로 지정할 수 있습니다. 왼쪽의 DataFrame에 있는 도시만 남습니다. 비슷하게 how=\"right\"는 오른쪽 DataFrame에 있는 도시만 결과에 남습니다. 예를 들면:\n\npd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\")\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Florida\n    \n    \n      3\n      NaN\n      Houston\n      NaN\n      NaN\n      2242193\n      Texas\n    \n  \n\n\n\n\n조인할 키가 DataFrame 인덱스라면 left_index=True나 right_index=True로 지정해야 합니다. 키 열의 이름이 다르면 left_on과 right_on을 사용합니다. 예를 들어:\n\ncity_pop2 = city_pop.copy()\ncity_pop2.columns = [\"population\", \"name\", \"state\"]\ncity_pop2\n\n\n\n\n\n  \n    \n      \n      population\n      name\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n\npd.merge(left=city_loc, right=city_pop2, left_on=\"city\", right_on=\"name\")\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      name\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      San Francisco\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New York\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Miami\n      Florida"
  },
  {
    "objectID": "data_mining/pandas/week_1b_pandas.html#연결",
    "href": "data_mining/pandas/week_1b_pandas.html#연결",
    "title": "Pandas",
    "section": "연결",
    "text": "연결\nDataFrame을 조인하는 대신 그냥 연결할 수도 있습니다. concat() 함수가 하는 일입니다:\n\ncity_loc\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n    \n  \n\n\n\n\n\ncity_pop\n\n\n\n\n\n  \n    \n      \n      population\n      city\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n\nresult_concat = pd.concat([city_loc, city_pop])\nresult_concat\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n이 연산은 (행을 따라) 수직적으로 데이터를 연결하고 (열을 따라) 수평으로 연결하지 않습니다. 이 예에서 동일한 인덱스를 가진 행이 있습니다(예를 들면 3). 판다스는 이를 우아하게 처리합니다:\n\nresult_concat.loc[3]\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n  \n\n\n\n\n또는 인덱스를 무시하도록 설정할 수 있습니다:\n\npd.concat([city_loc, city_pop], ignore_index=True)\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      5\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      6\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      7\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      8\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n한 DataFrame에 열이 없을 때 NaN이 채워져 있는 것처럼 동작합니다. join=\"inner\"로 설정하면 양쪽의 DataFrame에 존재하는 열만 반환됩니다:\n\npd.concat([city_loc, city_pop], join=\"inner\")\n\n\n\n\n\n  \n    \n      \n      state\n      city\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n    \n    \n      1\n      NY\n      New York\n    \n    \n      2\n      FL\n      Miami\n    \n    \n      3\n      OH\n      Cleveland\n    \n    \n      4\n      UT\n      Salt Lake City\n    \n    \n      3\n      California\n      San Francisco\n    \n    \n      4\n      New-York\n      New York\n    \n    \n      5\n      Florida\n      Miami\n    \n    \n      6\n      Texas\n      Houston\n    \n  \n\n\n\n\naxis=1로 설정하면 DataFrame을 수직이 아니라 수평으로 연결할 수 있습니다:\n\npd.concat([city_loc, city_pop], axis=1)\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n      city\n      state\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      808976.0\n      San Francisco\n      California\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      8363710.0\n      New York\n      New-York\n    \n    \n      5\n      NaN\n      NaN\n      NaN\n      NaN\n      413201.0\n      Miami\n      Florida\n    \n    \n      6\n      NaN\n      NaN\n      NaN\n      NaN\n      2242193.0\n      Houston\n      Texas\n    \n  \n\n\n\n\n이 경우 인덱스가 잘 정렬되지 않기 때문에 의미가 없습니다(예를 들어 Cleveland와 San Francisco의 인덱스 레이블이 3이기 때문에 동일한 행에 놓여 있습니다). 이 DataFrame을 연결하기 전에 도시로 인덱스를 재설정해 보죠:\n\npd.concat([city_loc.set_index(\"city\"), city_pop.set_index(\"city\")], axis=1)\n\n\n\n\n\n  \n    \n      \n      state\n      lat\n      lng\n      population\n      state\n    \n    \n      city\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      San Francisco\n      CA\n      37.781334\n      -122.416728\n      808976.0\n      California\n    \n    \n      New York\n      NY\n      40.705649\n      -74.008344\n      8363710.0\n      New-York\n    \n    \n      Miami\n      FL\n      25.791100\n      -80.320733\n      413201.0\n      Florida\n    \n    \n      Cleveland\n      OH\n      41.473508\n      -81.739791\n      NaN\n      NaN\n    \n    \n      Salt Lake City\n      UT\n      40.755851\n      -111.896657\n      NaN\n      NaN\n    \n    \n      Houston\n      NaN\n      NaN\n      NaN\n      2242193.0\n      Texas\n    \n  \n\n\n\n\nFULL OUTER JOIN을 수행한 것과 비슷합니다. 하지만 state 열이 state_x와 state_y로 바뀌지 않았고 city 열이 인덱스가 되었습니다.\nappend() 메서드는 DataFrame을 수직으로 연결하는 단축 메서드입니다:\n\ncity_loc.append(city_pop)\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n\npd.concat([city_loc,city_pop])\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n판다스의 다른 메서드와 마찬가지로 append() 메서드는 실제 city_loc을 수정하지 않습니다. 복사본을 만들어 수정한 다음 반환합니다."
  },
  {
    "objectID": "geo/4/ch4.html",
    "href": "geo/4/ch4.html",
    "title": "Chapter 4 Spatial data operations",
    "section": "",
    "text": "공간정보분석 4장 코드"
  },
  {
    "objectID": "geo/4/ch4.html#소개",
    "href": "geo/4/ch4.html#소개",
    "title": "Chapter 4 Spatial data operations",
    "section": "4.1 소개",
    "text": "4.1 소개"
  },
  {
    "objectID": "geo/4/ch4.html#벡터-데이터에-대한-공간-작업",
    "href": "geo/4/ch4.html#벡터-데이터에-대한-공간-작업",
    "title": "Chapter 4 Spatial data operations",
    "section": "4.2 벡터 데이터에 대한 공간 작업",
    "text": "4.2 벡터 데이터에 대한 공간 작업\n\n이번 세션은 sf 패키지 에서 단순 기능으로 표현된 벡터 지리 데이터에 대한 공간 연산의 개요를 제공함\n\n\n4.2.1 Spatial subsetting(공간 부분 집합)\n\n공간부분집합의 예는 spData의 nz 의 및 nz_height데이터 세트를 활용\n\n\nnz                     # MULTIPOLYGON의 해당되는 정보\ncanterbury  <-  nz %>% filter(Name == \"Canterbury\")\ncanterbury             # MULTIPOLYGON\ncanterbury_height <-  nz_height[canterbury, ]\ncanterbury_height      # POINT\n\nnz_height[canterbury,]\nplot(nz_height[canterbury,])\n\nnz_height[canterbury, , op = st_disjoint]\nplot(nz_height[canterbury, , op = st_disjoint])\n\nnz_height[canterbury, 2, op = st_disjoint]\nplot(nz_height[canterbury, 2, op = st_disjoint])\n\n\nst_intersects() 를 활용한 공간 부분 집합 추출\n\n\nsel_sgbp <-  st_intersects(x = nz_height, y = canterbury) # 두 공간에서 겹치는 지역은 1로 출력 아닌 것은 empty로 출력\nclass(sel_sgbp)\n#> [1] \"sgbp\" \"list\"\nsel_logical  <-  lengths(sel_sgbp) > 0 # \ncanterbury_height2 <-  nz_height[sel_logical, ]\ncanterbury_height2\n\n\nfilter()함수를 활용한 공간부분집합 추출\n\n\ncanterbury_height3  <-  nz_height %>%\n  filter(st_intersects(x = ., y = canterbury, sparse = FALSE))\n\n#아래 두개의 차이 확인\nst_intersects(x = nz_height, y = canterbury, sparse = FALSE)\nst_intersects(x = nz_height, y = canterbury)\n\n\n\n4.2.2 Topological relations(위상 관계)\n\n위상 관계는 객체의 간의 공간적 관계를 설명함\n\n이해를 돕기 위해 아래와 같이 (a) 폴리곤, (l) 라인, (p) 포인트를 생성함\n\n# create a polygon\na_poly  <-  st_polygon(list(rbind(c(-1, -1), c(1, -1), c(1, 1), c(-1, -1))))\na <-   st_sfc(a_poly)\n# create a line\nl_line <-   st_linestring(x = matrix(c(-1, -1, -0.5, 1), ncol = 2))\nl <-   st_sfc(l_line)\n# create points\np_matrix <- matrix(c(0.5, 1, -1, 0, 0, 1, 0.5, 1), ncol = 2)\np_multi <- st_multipoint(x = p_matrix)\np <- st_cast(st_sfc(p_multi), \"POINT\")\n\nplot(a, col = c(\"gray\"), border = c(\"red\"))\nplot(l,add = T)\nplot(p,add = T)\nbox(col=\"black\")\n\naxis(side = 1, at = seq(-1.0, 1.0, 0.5), tck = 0.02)\naxis(side = 2, at = seq(-1, 1, 0.5), tck = 0.02, las = 1)\ntext(p_matrix,pos = 1) # 1 = 6시 2 = 9시 3 = 12시 4 = 3시 \n\n\n\n폴리곤 a에 교차하는 p포인트는?\n\n1번과 2번 포인트가 삼각형 내에 또는 위에 있음\nst_intersects(p, a)를 활용하여 구할 수 있음\nsparse = FALSE로 설정하면 TRUE or FALSE로 출력\n\n\n\nst_intersects(p, a)\n#> Sparse geometry binary ..., where the predicate was `intersects'\n#> 1: 1\n#> 2: 1\n#> 3: (empty)\n#> 4: (empty)\n\nst_intersects(p, a, sparse = FALSE)\n#>       [,1]\n#> [1,]  TRUE\n#> [2,]  TRUE\n#> [3,] FALSE\n#> [4,] FALSE\n\nst_intersects() 의 반대는 st_disjoint()이며, 이는 선택 객체와 어떤 식으로든 공간적으로 관련되지 않는 객체만 반환(참고 [, 1]은 결과를 벡터로 변환)\n\n\nst_disjoint(p, a, sparse = F)[, 1]\n#> [1] FALSE FALSE  TRUE  TRUE\n\n\nst_within() 는 완전히 객체 내부에 있는 객체들만 TRUE로 출력\n\n삼각형 내부에 있는 1번째 포인트만 TRUE로 출력\n\n\n\nst_within(p, a, sparse = FALSE)[, 1]\n#> [1]  TRUE FALSE FALSE FALSE\n\n\nst_touches() 는 삼각형 위(테두리)에 있는 객체들 출력\n\n삼각형 테두리에 있는 2번째 포인트만 TRUE로 출력\n\n\n\nst_touches(p, a, sparse = FALSE)[, 1]\n#> [1] FALSE  TRUE FALSE FALSE\n\n\nst_is_within_distance는 삼각형에서 주어진 거리보다 가까운 객체들을 반환\n\n\nsel  <-  st_is_within_distance(p, a, dist = 0.9) # can only return a sparse matrix\nlengths(sel) > 0\n#> [1]  TRUE  TRUE FALSE  TRUE"
  },
  {
    "objectID": "geo/3/ch3.html",
    "href": "geo/3/ch3.html",
    "title": "Chapter 3 Attribute data operations",
    "section": "",
    "text": "공간정보분석 3장 코드"
  },
  {
    "objectID": "geo/2/ch2.html",
    "href": "geo/2/ch2.html",
    "title": "Chapter 2 Geograpic data in R",
    "section": "",
    "text": "공간정보분석 2장 코드"
  },
  {
    "objectID": "geo/2/ch2.html#소개",
    "href": "geo/2/ch2.html#소개",
    "title": "Chapter 2 Geograpic data in R",
    "section": "2.1 소개",
    "text": "2.1 소개\nVector data VS. Raster data"
  },
  {
    "objectID": "geo/2/ch2.html#벡터-데이터",
    "href": "geo/2/ch2.html#벡터-데이터",
    "title": "Chapter 2 Geograpic data in R",
    "section": "2.2 벡터 데이터",
    "text": "2.2 벡터 데이터\n\n벡터 데이터 의 두가지 의미 (혼동주의)\n\n공간 위치데이터의 점,선,면을 나타내는 데이터\ndata.frame 과 matrix 같은 R class\n\n\n\nsf패키지(Simple Features)\n\nsf 패키지는 sp 패키지의 기능을 승계하였으며, 이에 더해 지리공간 데이터를 읽고 쓰는 ‘GDAL’, 지리적 연산을 할 때 사용하는 ‘GEOS’, 지도의 투영 변환(projection conversions)과 데이터 변환(datum transformations)을 위한 ‘PROJ’ 와 R과의 인터페이스를 제공\n선택적으로 지리적 좌표에 대한 구면 기하 연산 (spherical geometry operations) 을 위해 ‘s2’ 패키지를 사용\nsf 는 모든 벡터 유형(점,선,면, 다각형)을 지원함(raster는 지원하지 않음)\nsf 패키지의 장점\n\n지리공간 벡터 데이터를 빠르게 읽고 쓸 수 있음\n지리공간 벡터 데이터 시각화 성능의 고도화(tmap, leaflet, mapview 지리공간 데이터 시각화 패키지가 sf 클래스 지원)\n대부분의 연산에서 sf 객체는 DataFrame 처럼 처리가 가능함\nsf 함수들은 ‘%>%’ 연산자 (chain operator) 와 함께 사용할 수 있고, R의 tidyverse 패키지들과도 잘 작동함(sp 패키지도 spdplyr 패키지를 설치하면 dplyr의 %>% 체인 연산자와 기능을 사용할 수 있음)\nsf 함수이름은 st_ 로 시작하여 상대적으로 일관성이 있고 직관적임\n\n\n\n# sf패키지 확인\nvignette(package = \"sf\") # vignetee(package = \"\"): 비니에트 함수는 설치된 모든 패키지에 대한 이용가능한 모든 목록을 출력 \nvignette(\"sf1\") # 패키지에 대한 소개\n\n\nworld데이터셋은 spData에 의해 제공된\n\n\n> names(world)\n  [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"      \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\" \n> plot(world)\n\n\n\n> world_mini = world[1:2,1:3]\n> world_mini\nSimple feature collection with 2 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: -0.95\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 4\n  iso_a2 name_long continent                                                                                    geom\n  <chr>  <chr>     <chr>                                                                          <MULTIPOLYGON [°]>\n1 FJ     Fiji      Oceania   (((-180 -16.55522, -179.9174 -16.50178, -179.7933 -16.02088, -180 -16.06713, -180 -1...\n2 TZ     Tanzania  Africa    (((33.90371 -0.95, 31.86617 -1.02736, 30.76986 -1.01455, 30.4191 -1.134659, 30.81613...\n\n\n기존의 sp에 사용되는 공간 데이터는 sf로 변환을 통해 사용 가능\n\nst_as_sf(): sf로 변환 하는 함수\n\n\n\nlibrary(sp)\nworld_sp = as(world, Class = \"Sptial\")\nworld_sf = st_as_sf(world_sp)\n\n\n\nplot 함수를 이용해서 기본 지도 만들기\n\n\nplot(world[3:6])\nplot(world[\"pop\"])\n\n\n\n\n\n\n\n\n\n\n\n\n다른 지도층을 추가하기\n\nplot() 함수 내에 add = TRUE 매개변수를 사용하면 나중에 그 위에 다른 지도를 겹쳐서, 즉 층을 추가하여 지도를 덮어쓰기로 그릴 수 있음\n단, 첫번째 지도 그래프에 키(key)가 있을 경우에는 reset = FALSE 매개변수를 꼭 설정해준 다음에, 이후에 다음번 plot(add = TRUE)를 사용\n\n\n\nworld_aisa = world[world$continent == \"Asia\",] #국가에서 아시아만 뽑은 후\nasia = st_union(world_aisa) #아시아 이름으로 하나의 좌표로 합침\n\n#아시아만 빨간색으로 표시\nplot(world[\"pop\"], reset=FALSE) #reset =FLASE이면 지도 요소를 더 추가할 수 있는 모드로 플롯을 유지\nplot(asia,add=TRUE,col = \"red\")  #add=TRUE : 기존에 있는거 유지 후 그 위에다 그림\n\n\n\n\n\n\n\n\n\n\n\n\n\nBase Plot arguments\n\n대륙별 중심점에 원을 덮어 씌우기\n\nst_centroid(): 폴리곤의 중심점을 계산하는 함수\nof_largest = TRUE : if TRUE, return centroid of the largest (sub)polygon of a MULTIPOLYGON rather than of the whole MULTIPOLYGON\n\n\n\nplot(world[\"continent\"],reset = FALSE)  #국가를 표시 하고 무언갈 더 추가할수 있게 FALSE로 해 놓기\ncex = sqrt(world$pop)/10000 # pop변수에 제곱근을 취하고 1000으로 나누어성 지도 시각화를 위해 크기를 맞춤\nworld_cents = st_centroid(world, of_largest = TRUE) # 다각형(국가별) 중앙점 계산\nplot(st_geometry(world_cents),add = TRUE,cex = cex) # 인구크기에 따라 대륙별 중앙점에 원그려넣기\n\n\n\n특정 나라를 중심으로 확장하여 주변 나라 표시하기\n\n\nindia = world[world$name_long == \"India\", ]\nplot(st_geometry(india),expandBB=c(0, 0.2, 0.1, 1),col = \"gray\",lwd = 3)\nplot(world_aisa[0],add=TRUE)\n\n\nexpandBB: 각 방향으로 경계 상자를 확장(아래, 왼쪽, 위, 오른쪽)\nlwd: 선굵기\nworld_asia[0]: 아시아에 대한 geometry column \n\n\n\nGeometry types\n\nsf패키지에서 지원하는 17개의 geometry types이 있음\n\nthe seven most commonly used types:\n\nPOINT (5 2)\nLINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\nMULTIPOINT (5 2, 1 3, 3 4, 3 2)\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\nMULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5), (0 2, 1 2, 1 3, 0 3, 0 2)))  \nGEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)) \n\n\n\n\n\nSimple feature geometries(sfg)\n\nsfg는 “Simple Feature Geometries”의 약어로, 공간 데이터의 기하학적 특성을 표현하는 방법 중 하나\nsfg는 지오메트리 객체를 나타내며, 일반적으로 점(Point), 선(Line), 면(Polygon) 등과 같은 기본 기하학적 요소를 포함\nR에서 simple feature geometry types\n\nA point: st_point()\nA linestring: st_linestring()\nA polygon: st_polygon()\nA multipoint: st_multipoint()\nA multilinestring: st_multilinestring()\nA multipolygon: st_multipolygon()\nA geometry collection: st_geometrycollection()\n\nsfg objects can be created from 3 base R data types:\n\n\nA numeric vector: a single point\nA matrix: a set of points, where each row represents a point, a multipoint or linestring\nA list: a collection of objects such as matrices, multilinestrings or geometry collections\n\n\nst_point()\n\n\nst_point(c(5, 2))                 # XY point\n#> POINT (5 2)\nst_point(c(5, 2, 3))              # XYZ point\n#> POINT Z (5 2 3)\nst_point(c(5, 2, 1), dim = \"XYM\") # XYM point\n#> POINT M (5 2 1)\nst_point(c(5, 2, 3, 1))           # XYZM point\n#> POINT ZM (5 2 3 1)\n\n\nmultipoint (st_multipoint()) and linestring (st_linestring())\n\n\n# the rbind function simplifies the creation of matrices\n## MULTIPOINT\nmultipoint_matrix = rbind(c(5, 2), c(1, 3), c(3, 4), c(3, 2))\nst_multipoint(multipoint_matrix)\n#> MULTIPOINT ((5 2), (1 3), (3 4), (3 2))\n## LINESTRING\nlinestring_matrix = rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2))\nst_linestring(linestring_matrix)\n#> LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\n\n\nist를 사용 : multilinestrings, (multi-)polygons and geometry collections\n\n\n## POLYGON\npolygon_list = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\nst_polygon(polygon_list)\n#> POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\n## POLYGON with a hole\npolygon_border = rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))\npolygon_hole = rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4))\npolygon_with_hole_list = list(polygon_border, polygon_hole)\nst_polygon(polygon_with_hole_list)\n#> POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4))\n\n## MULTILINESTRING\nmultilinestring_list = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), \n                            rbind(c(1, 2), c(2, 4)))\nst_multilinestring((multilinestring_list))\n#> MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\n\n## MULTIPOLYGON\nmultipolygon_list = list(list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))),\n                         list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2))))\nst_multipolygon(multipolygon_list)\n#> MULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5)), ((0 2, 1 2, 1 3, 0 3, 0 2)))\n\n## GEOMETRYCOLLECTION\ngemetrycollection_list = list(st_multipoint(multipoint_matrix),\n                              st_linestring(linestring_matrix))\nst_geometrycollection(gemetrycollection_list)\n#> GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2),\n#>   LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))\n\n\n\nSimple feature columns(sfc)\n\n두 개의 지리특성(features)를 하나의 칼럼 객체로 합침\nSFC는 “Simple Feature Columns”의 약어로, 공간 데이터를 표현하는 방법 중 하나\nSFC는 일반적으로 지리 정보 시스템(GIS)에서 사용되며, 지도 및 공간 데이터를 저장하고 분석하는 데 사용됨\nSFC는 일반적으로 공간 데이터를 테이블 형태로 나타내며, 각 행은 하나의 공간 객체를 나타냄\n\n예를 들어, 도시의 경계를 포함하는 행정 구역 데이터를 저장할 때, 각 행은 구역의 이름, 인구, 경계 등을 포함하는 속성 데이터와 함께 구역의 경계를 나타내는 지오메트리 데이터를 포함\nsfc와 sfg는 공간 데이터를 다루는 데 사용되는 서로 다른 개념. sfc는 공간 데이터를 저장하고 관리하는 방법을 나타내며, sfg는 공간 데이터의 기하학적 특성을 표현하는 방법을 나타냄\nsfc 공간 데이터를 sfg공간 데이터로 변경할 때 st_sfc() 함수를 사용\n\na list of sfg \n동일한 단순 지리특성 기하 유형 합치기\n\n두개의 단순 지리특성 기하 점(2 sfg points)를 한 개의 단순 지리특성 칼럼(1 sfc) 객체로 합치기\n두개의 단순 지리특성 기하 면(2 sfg polygons)를 한 개의 단순 지리특성 칼럼(1 sfc) 객체로 합치기\n\n서로 다른 단순 지리특성 기하 유형 합치기\n\n단순 지리특성 기하 점과 면을 합쳐서 한 개의 단순 지리특성 칼럼(1 sfc) 객체로 만들기\n\n\n두개의 단순 지리특성 기하 점(2 sfg points)를 st_sfc() 함수로 한개의 단순 지리특성 칼럼(1 sfc) 객체로 합치기\n\n::: {.cell}\n# sfc POINT\npoint1 = st_point(c(5, 2))\npoint2 = st_point(c(1, 3))\npoints_sfc = st_sfc(point1, point2)\npoints_sfc\n#> Geometry set for 2 features \n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 1 ymin: 2 xmax: 5 ymax: 3\n#> CRS:           NA\n#> POINT (5 2)\n#> POINT (1 3)\n:::\n\n두개의 단순 지리특성 기하 면(2 sfg polygons)를 st_sfc() 함수로 한개의 단순 지리특성 칼럼(1 sfc) 객체로 합치기\n\n\nst_geometry_type(): 기하유형을 확인\n\n::: {.cell}\n# sfc POLYGON\npolygon_list1 = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\npolygon1 = st_polygon(polygon_list1)\npolygon_list2 = list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))\npolygon2 = st_polygon(polygon_list2)\npolygon_sfc = st_sfc(polygon1, polygon2)\npolygon_sfc\nst_geometry_type(polygon_sfc)\n#> [1] POLYGON POLYGON\n#> 18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n# sfc MULTILINESTRING\nmultilinestring_list1 = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), \n                            rbind(c(1, 2), c(2, 4)))\nmultilinestring1 = st_multilinestring((multilinestring_list1))\nmultilinestring_list2 = list(rbind(c(2, 9), c(7, 9), c(5, 6), c(4, 7), c(2, 7)), \n                            rbind(c(1, 7), c(3, 8)))\nmultilinestring2 = st_multilinestring((multilinestring_list2))\nmultilinestring_sfc = st_sfc(multilinestring1, multilinestring2)\nmultilinestring_sfc\nst_geometry_type(multilinestring_sfc)\n:::\n\n단순 지리특성 기하 점과 면을 st_sfc() 함수로 합쳐서 한개의 단순 지리특성 칼럼(1 sfc) 객체로 만들기\n\n::: {.cell}\n# sfc GEOMETRY\npoint_multilinestring_sfc = st_sfc(point1, multilinestring1)\nst_geometry_type(point_multilinestring_sfc)\n#> [1] POINT           MULTILINESTRING\n#> 18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n:::\n\nsfc 객체는 CRS(coordinate reference systems, 좌표계시스템) 에 대한 정보를 추가로 저장할 수 있음\n\n특정 CRS를 지정하기 위해 (a)epsg (SRID)또는 (b)proj4string속성을 사용할수 있음\n\n\nst_crs(points_sfc)\n#> Coordinate Reference System: NA\n\n\n\n☐ 좌표계 정보를 추가하는 방법\n\nepsg 코드를 입력\n\n\nepsg코드 장점\n\n짧아서 기억하기 쉬움\n\nEPSG: European Petroleum Survey Group, 지도 투영과 datums 에 대한 좌표계 정보 데이터베이스를 제공\n\n\nsfc 객체 내에 모든 geometries는 동일한 CRS를 가져야함\n\nepsg code 를 4326 로 설정\n\nEPSG:4326 → WGS84 경위도: GPS가 사용하는 좌표계\n\n서비스: 구글 지구(Google Earth)\n\n단위: 소수점 (decimal degrees)\n\n+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n    # EPSG definition\npoints_sfc_wgs = st_sfc(point1, point2, crs = 4326)\nst_crs(points_sfc_wgs)\n#> Coordinate Reference System:\n#>   User input: EPSG:4326 \n#>   wkt:\n#> GEOGCRS[\"WGS 84\",\n#>     DATUM[\"World Geodetic System 1984\",\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Horizontal component of 3D system.\"],\n#>         AREA[\"World.\"],\n#>         BBOX[-90,-180,90,180]],\n#>     ID[\"EPSG\",4326]]\n\n\n\n\nPROJ.4 문자열을 직접 입력\n\n\nproj4string 장단점\n\n투사 유형이나 datum, 타원체 등의 다른 모수들을 구체화할 수 있는 유연성이 있음\n사용자가 구체화를 해야 하므로 길고 복잡하며 기억하기 어려움\nproj4string은 문자열 형식으로 저장되며, 일반적으로 PROJ.4 라이브러리에서 사용하는 형식과 호환됨\n이 문자열에는 좌표계의 이름, 중앙 메리디언, 기준 위도 및 경도, 원점 위도 및 경도, 스케일링 요소 등의 정보가 포함\n\n예를 들어, WGS 84 좌표계의 proj4string은 다음과 같이 표시됨\nproj4string을 변경하면 공간 데이터를 다른 좌표계로 변환 가능\n\n\nst_transform() 함수를 사용하여 다른 좌표계로 변환가능\n\n\n# PROJ4STRING definition\nst_sfc(point1, point2, crs = \"+proj=longlat +datum=WGS84 +no_defs\")\n\n\n\nsf class\n\n위의 위치데이터에 속성데이터(이름, 특정 값, 그룹 등)를 추가\n아래 예시는 2017년 6월 21일 런던의 25°C 온도를 나타냄\na geometry (the coordinates), and three attributes with three different classes (place name, temperature and date)\nsimple feature geometry column (sfc )에 속성(data.frame)을 나타내는 sf(simple features)의 calss를 합침\nst_sf() 를 이용하여 sfc와 class sf의 객체들을 하나로 통합할 수 있음\n\n\nlnd_point = st_point(c(0.1, 51.5))                 # sfg object\nlnd_geom = st_sfc(lnd_point, crs = 4326)           # sfc object\nlnd_attrib = data.frame(                           # data.frame object\n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n)\nlnd_sf = st_sf(lnd_attrib, geometry = lnd_geom)    # sf object\n\n\nsfg (simple feature geometry) 를 만듬\nCRS(좌표계시스템)를 가지는 sfc (simple feature geometry column)으로 전환\nst_sf() 를 이용하여data.frame 에 저장된 속성 정보와 sfc 를 통합\nsf object 완성\n\n\n> lnd_sf\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 0.1 ymin: 51.5 xmax: 0.1 ymax: 51.5\nGeodetic CRS:  WGS 84\n    name temperature       date         geometry\n1 London          25 2017-06-21 POINT (0.1 51.5)\n\n> class(lnd_sf)\n#> [1] \"sf\"         \"data.frame\""
  },
  {
    "objectID": "geo/2/ch2.html#raster-data",
    "href": "geo/2/ch2.html#raster-data",
    "title": "Chapter 2 Geograpic data in R",
    "section": "2.3 Raster data",
    "text": "2.3 Raster data\n\n지리적 레스터 데이터 모델은 래스터 헤더와 일반적으로 동일한 간격의 셀(픽셀)을 나타내는 matrix로 구성됨\n\nRaster header: 좌표 참조 시스템(CRS, Coordinate Reference System), 시작점(the origin)과 범위 (the extent)를 정의함\n\n더는 열 수, 행 수 및 셀 크기 해상도를 통해 범위를 정의\n셀의 ID를 사용하여 각 단일 셀에 쉽게 접근하고 수정\n\n행렬(matrix): 동일한 크기의 픽셀 또는 셀(pixel, or cell)을 표현. 픽셀 ID(pixel IDs)와 픽셀 값(pixel values)\n\n원점(또는 시작점)은 종종 행렬의 왼쪽 아래 모서리 좌표(R의 래스터 패키지는 기본적으로 왼쪽 위 모서리를 사용)\n래스터 레이어의 셀에는 단일 값(숫자 또는 범주)만 포함\n\n\n\n\n레스터 데이터 유형: (A)셀 ID, (B)셀 값, (C)컬러 래스터 맵\n\n\n\n\n\n연속 및 범주 래스터의 예"
  },
  {
    "objectID": "geo/2/ch2.html#an-introduction-to-raster",
    "href": "geo/2/ch2.html#an-introduction-to-raster",
    "title": "Chapter 2 Geograpic data in R",
    "section": "An introduction to raster",
    "text": "An introduction to raster\n\nraster package는 R에서 raster objects을 만들고, 읽고, 내보내고, 조작 및 처리하기 위한 광범위한 기능을 제공\n래스터 개념을 설명하기 위해 spDataLarge 의 데이터 세트를 사용\nZion National Park(미국 유타) 지역을 덮는 몇 개의 래스터 개체와 하나의 벡터 개체로 구성\nsrtm.tif은 이 지역의 디지털 표고 모델\n\n\ninstall.packages(\"rgdal\") \nlibrary(rgdal)\n\ninstall.packages(\"spDataLarge\", repos = \"https://nowosad.github.io/drat/\", type = \"source\") #지리공간 데이터 샘플을 내장\ninstall.packages(\"raster\")\n\nlibrary(spDataLarge)\nlibrary(raster)\n\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\") \nnew_raster = raster(raster_filepath)\n# rgdal 설치 error발생시 rgdal 설치및로드 필요\n\n\nraster header (extent, dimensions, resolution, CRS) and some additional information (class, data source name, summary of the raster values)\n\n클래스(class) 차원(dimentions), 해상도(resolution), 범위(extent), 좌표 참조 시스템 (Coordinates Reference System), 출처(Source), 이름(names), 최소/최대 값(min, max values) 속성 정보\n\n\n\n> new_raster\nclass      : RasterLayer \ndimensions : 457, 465, 212505  (nrow, ncol, ncell)\nresolution : 0.0008333333, 0.0008333333  (x, y)\nextent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : srtm.tif \nnames      : srtm \nvalues     : 1024, 2892  (min, max)\n\n\nFunctions\n\ndim(): 행, 열 및 레이어의 수를 반환\nncell(): 셀 수(픽셀)를 반환\nres():  래스터의 공간 해상도\nextent():  공간적 범위\ncrs(): 좌표 참조 시스템\ninMemory():  레스터 데이터가 메모리(기본값) 또는 디스크에 저장되는지 여부를 보고\n\n\n> dim(new_raster)\n[1] 457 465   1\n> ncell(new_raster)\n[1] 212505\n> extent(new_raster)\nclass      : Extent \nxmin       : -113.2396 \nxmax       : -112.8521 \nymin       : 37.13208 \nymax       : 37.51292 \n> crs(new_raster)\nCoordinate Reference System:\nDeprecated Proj.4 representation: +proj=longlat +datum=WGS84 +no_defs \nWKT2 2019 representation:\nGEOGCRS[\"WGS 84 (with axis order normalized for visualization)\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]] \n> inMemory(new_raster)\n[1] FALSE\n\n> help(\"raster-package\")"
  },
  {
    "objectID": "geo/2/ch2.html#basic-map-making",
    "href": "geo/2/ch2.html#basic-map-making",
    "title": "Chapter 2 Geograpic data in R",
    "section": "Basic map making",
    "text": "Basic map making\n\nsf package와 같이 raster 역시 plot()함수 사용 가능\n\n\nplot(new_raster)"
  },
  {
    "objectID": "geo/2/ch2.html#raster-classes",
    "href": "geo/2/ch2.html#raster-classes",
    "title": "Chapter 2 Geograpic data in R",
    "section": "Raster classes",
    "text": "Raster classes\n3가지의 레스터 클래스(Raster Classes)의 특장점 (1) RasterLayer Class (2) RasterBrick Class (3) RasterStack Class\n\n1. RasterLayer class\n\nRasterLayer class는 래스터 객체 중에서 가장 간단한 형태의 클래스이며, 한개의 층으로 구성되어 있음\nRasterLayer Class 객체를 만드는 가장 쉬운 방법은 기존의 RasterLayer Class 객체 파일을 읽어오는 것\n\n아래 예에서는 raster 패키지의 raster() 함수를 사용해서 spDataLarge 패키지에 내장되어 있는 srtm.tif 레스터 층 클래스 객체를 읽어와서 raster_layer 라는 이름의 단 한개의 층만을 가진 RasterLayer Class 객체를 만듬\nnlayers() 함수로 층의 개수를 살펴보면 ’1’개 인 것을 확인할 수 있습니다.\n\n\n\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nnew_raster = raster(raster_filepath)\n\n## number of layers \nnlayers(new_raster)\n\n\nRasterLayer 클래스 객체를 raster() 함수를 사용해서 처음부터 직접 만들 수도 있음\n\n8개의 행과 8개의 열, 총 64개의 셀(픽셀)을 가진 RasterLayer 클래스를 직접 만들기\n레스터 객체의 좌표 참조 시스템(CRS, Coordinates Reference System)은 WGS84 가 기본 설정값(해상도(resolution)의 단위가 도 (in degrees))\nres = 0.5 로서 해상도를 0.5도로 설정\n각 셀의 값은 왼쪽 상단부터 시작하여, 행 방향(row-wise)으로 왼쪽에서 오른쪽으로 채워짐\n\n\n\nmy_raster = raster(nrows = 8, ncols = 8, res = 0.5, xmn = -2.0, xmx = 2.0, ymn = -2.0, ymx = 2.0, vals = 1:64)\nmy_raster\n## plotting \nplot(my_raster, main = \"my raster (64 cells = 8 rows * 8 cols)\")           \n\n\n\n\n2. RasterBrick class\n\nRasterBrick and RasterStack 클래스는 여러 개의 층(multiple layers)을 가질 수 있음\n특히, RasterBrick클래스는 단일 스펙드럼 위성 파일(a single multispectral satellite file) 이나 또는 메모리의 단일 다층 객체(a single multilayer object in memory)의 형태로 다층의 레스터 객체를 구성\n아래의 예는 raster 패키지의 brick()함수를 사용해서 spDataLarge 패키지에 들어있는 landsat.tif 의 다층 레스터 파일을 RasterBrick클래스 객체로 불러온 것\n\nnlayers() : the number of layers stored in a Raster*  object\n\n\nmulti_raster_file = system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nr_brick = brick(multi_raster_file)\n\nr_brick\nnlayers(r_brick)\n\nplot(r_brick) #plotting RasterBrick object with 4 layers\n\n\n> r_brick\nclass      : RasterBrick \ndimensions : 1428, 1128, 1610784, 4  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=12 +datum=WGS84 +units=m +no_defs \nsource     : landsat.tif \nnames      : landsat.1, landsat.2, landsat.3, landsat.4 \nmin values :      7550,      6404,      5678,      5252 \nmax values :     19071,     22051,     25780,     31961\n\n> nlayers(r_brick)\n[1] 4\n\n\n\n\n\n3. RasterStack class\n\n다 층 (multi-layers) 레스터 객체로 구성\n같은 범위와 해상도를 가진 여러개의 RasterLayer 클래스 객체들을 리스트로 묶어서 RasterStack 클래스 객체를 만듬\nRasterBrick클래스가 동일한 복수개의 RasterLayer층으로 구성되는 반면에, RasterStack클래스는 여러개의 RasterLayer와RasterBrick 클래스 객체가 혼합되어서 구성할 수 있음\n연산 속도면에서 보면 일반적으로 RasterBrick 클래스가 RasterStack 클래스보다 빠름\n아래 예시\n\n\nraster(raster_brick, layer = 1) 함수를 사용해서 위에서 불러왔던 RasterBrick 클래스 객체의 1번째 층만 가져다가 raster_on_disk 라는 이름으로 레스터 객체를 하나 만듬\n\n\nraster() 함수로 동일한 범위와 해상도, 좌표 참조 시스템(CRS)를 가지고 난수로 셀의 값을 채운 raster_in_memory 라는 이름의 메모리에 있는 RasterLayer 클래스 객체를 만듬\n\n\nseq_len(n) : 1부터 n까지 입력(1씩 커짐)\n\n다음에 stac() 함수로 raster_stack = stack(raster_in_memory, raster_on_disk) 처럼 (a) + (b) 하여 쌓아서 raster_stack 라는 이름의 RasterStack 클래스 객체를 만듬\n마지막으로 plot() 함수로 RasterStack 클래스 객체에 쌓여 있는 2개의 객체를 시각화 (raster_in_memory 는 난수를 발생시켜 셀 값을 채웠기 때문에 시각화했을 때 아무런 패턴이 없음)\n\n\nraster_on_disk = raster(r_brick, layer = 1)\nraster_in_memory = raster(xmn = 301905, xmx = 335745,\n                          ymn = 4111245, ymx = 4154085, \n                          res = 30)\nvalues(raster_in_memory) = sample(seq_len(ncell(raster_in_memory)))\ncrs(raster_in_memory) = crs(raster_on_disk) #같은 좌표 입력\n\nr_stack = stack(raster_in_memory, raster_on_disk)\nr_stack\n\nplot(r_stack)\n\n\n\n\n언제 어떤 래스터 클래스를 사용하는 것이 좋은가?\n\n하나의 다층 레스터 파일이나 객체(a single multilayer file or object)를 처리하는 것이라면 RasterBrick 이 적합\n반면에, 여러개의 래스터 파일들(many files)이나 여러 종류의 레스터 클래스를 한꺼번에 연결해서 연산하고 처리해야 하는 경우라면 RasterStack Class 가 적합"
  },
  {
    "objectID": "geo/2/ch2.html#coordinate-reference-systems",
    "href": "geo/2/ch2.html#coordinate-reference-systems",
    "title": "Chapter 2 Geograpic data in R",
    "section": "2.4 Coordinate Reference Systems",
    "text": "2.4 Coordinate Reference Systems"
  },
  {
    "objectID": "geo/2/ch2.html#crscoordinate-reference-systems",
    "href": "geo/2/ch2.html#crscoordinate-reference-systems",
    "title": "Chapter 2 Geograpic data in R",
    "section": "CRS(Coordinate Reference Systems)",
    "text": "CRS(Coordinate Reference Systems)\n\n지리 공간 데이터 분석에서 가장 기본이 되고 또 처음에 확인을 해보아야 하는 좌표계, 좌표 참조 시스템(CRS, Coordinate Reference Systems)에 대한 소개\n\n지리 좌표계 (Geographic Coordinate Reference Systems)\n투영(투사) 좌표계 (Projected Coordinate Reference Systems)"
  },
  {
    "objectID": "geo/2/ch2.html#crs-in-r",
    "href": "geo/2/ch2.html#crs-in-r",
    "title": "Chapter 2 Geograpic data in R",
    "section": "CRS in R",
    "text": "CRS in R\n\nR에서 좌표계를 표현할 때는 (a) epsg 코드 (epsg code)나 또는 (b)proj4string 정의 (proj4string definition)를 사용\nR에서 CRS를 설명하는 두 가지 주요 방법은(a) epsg코드 또는(b)proj4string정의\nepsg코드\n\n일반적으로 더 짧으므로 기억하기 쉬움\n또한 이 코드는 잘 정의된 좌표 참조 시스템을 하나만 참조\n\nproj4string정의\n\n투영 유형, 데이텀 및 타원체와 같은 다양한 매개변수를 지정할 때 더 많은 유연성을 얻을 수 있음\n다양한 투영을 지정하고 기존 투영을 수정할 수 있음 (이것은 또한 proj4string접근 방식을 더 복잡하게 만듬)\n\n벡터 데이터의 좌표계\n\n벡터 지리 데이터에 대해서는 sf 패키지의 ****st_crs()함수를 사용해서 좌표계를 확인\nspDataLarge 패키지에 들어있는 Zion 국립 공원의 경계를 다각형면(Polygon)으로 나타내는 zion.gpkg 벡터 데이터를 st_read() 함수로 불러와서, st_crs()함수로 좌표계를 조회\n\n\nlibrary(sf)\nvector_filepath = system.file(\"vector/zion.gpkg\", package = \"spDataLarge\")\nnew_vector = st_read(vector_filepath)\n## st_read() : read vector dataset in R sf package\n\nst_crs(new_vector) # get CRS\n\n\n좌표계가 비어있거나 잘못 입력되어 있는 경우 **st_set_crs(vector_object,EPSG code)** 구문으로 좌표계를 설정할수 있음\n**st_set_crs()** 함수는 좌표계를 변경하는 것이 투영 데이터를 변환하는 것은 아니며, 투영 데이터 변환을 하려면 st_transform() 함수를 이용\n\n\n## -- st_set_crs() : setting a CRS (coordinate reference system) \nnew_vector_2 = st_set_crs(new_vector, 4326) # set CRS with EPSG 4326 code \n# Warning message: \n# st_crs<- : replacing crs does not reproject data; use st_transform for that\n\n래스터 데이터에서 좌표계\n\n레스터 모델의 객체에 대해서는 raster 패키지의 projection() 함수를 사용해서 좌표계를 확인하거나 설정\n\n## -- raster::projection() : get or set CRS in raster* objects \nlibrary(raster) \nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\") \nnew_raster = raster(raster_filepath) \nprojection(new_raster) # get CRS in raster objects # [1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n레스터 데이터에 대해서 좌표계를 새로 설정할 때도 역시 projection()함수를 사용\n\nnew_raster3  <-  new_raster\nprojection(new_raster3) <-  \"+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 \n                        +units=m +no_defs\" # set CRS\n\n벡터 데이터의 경우 좌표계를 설정할 때 ‘EPSG 코드’나 ’Proj4string 정의’ 모두 사용 가능한 반면에, 레스터 데이터는 ’Proj4string 정의’만 사용\n중요한 것은 st_crs()및 projection()함수는 좌표의 값이나 지오메트리를 변경하지 않음\n\n\nnew_raster3 = new_raster\nprojection(new_raster3) = \"+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 \n                          +units=m +no_defs\" # set CRS"
  },
  {
    "objectID": "geo/2/ch2.html#unit단위",
    "href": "geo/2/ch2.html#unit단위",
    "title": "Chapter 2 Geograpic data in R",
    "section": "Unit(단위)",
    "text": "Unit(단위)\n\n좌표계 (CRS) 정보 안에 들어있는 공간의 단위 (Spatial Units)\n지도를 제작하거나 볼 때 측정 단위 (measurement units)가 미터(meters) 인지 혹은 피트(feets) 인지 명시적으로 표현하고 정확하게 확인할 필요\n벡터의 지리적 데이터나 레스터의 픽셀에서 측정되는 단위라는 맥락(context)를 알 수 있고, 실제 지표면과 지도 표현 간의 관계, 거리를 알 수 있고, 또 거리나 면적 등을 계산할 수 있음\n\n\n지리공간 벡터 데이터의 측정 단위(Units in Vector data)\n\n\nsf 객체의 지리공간 벡터 데이터는 단위에 대해서 native support 이여서, 다른 외부 모듈이나 확장 프로그램을 설치하지 않아도 sf 객체 내에 단위가 들어가 있음\n그래서 sf 객체 벡터 데이터에 대해서 연산을 하게 되면 units 패키지에 의해 정의된 “단위 속성”도 같이 반환해주어서 단위로 인한 혼란을 미연에 방지할 수 있음(대부분의 좌표계는 미터(meters)를 사용하지만, 일부는 피트(feets)를 사용하기 때문에 단위가 혼란스러울 수 있음. raster 패키지는 단위가 native support가 아님.)\n\nR의 spData 패키지에 들어있는 “world” 데이터셋을 활용하여 Luxembourgd와 대한민국의 벡터 데이터를 가져와서, st_area()함수로 면적을 계산\nsf 패키지의 st_area() 함수로 벡터 데이터의 면적으로 계산 하면, 결과값의 뒤에 [m^2] 이라고 해서 2차원 공간 상의 “제곱미터” 단위가 같이 반환\n\n\nlibrary(spData)\nnames(world)\nluxembourg = world[world$name_long == \"Luxembourg\", ]\nluxembourg\n\nsouth_korea = world[world$name_long == \"Republic of Korea\", ] \nsouth_korea\nplot(south_korea)\nplot(south_korea[1])\n\nst_area(luxembourg) \nst_area(south_korea)\n\n> st_area(luxembourg) \n2408817306 [m^2]\n> st_area(south_korea)\n99020196082 [m^2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n면적 단위가 [m^2] 이다보니 결과값의 자리수가 너무 길게 표현됨\n계산의 단위를 “제곱킬로미터[km^2]로 변경하려면 units 패키지의 set_units(st_object, units) 함수로 단위를 설정할 수 있음\n기존의 면적 단위인 ‘제곱미터(m^2)’ 로 계산된 결과값을 1,000,000 으로 나누게 되면 결과값은 맞더라도 단위가 ‘제곱미터(m^2)’ 로 그대로여서, 우리가 원하던 단위인 ‘제곱킬로미터(km^2)’ 가 아니게 되므로 주의가 필요\n\n\n지리공간 래스터 데이터의 측정 단위(Units in Raster data)\n\n\n벡터 데이터를 다루는 sf 패키지는 단위가 native support 여서 조회나 계산 결과를 반환할 때 단위(units)를 속성으로 반환\n하지만 레스터 데이터를 다루는 raster 패키지는 단위에 대해서 native support 가 아니므로, 단위에 대해서 혼란스러울 수 있으므로 조심해야 함\n\nspDataLarge 패키지에 들어있는 strm.tif 파일을 raster() 함수로 읽어옴\n이 데이터는 st_crs() 함수로 좌표계를 확인해보면 “WGS84 투영”을 사용하므로, 십진수 각도 (decimal degrees as units) 를 단위로 사용\nres() 함수로 해상도를 확인해보면, 단지 숫자형 벡터 (numeric vector) 만 반환할 뿐, 단위에 대한 속성 정보는 없음 (no units attributes)\n\n\n\nㄹ## -- units in raster data library(raster) \nlibrary(spDataLarge) \nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\") \nnew_raster = raster(raster_filepath)\n## -- getting CRS \nst_crs(new_raster) \nplot(new_raster)\n\nres(new_raster)\n\n\nUTM 투영을 사용한다면, 이에 따라서 단위가 바뀌지만, res()로 해상도를 살펴보면 역시 단지 숫자형 벡터만 반환할 뿐, 단위 속성 정보는 없음\n\n\n## -- if we used the UTM projection, the units would change. \nrepr = projectRaster(new_raster, crs = \"+init=epsg:26912\") \n## -- no units attributes, just only returns numeric vector \nres(repr) # [1] 73.8 92.5"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Hello. Welcome to my blog."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jungwoo Lee",
    "section": "",
    "text": "Hannam University | Deajeon, Kor"
  },
  {
    "objectID": "about.html#introduction",
    "href": "about.html#introduction",
    "title": "Jungwoo Lee",
    "section": "Introduction",
    "text": "Introduction\nHello. My name is Jungwoo Lee."
  },
  {
    "objectID": "data_vis.html",
    "href": "data_vis.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nplotnine\n\n\n\n\n\n\n\ncode\n\n\nvisualation\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\ngeom_line & heatmap\n\n\n(ncdc_normal)\n\n\n\n\ncode\n\n\nvisualation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\ngeom_line & heatmap\n\n\n(기상청자료)\n\n\n\n\ncode\n\n\nvisualation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nCoordinate systems\n\n\n(ncdc_normal)\n\n\n\n\ncode\n\n\nvisualation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nCoordinate systems\n\n\n(기상청 자료)\n\n\n\n\ncode\n\n\nvisualation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nColor scales\n\n\n\n\n\n\n\ncode\n\n\nvisualation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nColor scales\n\n\n(2023년 3월 주민등록인구 및 세대현황)\n\n\n\n\ncode\n\n\nvisualation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "geo.html",
    "href": "geo.html",
    "title": "Geocomputation",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nChapter 4 Spatial data operations\n\n\n\n\n\n\n\ncode\n\n\nvisualation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nChapter 3 Attribute data operations\n\n\n\n\n\n\n\ncode\n\n\nvisualation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nChapter 2 Geograpic data in R\n\n\n\n\n\n\n\ncode\n\n\nvisualation\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data_vis/5-1/pop_kor.html",
    "href": "data_vis/5-1/pop_kor.html",
    "title": "Color scales",
    "section": "",
    "text": "데이터 시각화 5주차 과제\n\n\n\n\n\n# 패키지 로드\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(colorspace)\nlibrary(geojsonsf)\nlibrary(sf)\nlibrary(stringr)\nlibrary(extrafont) # mac os 한글 깨지는 처리해주는 패키지\n\n\n# 데이터 로드\nkor_202303 <- read.csv('/Users/jungwoolee/Desktop/college/Data Visualization/data/202303_202303_주민등록인구및세대현황_월간.csv',  fileEncoding = \"euc-kr\")\nkor_sido <- geojson_sf('/Users/jungwoolee/Desktop/college/Data Visualization/data/KOR_SIDO.json')\n\n\n# 데이터 전처리\nkor_202303$행정구역 <- str_sub(kor_202303$행정구역, -30, -15) # 행정구역에서 글자만 추출\nkor = str_replace_all(kor_202303[,2],\",\", \"\") # 쉼표(,) 제거\nkor_202303 <- kor_202303 %>% mutate(총인구수 = kor) # 쉼표 제거 해준 새로운 총인구수 마지막 열에 추가\nkor_202303[, \"총인구수\"] = as.numeric(kor_202303$총인구수) # 총인구수 숫자형 벡터로 변경\nkor_202303 %>% sapply(class)\nkor_202303 <- kor_202303[, -2] # 기존 X2023년03월_총인구수 열 삭제\n\nuse_map <- kor_sido\nuse_map$CTPRVN_CD <- paste(use_map$CTPRVN_CD, \"00000000\",sep = \"\")\nnames(use_map) <- c(\"행정구역_코드\", \"행정구역\", \"geometry\",  \"CTP_ENG_NM\") # 열 순서 바꿔주기\nuse_map[, \"행정구역_코드\"] = as.numeric(use_map$행정구역_코드) # 행정구역코드 숫자형 벡터로 변경\nuse_map <- use_map %>% merge(kor_202303,by=\"행정구역\", all.x = T) # use_map과 kor_202303 병합\n\n\n# ggplot 그리기\nuse_map %>% ggplot(aes(fill = 총인구수))+\n  geom_sf(color = \"gray90\")+\n  coord_sf(datum = NA)+\n  scale_fill_distiller(\n    name = \"인구수\",\n    palette = \"Blues\", type = 'seq', na.value = 'grey60',\n    direction = 1,\n    breaks = seq(0, 400, 30) * 1e+5,\n    labels = format(seq(0, 400, 30) * 1e+5, big.mark = \",\", scientific=FALSE), )+\n  theme_minimal() +\n  theme_light(base_family = \"AppleSDGothicNeo-Regular\") + \n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 1,\n    legend.position = c(.8,.2))"
  },
  {
    "objectID": "data_vis/3-1/weather.html",
    "href": "data_vis/3-1/weather.html",
    "title": "geom_line & heatmap",
    "section": "",
    "text": "데이터 시각화 3주차 과제\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n\n기상청 자료 2021년\n\n# 기상청 자료 2021년\ndata_2021 <- read.csv(\"/Users/jungwoolee/Desktop/college/Data Visualization/data/OBS_ASOS_DD_20220308125952.csv\", fileEncoding =  \"CP949\")\ndata_2021\n\n# 자료형 확인\ndata_2021 %>% sapply(class)\n\n# 기초통계량\ndata_2021 %>% summary()\n\n# Data type conversion: character > Date\ndata_2021$일시 <- data_2021$일시 %>% as.Date(\"%Y-%m-%d\")\ndata_2021 %>% sapply(class)\n\n\n# x축에 표시할 눈금\ndate_s <- \"2021-01-01\" %>% as.Date(\"%Y-%m-%d\") \ndate_e <- \"2022-01-01\" %>% as.Date(\"%Y-%m-%d\") \nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\")\nbreak_date\n\n\n# ggplot + 축 설정\ndata_2021 %>% names()\nggplot(data_2021, aes(x = 일시, y = 평균기온..C., color = 지점명)) + \n  geom_line(linewidth = 0.5) + \n  scale_x_date(name = \"월\", \n               breaks = break_date,\n               labels = c(\"1월\", \"4월\", \"7월\", \"10월\", \"1월\")) +\n  scale_y_continuous(name = \"평균기온\") + \n  theme_light(base_family = \"AppleSDGothicNeo-Regular\") \n# 강조는 \"AppleSDGothicNeo-ExtraBold\"\n\n# geom_smooth()\nggplot(data_2021, aes(x = 일시, y = 평균기온..C., color = 지점명)) + \n  geom_smooth(linewidth = 0.5, se = F, span = 0.2) + \n  scale_x_date(name = \"월\", \n               breaks = break_date,\n               labels = c(\"1월\", \"4월\", \"7월\", \"10월\", \"1월\")) +\n  scale_y_continuous(name = \"평균기온\") + \n  theme_light(base_family = \"AppleSDGothicNeo-Regular\") \n# se = confidence interval 구간 나타내는 함수\n# span = smooth lined을 더 부드럽게 나타내는 함수/ 숫자가 클 수록 더 부드러워 짐\n\n\n\n\n\n\n\n\n\n\n\n\n## heat map\ndata_2021_month <- data_2021 %>% \n  mutate(month = format(일시, \"%m\")) %>% \n  group_by(지점명, month) %>% \n  summarize(mean = mean(평균기온..C.))\n  # mutate(month = factor(month, levels = paste(1:12, \"월\", sep = \"\")))\n\nggplot(data_2021_month,aes(x = month, y = 지점명, fill = mean)) +\n  geom_tile(width = 0.95, height = 0.95) +\n  coord_fixed(expand = FALSE) + \n  scale_fill_viridis_c(option = \"B\", begin = 0.15, end = 0.98, name = \"temperature\") + \n  theme_light(base_family = \"AppleSDGothicNeo-Regular\") +\n  ylab(NULL) \n# ggtitle(\"2021년 기상청 자료\") +\n# theme(plot.title = element_text(hjust = 0.5, size = 30, color = \"blue\"))\n\n\n\n\n기상청 자료 2022년\n\n# 기상청 자료 2022년\ndata_2022 <- read.csv(\"/Users/jungwoolee/Desktop/college/Data Visualization/data/OBS_ASOS_DD_20230322080932.csv\", fileEncoding =  \"CP949\")\nhead(data_2022)\n\n# Data type conversion: character > Date\ndata_2022$일시 <- data_2022$일시 %>% as.Date(\"%Y-%m-%d\")\n# 자료형 확인\ndata_2022 %>% sapply(class)\n\n\n# x축에 표시할 눈금\ndate_s <- \"2022-01-01\" %>% as.Date(\"%Y-%m-%d\") \ndate_e <- \"2023-01-01\" %>% as.Date(\"%Y-%m-%d\") \nbreak_date <- seq.Date(date_s, date_e, by = \"2 month\")\nbreak_date\n\n#지점명확인\ndata_2022$지점명 %>%  table()\ndata_2022_use <- data_2022 %>% filter(지점명 %in%c(\"대전\", \"서울\", \"세종\", \"제주\"))\n\n\nggplot(data_2022_use, aes(x = 일시, y = 평균기온..C., color = 지점명)) + \n  geom_line(linewidth = 0.5) + \n  scale_x_date(name = \"월\", \n               breaks = break_date,\n               labels = c(\"1월\", \"3월\", \"5월\", \"7월\", \"9월\", \"11월\", \"1월\")) +\n  scale_y_continuous(name = \"평균기온\") + \n  theme_light(base_family = \"AppleSDGothicNeo-Regular\") \n\nggplot(data_2022_use, aes(x = 일시, y = 평균기온..C., color = 지점명)) + \n  geom_smooth(linewidth = 0.5, se = F, span = 0.2) + \n  scale_x_date(name = \"월\", \n               breaks = break_date,\n               labels = c(\"1월\", \"3월\", \"5월\", \"7월\", \"9월\", \"11월\", \"1월\")) +\n  scale_y_continuous(name = \"평균기온\") + \n  theme_light(base_family = \"AppleSDGothicNeo-Regular\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_2022_month <- data_2022_use %>% \n  mutate(month = format(일시, \"%m\")) %>% \n  group_by(지점명, month) %>% \n  summarize(mean = mean(평균기온..C.))\n# mutate(month = factor(month, levels = paste(1:12, \"월\", sep = \"\")))\n\n\n# heatmap\nggplot(data_2022_month,aes(x = month, y = 지점명, fill = mean)) +\n  geom_tile(width = 0.95, height = 0.95) +\n  coord_fixed(expand = FALSE) + \n  scale_fill_viridis_c(option = \"B\",begin = 0.15,end = 0.98, name = \"temperature\") + \n  theme_light(base_family = \"AppleSDGothicNeo-Regular\") +\n  ylab(NULL)\n# ggtitle(\"2022년 기상청 자료\") + \n# theme(plot.title = element_text(hjust = 0.5, size = 30, color = \"blue\"))"
  },
  {
    "objectID": "data_vis/4/coordinate.html",
    "href": "data_vis/4/coordinate.html",
    "title": "Coordinate systems",
    "section": "",
    "text": "데이터 시각화 4주차 실습\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n\nncdc_normals <- read.csv(\"/Users/jungwoolee/Desktop/college/Data Visualization/data/ncdc_normals.csv\")\nstation_loc <- data.frame(station_id = c(\"USW00014819\",\"USC00042319\",\"USW00093107\",\"USW00012918\"),\n                          location = c(\"Chicago\",\"Death Valley\",\"San Diego\",\"Houston\"))\ntemps_long <- ncdc_normals %>% inner_join(station_loc, by=\"station_id\")  #station_id기준으로 합쳐라\ntemps_long$date <- temps_long$date %>% as.Date(\"%Y-%m-%d\") # class를 Date로 바꿔줌\n\n\n# x축에 표시할 눈금\ndate_s <- \"0000-01-01\" %>% as.Date(\"%Y-%m-%d\") \ndate_e <- \"0001-01-01\" %>% as.Date(\"%Y-%m-%d\") \nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\")\n\n\nlibrary(cowplot)\n\n# Houston 필터\ndata_Houston <- temps_long %>% filter(location == \"Houston\")\ndata_Houston %>% dim\ndata_Houston %>% head()\n\n\n# x축에 표시할 눈금\ndate_s <- \"0000-01-01\" %>% as.Date(\"%Y-%m-%d\") \ndate_e <- \"0001-01-01\" %>% as.Date(\"%Y-%m-%d\") \nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\")\n\n\n# ggplot 그리기 \ntemp_plot <- ggplot(data_Houston, aes(x = date, y = temperature)) +\n  geom_line(linewidth = 1, color = \"royalblue\") + \n  scale_x_date(name = \"month\", breaks = break_date,\n               labels = c(\"Jan\", \"Apr\", \"Jul\", \"Oct\", \"Jan\")) +\n  scale_y_continuous(name = \"temp\") +\n  theme_light()\n\n\n# 여러 플롯 정렬\nplot_ab <- plot_grid(temp_plot,\n                     temp_plot,\n                     nrow = 1,\n                     rel_widths = c(1,2),\n                     labels = c(\"a\",\"b\"))\nplot_ab\n\nplot_abc <- plot_grid(plot_ab,\n                      temp_plot,\n                      ncol = 1,\n                      rel_heights = c(1.5,1),\n                      labels = c(\"\",\"c\"))\nplot_abc\n\n\n\ngeom_hline()\n\nlibrary(ggrepel)\n\nUS_census <- read.csv(\"/Users/jungwoolee/Desktop/college/Data Visualization/data/US_census.csv\")\ntx_counties <- US_census %>% \n  filter(state == \"Texas\") %>% \n  select(name, pop2010) %>% \n  mutate(county = gsub(\"County\", \"\", name),\n         popratio = pop2010/median(pop2010)) %>% # 중위수\n  arrange(pop2010 %>% desc()) %>% \n  mutate(index = 1:n(),\n         label = ifelse(index <= 3 | index > n()-3 | runif(n(5)) < 0.04, county, \"\")) # 앞, 뒤에서 3등까지, 랜덤으로 표시\n\n\nggplot(tx_counties, aes(x = index, y = popratio)) +\n  geom_hline(yintercept = 0, linetype = 2, color = \"grey40\") +\n  geom_point(size = 1, color = \"royalblue\") +\n  geom_text_repel(aes(label = label),\n                  min.segment.length = 0,\n                  max.overlaps = 100) +\n  theme_light() +\n  theme(panel.border = element_blank())\n\n\n\nlabel_log10 <- sapply(-2:2, function(i) as.expression(bquote(10^ .(i))))\n\nggplot(tx_counties, aes(x = index, y = popratio)) +\n  geom_hline(yintercept = 0, linetype = 2, color = \"grey40\") +\n  geom_point(size = 1, color = \"royalblue\") +\n  geom_text_repel(aes(label = label),\n                  min.segment.length = 0,\n                  max.overlaps = 100) +\n  scale_y_log10(name = \"population number / median\", \n                breaks = 10^(-2:2),\n                labels = label_log10) +\n  scale_x_continuous(name = \"Texas counties, from most to least populous\",\n                     breaks = NULL) + # 눈금\n  theme_light() +\n  theme(panel.border = element_blank())\n\n\n\n\ncoord_polar()\n\n# figure 3.10\ndate_s <- \"0000-01-01\" %>% as.Date(\"%Y-%m-%d\")\ndate_e <- \"0001-01-01\" %>% as.Date(\"%Y-%m-%d\")\nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\")\ndate_lab = format(break_date, \"%B\")\n\n\nggplot(temps_long, aes(x = date, y = temperature, color = location)) +\n  geom_line(linewidth = 1) +\n  scale_x_date(name = \"month\",\n               breaks = break_date,\n               labels = date_lab) +\n  scale_y_continuous(name = \"temperature\",\n                     limits = c(0, 105)) +\n  coord_polar(theta = \"x\", start = pi, direction = -1) + # 6시 위치에서 반시계 방향\n  # coord_polar(theta = \"x\", start = 0, direction = 1) + # 12시 위치에서 시계 방향\n  theme_light() +\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "data_vis/3/geom_line.html",
    "href": "data_vis/3/geom_line.html",
    "title": "geom_line & heatmap",
    "section": "",
    "text": "데이터 시각화 3주차 실습\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n\ngeom_line()\n\n#데이터 불러오기 1)\n#data_file <- choose.files()  #경로지정해서 데이터 불러오기\n#ncdc_normals <- read.csv(data_file)\n\n#데이터 불러오기 2)\nncdc_normals <- read.csv(\"/Users/jungwoolee/Desktop/college/Data Visualization/ncdc_normals.csv\")\nncdc_normals\n\n#데이터 확인\nncdc_normals %>% dim()\nncdc_normals %>% head()\nncdc_normals %>% summary()\nncdc_normals %>% sapply(class)  #class로 합쳐 문자형인지 숫자형인지 확인\n\nncdc_normals$date <- ncdc_normals$date %>% as.Date(\"%Y-%m-%d\") #날짜가 문자형이기 때문에 날짜형식으로 바꿈\nncdc_normals %>% sapply(class) #문자,숫자형 다시 확인 #data가 Date로 바뀜\n\n\n#station_id 갯수\nncdc_normals$station_id %>% unique() %>% length()\n\n#station_id,location을 데이터 프레임으로 만들기\nstation_loc <- data.frame(station_id = c(\"USW00014819\",\"USC00042319\",\"USW00093107\",\"USW00012918\"),\n                          location = c(\"Chicago\",\"Death Valley\",\"San Diego\",\"Houston\"))\nstation_loc\n\n#기존 데이터와 데이터 프레임 합치기\ntemps_long <- ncdc_normals %>% inner_join(station_loc, by=\"station_id\")  #station_id기준으로 합쳐라\ntemps_long %>% head()\n\n\n# x축에 표시할 눈금\ndate_s <- \"0000-01-01\" %>% as.Date(\"%Y-%m-%d\") \ndate_e <- \"0001-01-01\" %>% as.Date(\"%Y-%m-%d\") \nbreak_date <- seq.Date(date_s, date_e, by = \"3 month\")\n\n\n#결과 데이터로 ggplot 그리기 \n#aes : 어떤 데이터로 그릴거니? \n#geom_line : 어떤 라인? #theme_light : 그래프 뒷배경 밝게\nggplot(temps_long,aes(x=date,y=temperature,color=location)) + \n  geom_line() + \n  scale_x_date(name = \"month\", breaks = break_date,\n               labels = c(\"Jan\", \"Apr\", \"Jul\", \"Oct\", \"Jan\")) +\n  scale_y_continuous(name = \"temp\", \n                     #limits = c(0,100)\n                     ) + \n  ylab(\"Temp\") +\n  labs(title = \"Fig. 2.2\", subtitle = \"Daily temperature normals\") +\n  theme_light() \n\n\n\n\nheat map\n\n# 월 평균\nmean_temps <- temps_long %>% \n  group_by(location, month) %>% \n  summarise(mean = mean(temperature)) %>% \n  ungroup() %>% \n  mutate(month = factor(month %>% paste(), \n         levels = 1:12 %>% paste()))\nmean_temps\n# paste = 글자로 바꿔줌\n\n\n# ggplot + geom_tile + fill color\nggplot(mean_temps, aes(x=month, location, fill = mean)) +\n  geom_tile(width = .95, height = .95) + \n  coord_fixed(expand = F) + \n  scale_fill_viridis_c(option = \"B\", begin = 0.15, end = 0.98) +\n  ylab(\"\")\n# ylab(NULL) = 제목 자체를 빼서 그래프가 확장됨"
  },
  {
    "objectID": "data_vis/4-1/weather_2.html",
    "href": "data_vis/4-1/weather_2.html",
    "title": "Coordinate systems",
    "section": "",
    "text": "데이터 시각화 4주차 과제\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggrepel)\nlibrary(extrafont)\n\n\n# 시군구 인구수 2023년\ndata_202302 <- read.csv(\"/Users/jungwoolee/Desktop/college/Data Visualization/data/행정구역.csv\", encoding = \"UTF-8\")\ndata_202302\n\n\nkor_census <- data_202302 %>% filter(X2023.02>0) %>% \n  mutate(popratio = X2023.02 / median(X2023.02)) %>% \n  arrange(popratio %>% desc()) %>% \n  mutate(index = 1:n(),\n         labels = ifelse(index <= 5 | index > n()-5 | index == median(index), 행정구역.시군구.별,\"\"))\n\nlabel_log10 <- sapply(-2:2, function(i) as.expression(bquote(10^ .(i))))\n\n\n# ggplot 그리기\nggplot(kor_census, aes(x = index, y = popratio)) +\n  geom_hline(yintercept = 1, linetype = 2, color = \"grey40\") +\n  geom_point(size = 1, color = \"royalblue\") +\n  geom_text_repel(aes(label = labels), \n                  min.segment.length = 0,\n                  max.overlaps = 100,\n                  family = \"AppleGothic\") + \n  scale_y_log10(name = \"인구 수 / 중위 수\",\n                breaks = 10^(-2:2),\n                labels = label_log10,\n                limits = c(10^-1.3, 10^1.3)) + # 그래프를 그렸을 때 아래에 모여있을 때 파악하기 힘들기 때문에 로그변환을 해줌\n  scale_x_continuous(name = \"행정구역(시군구)별 주민등록세대수\",\n                     breaks = NULL) + # breaks = NULL이면 x축 안보임, breaks=seq(0,350, by=50)이면 50부터 50씩 커짐\n  theme_light(base_family = \"AppleGothic\") +\n  theme(panel.border = element_blank())\n\n\n\n# 기상청 자료 2022년\ndata_2022<-read.csv(\"/Users/jungwoolee/Desktop/college/Data Visualization/data/OBS_ASOS_DD_20230322080932.csv\", fileEncoding =  \"CP949\")\ndata_2022$일시 <- data_2022$일시 %>% as.Date(\"%Y-%m-%d\")\n\n\ndate_s <- \"2022-01-01\" %>% as.Date(\"%Y-%m-%d\") \ndate_e <- \"2023-01-01\" %>% as.Date(\"%Y-%m-%d\") \nbreak_date <- seq.Date(date_s, date_e, by = \"2 month\")\ndate_lab = format(break_date, \"%b\")\n\n# 지점명 뽑기\ndata_2022_use <- data_2022 %>% filter(지점명 %in%c(\"대전\",\"서울\",\"세종\",\"제주\"))\ndata_2022_use %>% sapply(class)\n\n\n# ggplot 그리기\nggplot(data_2022_use, aes(x = 일시, y = 평균기온..C., color = 지점명)) +\n  geom_line(linewidth = 0.5) +\n  scale_x_date(name = \"월\",\n               breaks = break_date,\n               labels = c(\"1월\", \"3월\", \"5월\", \"7월\", \"9월\", \"11월\", \"1월\")) +\n  scale_y_continuous(name = \"평군기온\",\n                     limits = c(-20, 30)) +\n  coord_polar(theta = \"x\", start = pi, direction = -1) +\n  theme_light(base_family = \"AppleGothic\") +\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "data_vis/2/2주차 실습 복사본.html",
    "href": "data_vis/2/2주차 실습 복사본.html",
    "title": "plotnine",
    "section": "",
    "text": "데이터시각화 2주차 실습\n\n\nhttps://plotnine.readthedocs.io/en/stable/installation.html\n\n\npip install plotnine\n\nCollecting plotnine\n  Downloading plotnine-0.10.1-py3-none-any.whl (1.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 519.4 kB/s eta 0:00:0000:0100:01\nRequirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/site-packages (from plotnine) (0.5.2)\nRequirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/site-packages (from plotnine) (0.13.2)\nRequirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.10/site-packages (from plotnine) (3.5.3)\nRequirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/site-packages (from plotnine) (1.5.1)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/site-packages (from plotnine) (1.23.5)\nRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/site-packages (from plotnine) (1.9.3)\nCollecting mizani>=0.8.1\n  Downloading mizani-0.8.1-py3-none-any.whl (64 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.7/64.7 kB 728.6 kB/s eta 0:00:00a 0:00:01\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3.5.0->plotnine) (9.3.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3.5.0->plotnine) (3.0.9)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3.5.0->plotnine) (4.37.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3.5.0->plotnine) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3.5.0->plotnine) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3.5.0->plotnine) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3.5.0->plotnine) (21.3)\nCollecting palettable\n  Downloading palettable-3.3.0-py2.py3-none-any.whl (111 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111.8/111.8 kB 617.9 kB/s eta 0:00:00a 0:00:01\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas>=1.3.5->plotnine) (2022.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from patsy>=0.5.1->plotnine) (1.16.0)\nInstalling collected packages: palettable, mizani, plotnine\nSuccessfully installed mizani-0.8.1 palettable-3.3.0 plotnine-0.10.1\n\n[notice] A new release of pip available: 22.3.1 -> 23.0.1\n[notice] To update, run: python3.10 -m pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n### Packages\nfrom plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap\nfrom plotnine.data import mtcars\n\n\n### Example\n(ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)'))\n+ geom_point()\n+ stat_smooth(method='lm'))\n\n\n\n\n<ggplot: (350876694)>\n\n\n\n### Example\n(ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)'))\n+ geom_point()\n+ stat_smooth(method='lm')\n+ facet_wrap('~gear'))\n\n\n\n\n<ggplot: (284664025)>\n\n\n\n### Packages\nimport pandas as pd\nimport numpy as np\nfrom plotnine import *\n\n\n### Data\nfrom plotnine.data import mpg\n\n\n### Data\ndata_raw = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/mpg.csv\")\n\n\nmanufacturer: 자동차 제조사\ndispl: 자동차 배기량\ncyl: 실린더 수\ndrv: 구동 방식\ncty: 도심 연비\nhwy: 고속도로 연비\nfl: 연료 종류\n\n\ndata_raw.shape\n\n(234, 12)\n\n\n\ndata_raw.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      manufacturer\n      model\n      displ\n      year\n      cyl\n      trans\n      drv\n      cty\n      hwy\n      fl\n      class\n    \n  \n  \n    \n      0\n      1\n      audi\n      a4\n      1.8\n      1999\n      4\n      auto(l5)\n      f\n      18\n      29\n      p\n      compact\n    \n    \n      1\n      2\n      audi\n      a4\n      1.8\n      1999\n      4\n      manual(m5)\n      f\n      21\n      29\n      p\n      compact\n    \n    \n      2\n      3\n      audi\n      a4\n      2.0\n      2008\n      4\n      manual(m6)\n      f\n      20\n      31\n      p\n      compact\n    \n    \n      3\n      4\n      audi\n      a4\n      2.0\n      2008\n      4\n      auto(av)\n      f\n      21\n      30\n      p\n      compact\n    \n    \n      4\n      5\n      audi\n      a4\n      2.8\n      1999\n      6\n      auto(l5)\n      f\n      16\n      26\n      p\n      compact\n    \n  \n\n\n\n\n\ndata_raw.describe(include = 'all')\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      manufacturer\n      model\n      displ\n      year\n      cyl\n      trans\n      drv\n      cty\n      hwy\n      fl\n      class\n    \n  \n  \n    \n      count\n      234.000000\n      234\n      234\n      234.000000\n      234.000000\n      234.000000\n      234\n      234\n      234.000000\n      234.000000\n      234\n      234\n    \n    \n      unique\n      NaN\n      15\n      38\n      NaN\n      NaN\n      NaN\n      10\n      3\n      NaN\n      NaN\n      5\n      7\n    \n    \n      top\n      NaN\n      dodge\n      caravan 2wd\n      NaN\n      NaN\n      NaN\n      auto(l4)\n      f\n      NaN\n      NaN\n      r\n      suv\n    \n    \n      freq\n      NaN\n      37\n      11\n      NaN\n      NaN\n      NaN\n      83\n      106\n      NaN\n      NaN\n      168\n      62\n    \n    \n      mean\n      117.500000\n      NaN\n      NaN\n      3.471795\n      2003.500000\n      5.888889\n      NaN\n      NaN\n      16.858974\n      23.440171\n      NaN\n      NaN\n    \n    \n      std\n      67.694165\n      NaN\n      NaN\n      1.291959\n      4.509646\n      1.611534\n      NaN\n      NaN\n      4.255946\n      5.954643\n      NaN\n      NaN\n    \n    \n      min\n      1.000000\n      NaN\n      NaN\n      1.600000\n      1999.000000\n      4.000000\n      NaN\n      NaN\n      9.000000\n      12.000000\n      NaN\n      NaN\n    \n    \n      25%\n      59.250000\n      NaN\n      NaN\n      2.400000\n      1999.000000\n      4.000000\n      NaN\n      NaN\n      14.000000\n      18.000000\n      NaN\n      NaN\n    \n    \n      50%\n      117.500000\n      NaN\n      NaN\n      3.300000\n      2003.500000\n      6.000000\n      NaN\n      NaN\n      17.000000\n      24.000000\n      NaN\n      NaN\n    \n    \n      75%\n      175.750000\n      NaN\n      NaN\n      4.600000\n      2008.000000\n      8.000000\n      NaN\n      NaN\n      19.000000\n      27.000000\n      NaN\n      NaN\n    \n    \n      max\n      234.000000\n      NaN\n      NaN\n      7.000000\n      2008.000000\n      8.000000\n      NaN\n      NaN\n      35.000000\n      44.000000\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n### Remove first column\ndata_use = data_raw.copy()\ndata_use = data_use.iloc[:,1:]\ndata_use\n\n\n\n\n\n  \n    \n      \n      manufacturer\n      model\n      displ\n      year\n      cyl\n      trans\n      drv\n      cty\n      hwy\n      fl\n      class\n    \n  \n  \n    \n      0\n      audi\n      a4\n      1.8\n      1999\n      4\n      auto(l5)\n      f\n      18\n      29\n      p\n      compact\n    \n    \n      1\n      audi\n      a4\n      1.8\n      1999\n      4\n      manual(m5)\n      f\n      21\n      29\n      p\n      compact\n    \n    \n      2\n      audi\n      a4\n      2.0\n      2008\n      4\n      manual(m6)\n      f\n      20\n      31\n      p\n      compact\n    \n    \n      3\n      audi\n      a4\n      2.0\n      2008\n      4\n      auto(av)\n      f\n      21\n      30\n      p\n      compact\n    \n    \n      4\n      audi\n      a4\n      2.8\n      1999\n      6\n      auto(l5)\n      f\n      16\n      26\n      p\n      compact\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      229\n      volkswagen\n      passat\n      2.0\n      2008\n      4\n      auto(s6)\n      f\n      19\n      28\n      p\n      midsize\n    \n    \n      230\n      volkswagen\n      passat\n      2.0\n      2008\n      4\n      manual(m6)\n      f\n      21\n      29\n      p\n      midsize\n    \n    \n      231\n      volkswagen\n      passat\n      2.8\n      1999\n      6\n      auto(l5)\n      f\n      16\n      26\n      p\n      midsize\n    \n    \n      232\n      volkswagen\n      passat\n      2.8\n      1999\n      6\n      manual(m5)\n      f\n      18\n      26\n      p\n      midsize\n    \n    \n      233\n      volkswagen\n      passat\n      3.6\n      2008\n      6\n      auto(s6)\n      f\n      17\n      26\n      p\n      midsize\n    \n  \n\n234 rows × 11 columns\n\n\n\n\n### ggplot: 시각화 자료 + 시각화 특성\nggplot(data_use, aes(x = 'displ', y = 'hwy'))\n\n\n\n\n<ggplot: (350925936)>\n\n\n\n### ggplot: 시각화 자료 + 시각화 특성 + 시각화 종류\nggplot(data_use, aes(x = 'displ', y = 'hwy')) + geom_point()\n\n\n\n\n<ggplot: (351070566)>\n\n\n\n### Smooth line & SE\nggplot(data_use, aes(x = 'displ', y = 'hwy')) + geom_smooth()\n\n/usr/local/lib/python3.10/site-packages/plotnine/stats/smoothers.py:321: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n<ggplot: (351040531)>\n\n\n\n### Point + Smooth line\nggplot(data_use, aes(x = 'displ', y = 'hwy')) + geom_smooth() + geom_point()\n\n/usr/local/lib/python3.10/site-packages/plotnine/stats/smoothers.py:321: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n<ggplot: (351028050)>"
  },
  {
    "objectID": "data_vis/5/color.html",
    "href": "data_vis/5/color.html",
    "title": "Color scales",
    "section": "",
    "text": "데이터 시각화 5주차 실습\n\n\n\n\n\n# 패키지 로드\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(colorspace)\n\n\n### ColorBrewer palettes\nRColorBrewer::display.brewer.all()\n\n\n\n# 데이터 로드\nUS_census <- read.csv(\"/Users/jungwoolee/Desktop/college/Data Visualization/data/US_census.csv\")\nUS_regions <- read.csv(\"/Users/jungwoolee/Desktop/college/Data Visualization/data/US_regions.csv\")\n\n\npopgrowth_df <- US_census %>% left_join(US_regions) %>% \n  group_by(region, division, state) %>% \n  summarise(pop2000 = sum(pop2000, na.rm = T),\n           pop2010 = sum(pop2010, na.rm = T),\n           popgrowth = (pop2010 - pop2000)/pop2000,\n           area = sum(area)) %>% \n    arrange(popgrowth) %>% \n    ungroup() %>% \n    mutate(state = factor(state, levels = state),\n           region = factor(region, levels = c(\"West\", \"South\", \"Midwest\", \"Northeast\")))\n\n\n# ggplot 그리기\nregion_colors <- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\")\nggplot(popgrowth_df, aes(x = state, y = 100*popgrowth, fill = region)) +\n  geom_col() + \n  scale_y_continuous(name = \"population growth, 2000 to 2010\",\n                     labels = scales::percent_format(scale = 1),\n                     expand = c(0,0)) +\n  scale_fill_manual(values = region_colors) +\n  coord_flip() +\n  theme_light() +\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size = 10),\n        legend.position = c(.58, .68),\n        legend.background = element_rect(fill = \"#ffffffb0\"))\n\n\n\n### 주민등록 인구 및 세대현황 데이터\nkor_202202 <- read.csv('/Users/jungwoolee/Desktop/college/Data Visualization/data/202202_주민등록인구및세대현황.csv')\nkor_202202 %>% head()          \nkor_202202 %>% sapply(class)\nkor_202202$행정구역_코드 <- kor_202202$행정구역_코드 %>% format()\n\n\nkor_202202_use <- kor_202202 %>% \n  filter(substr(행정구역,1,2)%in% c(\"서울\",\"대전\",\"대구\",\"부산\")) %>% \n  filter(substr(행정구역_코드,3,4)!=\"00\") %>% \n  select(행정구역,총인구수) %>% \n  arrange(총인구수)\n\nkor_202202_use$시도 = sapply(kor_202202_use$행정구역,\n                          function(x)strsplit(x,\" \")[[1]][1])\nkor_202202_use$시도 = factor(kor_202202_use$시도,\n                         levels = c(\"서울특별시\", \"대전광역시\", \"대구광역시\", \"부산광역시\"))\n\n\n#4개 지방의 색 지정\nregion_colors <- c(\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\")\n\nggplot(kor_202202_use,aes(x = reorder(행정구역, 총인구수),y = 총인구수 ,fill = 시도))+\n  geom_col()+\n  scale_y_continuous(name=\"총인구수, 2022년 2월\",labels = scales::comma) +\n  scale_fill_manual(values = region_colors) +\n  coord_flip()+\n  theme_light(base_family = \"AppleSDGothicNeo-Regular\")+ theme(panel.border = element_blank(), panel.grid.major.y = element_blank()) +\n  theme(axis.title.y = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size = 8),\n        legend.position = c(.58,.68),\n        legend.background = element_rect(fill = \"#ffffffb0\"))\n\n\n\nregion_colors <- c(\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\") %>% \n  lighten(0.4) %>% desaturate(0.8)\n\npopgrowth_df <- popgrowth_df %>% \n  mutate(region_highlight = ifelse(state %in% c(\"Texas\", \"Louisiana\"), NA, region %>% \n  paste()))\npopgrowth_df %>% head()\n\n\n# ggplot 그리기\nggplot(popgrowth_df, aes(x = state, y = 100*popgrowth, fill = region_highlight)) +\n  geom_col() +\n  scale_y_continuous(name = 'population growth, 2000 to 2010',\n                     labels = scales::percent_format(scale = 1),\n                     expand = c(0,0)) +\n  scale_fill_manual(values = region_colors,\n                    breaks = c(\"West\", \"South\", \"Midwest\", \"Northeast\"),\n                    na.value = '#56B4E9' %>% darken(0.3)) +\n  coord_flip() +\n  theme_light() +\n  theme(axis.title.y = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size = 8),\n        legend.position = c(.58,.68),\n        legend.background = element_rect(fill = \"#ffffffb0\"))\n\n\n\nregion_colors <- c(\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F0E442\") %>% \n  lighten(0.4) %>% desaturate(0.8)\n\nregion_colors[2] <- \"#56B4E9\" %>% darken(0.5)\n\n\nggplot(kor_202202_use, aes(x = reorder(행정구역, 총인구수),y = 총인구수 ,fill = 시도)) +\n  geom_col() +\n  scale_y_continuous(name=\"총인구수, 2022년 2월\", labels = scales::comma) +\n  scale_fill_manual(values = region_colors) +\n  coord_flip() +\n  theme_light(base_family = \"AppleSDGothicNeo-Regular\")+ theme(panel.border = element_blank(), panel.grid.major.y = element_blank()) +\n  theme(axis.title.y = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.length = unit(0,\"pt\"),\n        axis.text.y = element_text(size = 8),\n        legend.position = c(.58,.68),\n        legend.background = element_rect(fill = \"#ffffffb0\"))\n\n\n\n### 총인구\nlibrary(geojsonsf)\nlibrary(sf)\n\n\n### 지도\nkor_sido <- geojson_sf('/Users/jungwoolee/Desktop/college/Data Visualization/data/KOR_SIDO.json')\nkor_sigu <- geojson_sf('/Users/jungwoolee/Desktop/college/Data Visualization/data/KOR_SIGU.json')\n\nuse_map <- kor_sigu\nuse_map$행정구역_코드 <- paste(use_map$SIG_CD, \"00000\", sep = \"\")\nuse_map <- use_map %>% merge(kor_202202, by = \"행정구역_코드\", all.x = T)\n\n\nuse_map %>% ggplot(aes(fill = 총인구수)) + \n  geom_sf(color = \"gray90\") +\n  coord_sf(datum = NA) +\n  scale_fill_distiller(\n    name = '인구수',\n    palette = \"Blues\" , type = 'seq', na.value = 'grey60',\n    direction = 1,\n    breaks = seq(0, 10, 2) * 1e+5,\n    labels = format(seq(0, 10, 2) * 1e+5, big.mark = \",\", scientific = F),) +\n  theme_minimal() +\n  theme_light(base_family = \"AppleSDGothicNeo-Regular\") +\n  theme(legend.title.align = 0.5,\n        legend.text.align = 1.0,\n        legend.position = c(.85,.2))\n\n\n\n# 남녀 비율\nuse_map %>% \n  ggplot(aes(fill = 남여_비율)) +\n  geom_sf() + # 경계 생성\n  coord_sf(datum = NA) + # 좌표 삭제\n  scale_fill_continuous_diverging(palette = \"Blue-Red\", mid = 1, limits = 1 +c(-1, +1)*0.35) +\n  theme_minimal() +\n  theme_light(base_family = \"AppleSDGothicNeo-Regular\") +\n  theme(legend.title.align = 0.5,         # 범례 제목 정렬: 0 ~ 1\n        legend.text.align = 1.0,          # 범례 레이블 정렬: 0 ~ 1\n        legend.position = c(.85,.2))      # 범례의 위치\n\n\n\n\n\n\n\nrev = F\n\n\n\n\n\n\n\nrev = T"
  },
  {
    "objectID": "data_mining.html",
    "href": "data_mining.html",
    "title": "Data Mining",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nInteractive Maps (Japan earthequake)\n\n\n\n\n\n\n\ncode\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nCoordinate Reference Systems (purple martins)\n\n\n\n\n\n\n\ncode\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nYour First Map (kiva)\n\n\n\n\n\n\n\ncode\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\ngeopandas\n\n\n\n\n\n\n\ncode\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nseaborn and matplotlib\n\n\n\n\n\n\n\ncode\n\n\nJupyter\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nPandas\n\n\n\n\n\n\n\ncode\n\n\njupyter\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\n  \n\n\n\n\nNumpy\n\n\n\n\n\n\n\ncode\n\n\nJupyter\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nJungwoo Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hc.html",
    "href": "hc.html",
    "title": "수소 충전소 입지 선정",
    "section": "",
    "text": "data mining 개인 프로젝트\n\n\n\n수소차 증가와 충전소 부족\n요즘 운전하다 보면 전기차, 수소차와 같은 친환경차가 많이 보인다. 실제로 전체 자동차 누적 등록대수 2천550만3천대(23.01 기준)중 친환경차(전기·수소·하이브리드)가 총 159만대 (6.2%)를 차지하며 약진을 계속하고 있다. 전기차는 늘어가는 수에 맞춰 충전소도 늘어나고 있지만 수소차는 그러지 못하고 있습니다. 현재 우리나라에서 수소차 누적 판매량은 2만대가 넘었고 매년 약 1만대정도 팔리고 있습니다. 실제로 2021년 현대 자동차의 수소자동차인 넥쏘의 내수 판매량은 8473대로 꽤 많이 팔렸다는 것을 알 수 있습니다. 하지만 수소차 판매량에 비해 충전소는 턱없이 부족한 상황입니다. 2021년 기준 수소충전소는 148개로 충전소 한개당 약 135대를 충전해야하는 상황이다.\n\n\n\n수소충전소 구축의 어려움\n앞서 수소충전소가 148개 있다고 얘기했는데 국토교통부의 발표에 따르면 2040년까지 수소충전소를 1200개정도를 설치하겠다고 발표했다. 수소충전소 1개를 짓는데 약 20억에서 30억정도의 비용이 들어간다. 국토교통부의 계획인 1200개를 다 짓는다고 가정하면 약 3조6000억정도의 어마어마한 비용이 들어갑니다. 또한, 수소충전시설의 경우 부정적인 인식을 가지고 있다. 따라서 인구밀도가 높은 서울시의 경우 부지확보 곤란 및 주민 수용성 등의 문제로 충전소 입지 선정에 어려움을 겪고 있다. 수소연료공급시설은 전기차 충전소와 달리 건축법 시행령 제19호의 위험물 저장 및 처리시설에 해당하므로 사전에 부지 적정성을 확인해야 한다. 이때, 교육환경보호법률과 주택건설기준규정에 따라 학교, 공동주택, 어린이놀이터, 의료시설, 유치원, 어린이집, 경로당 등으로부터 수평거리 50m 이상 떨어진 곳에 배치되어야 하므로 수소충전소 부지 선정 시 고려해야 하는 요소가 많다. 따라서 인구밀도가 높은 서울시의 경우, 이러한 규제요인들을 고려하기가 까다롭다. 그래서 데이터를 활용해 적절한 수소충전소 설치 지역을 알아낼려고 한다.\n\n\n\n\n\n\n\nimport geopandas as gpd\nimport pandas as pd\nimport math\n\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\n한국가스안전공사_수소충전소_현황 데이터를 가지고 로드맵을 그려봤다.\n\n# data load\nhc_location = pd.read_csv('/Users/jungwoolee/Desktop/project/data mining/hc_acc.csv')\n\n# markercluster\nm_3 = folium.Map(location = [36.4801, 127.2892], tiles = 'cartodbpositron', zoom_start = 13)\n\n# 지도에 표시\nmc = MarkerCluster()\nfor idx, row in hc_location.iterrows():\n    if not math.isnan(row['경도']) and not math.isnan(row['위도']):\n        mc.add_child(Marker([row['위도'], row['경도']]))\nm_3.add_child(mc)\n\nm_3\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n2023년_04월_자동차_등록자료_통계로 현재 수소자동차의 현황을 파악학고 한국가스안전공사_수소충전소_현황을 이용해 각 지역에 충전소가 몇개 있는지 파악하고 지역마다 충전소당 충전차량이 몇대인지 계산해서 hydrogen fueled car info 데이터를 새로 만들었다."
  },
  {
    "objectID": "machine learning.html",
    "href": "machine learning.html",
    "title": "제주도 교통량 예측 ai 만들기",
    "section": "",
    "text": "No matching items"
  }
]